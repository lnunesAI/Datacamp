{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-modeling.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrocePxQOKpS5Ry7/TxG7E"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f5FTJNcbC9gb"},"source":["# Modeling\r\n",">  Time to bring everything together and build some models! In this last chapter, you will build a base model before tuning some hyperparameters and improving your results with ensembles. You will then get some final tips and tricks to help you compete more efficiently.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Datacamp]\r\n","- image: images/datacamp/___"]},{"cell_type":"markdown","metadata":{"id":"f9J9naPqLbDt"},"source":["> Note: This is a summary of the course's chapter 4 exercises \"Winning a Kaggle Competition in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"]},{"cell_type":"code","metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1614976008643,"user_tz":180,"elapsed":1962,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvrKEMkeEF2g"},"source":["## Baseline model"]},{"cell_type":"markdown","metadata":{"id":"Ho1eZTss8AvH"},"source":["### Replicate validation score"]},{"cell_type":"markdown","metadata":{"id":"0XCw46EVBszc"},"source":["<div class=\"\"><p>You've seen both validation and Public Leaderboard scores in the video. However, the code examples are available only for the test data. To get the validation scores you have to repeat the same process on the holdout set.</p>\r\n","<p>Throughout this chapter, you will work with New York City Taxi competition data. The problem is to predict the fare amount for a taxi ride in New York City. The competition metric is the root mean squared error.</p>\r\n","<p>The first goal is to evaluate the Baseline model on the validation data. You will replicate the simplest Baseline based on the mean of <code>\"fare_amount\"</code>. Recall that as a validation strategy we used a 30% holdout split with <code>validation_train</code> as train and <code>validation_test</code> as holdout DataFrames. Both of them are available in your workspace.</p></div>"]},{"cell_type":"code","metadata":{"id":"rVUjVbUPrUAq","executionInfo":{"status":"ok","timestamp":1614976018356,"user_tz":180,"elapsed":11661,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["from sklearn.model_selection import train_test_split\r\n","\r\n","train = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/2-machine-learning-scientist-with-python/23-winning-a-kaggle-competition-in-python/datasets/taxi_train.csv')\r\n","test = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/2-machine-learning-scientist-with-python/23-winning-a-kaggle-competition-in-python/datasets/taxi_test.csv')\r\n","train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime']).dt.tz_localize(None)\r\n","test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime']).dt.tz_localize(None)\r\n","validation_train, validation_test = train_test_split(train, test_size=0.3, random_state = 123)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"evlwYTWb7h6k","executionInfo":{"status":"ok","timestamp":1614976018365,"user_tz":180,"elapsed":11661,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c4fbc0b2-f8c4-49f4-8e96-cbd1cda67635"},"source":["import hashlib\r\n","X = train\r\n","hashlib.sha256(X.to_json().encode()).hexdigest()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'c837903792d6345a07ded5be113debff9ab18e167fae6679f0dcfef44ed4c4e8'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"id":"0_0rJTYk-Z-a","executionInfo":{"status":"error","timestamp":1614976018370,"user_tz":180,"elapsed":11664,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"8137e5e0-ce53-49b3-c6ff-f6aa7e3ec32a"},"source":["c837903792d6345a07ded5be113debff9ab18e167fae6679f0dcfef44ed4c4e8\r\n","94d6e2dc1cbe787117f7ee693fa8180d0bce9f5e58acf8328af5c1c966e4cdc4"],"execution_count":4,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-b0636721f563>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    94d6e2dc1cbe787117f7ee693fa8180d0bce9f5e58acf8328af5c1c966e4cdc4\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"Uv3-kGVF6CLh","executionInfo":{"status":"aborted","timestamp":1614976018366,"user_tz":180,"elapsed":11658,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["for x in range(0, 1000):\r\n","  validation_train, validation_test = train_test_split(train, test_size=0.3, random_state=x)\r\n","  print(x, hashlib.sha256(validation_train.to_json().encode()).hexdigest())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACHQFZQEBug8"},"source":["Instructions\r\n","<ul>\r\n","<li>Calculate the mean of <code>\"fare_amount\"</code> over the whole <code>validation_train</code> DataFrame.</li>\r\n","<li>Assign this naive prediction value to all the holdout predictions. Store them in the <code>\"pred\"</code> column.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"w48_1NTN8ASP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614976130526,"user_tz":180,"elapsed":1311,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b856c185-29bc-4847-ace4-ff86f64bf73b"},"source":["from sklearn.metrics import mean_squared_error\r\n","from math import sqrt\r\n","\r\n","# Calculate the mean fare_amount on the validation_train data\r\n","naive_prediction = np.mean(validation_train['fare_amount'])\r\n","\r\n","# Assign naive prediction to all the holdout observations\r\n","validation_test = validation_test.copy()\r\n","validation_test['pred'] = naive_prediction\r\n","\r\n","# Measure the local RMSE\r\n","rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\r\n","print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Validation RMSE for Baseline I model: 9.986\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I8PJoIapCZUR"},"source":["**It's exactly the same number you've seen in the slides, well done! So, to avoid overfitting you should fully replicate your models using the validation data**"]},{"cell_type":"markdown","metadata":{"id":"ToR3Y1GhCggK"},"source":["### Baseline based on the date"]},{"cell_type":"markdown","metadata":{"id":"gnreVQA0Ci4L"},"source":["<div class=\"\"><p>We've already built 3 different baseline models. To get more practice, let's build a couple more. The first model is based on the grouping variables. It's clear that the ride fare could depend on the part of the day. For example, prices could be higher during the rush hours.</p>\r\n","<p>Your goal is to build a baseline model that will assign the average \"fare_amount\" for the corresponding hour. For now, you will create the model for the whole <code>train</code> data and make predictions for the <code>test</code> dataset.</p>\r\n","<p>The <code>train</code> and <code>test</code> DataFrames are available in your workspace. Moreover, the \"pickup_datetime\" column in both DataFrames is already converted to a <code>datetime</code> object for you.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"5yO3KOk4Ck3z"},"source":["Instructions\r\n","<ul>\r\n","<li>Get the hour from the \"pickup_datetime\" column for the <code>train</code> and <code>test</code> DataFrames.</li>\r\n","<li>Calculate the mean \"fare_amount\" for each hour on the train data.</li>\r\n","<li>Make <code>test</code> predictions using <code>pandas</code>' <code>map()</code> method and the grouping obtained.</li>\r\n","<li>Write predictions to the file.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"5BQVWSwdDIaN","executionInfo":{"status":"ok","timestamp":1614976135598,"user_tz":180,"elapsed":1437,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Get pickup hour from the pickup_datetime column\r\n","train['hour'] = train['pickup_datetime'].dt.hour\r\n","test['hour'] = test['pickup_datetime'].dt.hour\r\n","\r\n","# Calculate average fare_amount grouped by pickup hour \r\n","hour_groups = train.groupby('hour')['fare_amount'].mean()\r\n","\r\n","# Make predictions on the test set\r\n","test['fare_amount'] = test.hour.map(hour_groups)\r\n","\r\n","# Write predictions\r\n","test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NL552N-o0cod","executionInfo":{"status":"ok","timestamp":1614976138570,"user_tz":180,"elapsed":1359,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"687500c9-03bd-4870-dd4a-42df50cb9a5e"},"source":["!head hour_mean_sub.csv"],"execution_count":7,"outputs":[{"output_type":"stream","text":["id,fare_amount\n","0,11.199879638916757\n","1,11.199879638916757\n","2,11.241585365853654\n","3,10.964889086069206\n","4,10.964889086069206\n","5,10.964889086069206\n","6,11.094688755020092\n","7,11.094688755020092\n","8,11.094688755020092\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uu_WNF-_DO9n"},"source":["**Such baseline achieves 1409th place on the Public Leaderboard which is slightly better than grouping by the number of passengers. Also, remember to replicate all the results for the validation set as it was done in the previous exercise.**"]},{"cell_type":"markdown","metadata":{"id":"eekKhmfGDQuo"},"source":["### Baseline based on the gradient boosting"]},{"cell_type":"markdown","metadata":{"id":"8-BWJzEMDTKI"},"source":["<div class=\"\"><p>Let's build a final baseline based on the Random Forest. You've seen a huge score improvement moving from the grouping baseline to the Gradient Boosting in the video. Now, you will use <code>sklearn</code>'s Random Forest to further improve this score.</p>\r\n","<p>The goal of this exercise is to take numeric features and train a Random Forest model without any tuning. After that, you could make test predictions and validate the result on the Public Leaderboard. Note that you've already got an <code>\"hour\"</code> feature which could also be used as an input to the model.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"9TS57abMDVTR"},"source":["Instructions\r\n","<ul>\r\n","<li>Add the <code>\"hour\"</code> feature to the list of numeric features.</li>\r\n","<li>Fit the <code>RandomForestRegressor</code> on the train data with numeric features and <code>\"fare_amount\"</code> as a target.</li>\r\n","<li>Use the trained Random Forest model to make predictions on the test data.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"VLqAo72RDigb","executionInfo":{"status":"ok","timestamp":1614976159659,"user_tz":180,"elapsed":18225,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["from sklearn.ensemble import RandomForestRegressor\r\n","\r\n","# Select only numeric features\r\n","features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\r\n","            'dropoff_latitude', 'passenger_count', 'hour']\r\n","\r\n","# Train a Random Forest model\r\n","rf = RandomForestRegressor()\r\n","rf.fit(train[features], train.fare_amount)\r\n","\r\n","# Make predictions on the test data\r\n","test['fare_amount'] = rf.predict(test[features])\r\n","\r\n","# Write predictions\r\n","test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THN5ym-f0l4Y","executionInfo":{"status":"ok","timestamp":1614976160799,"user_tz":180,"elapsed":1134,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"7bdc8992-d99c-444a-d7c8-5e4c8aa02a62"},"source":["!head rf_sub.csv"],"execution_count":9,"outputs":[{"output_type":"stream","text":["id,fare_amount\n","0,8.528000000000002\n","1,8.481000000000003\n","2,5.441999999999998\n","3,9.580999999999996\n","4,13.517999999999997\n","5,7.904000000000002\n","6,5.852999999999998\n","7,53.9464\n","8,11.937000000000001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0ypKDrWhDnw_"},"source":["**This final baseline achieves the 1051st place on the Public Leaderboard which is slightly better than the Gradient Boosting from the video. So, now you know how to build fast and simple baseline models to validate your initial pipeline.**"]},{"cell_type":"markdown","metadata":{"id":"sL3_v7XQDtvo"},"source":["## Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"SNQmOrNoGaih"},"source":["### Grid search"]},{"cell_type":"markdown","metadata":{"id":"CAndiS7QGc8q"},"source":["<div class=\"\"><p>Recall that we've created a baseline Gradient Boosting model in the previous lesson. Your goal now is to find the best <code>max_depth</code> hyperparameter value for this Gradient Boosting model. This hyperparameter limits the number of nodes in each individual tree. You will be using K-fold cross-validation to measure the local performance of the model for each hyperparameter value.</p>\r\n","<p>You're given a function <code>get_cv_score()</code>, which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation.</p></div>"]},{"cell_type":"code","metadata":{"id":"mhpV23Zn2MK8","executionInfo":{"status":"ok","timestamp":1614976344870,"user_tz":180,"elapsed":1302,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["from sklearn.model_selection import KFold\r\n","from sklearn.ensemble import GradientBoostingRegressor"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiWKiLd72IaN","executionInfo":{"status":"ok","timestamp":1614976348701,"user_tz":180,"elapsed":1315,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["def get_cv_score(train, params):\r\n","    # Create KFold object\r\n","    kf = KFold(n_splits=3, shuffle=True, random_state=123)\r\n","\r\n","    rmse_scores = []\r\n","    \r\n","    # Loop through each split\r\n","    for train_index, test_index in kf.split(train):\r\n","        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\r\n","    \r\n","        # Train a Gradient Boosting model\r\n","        gb = GradientBoostingRegressor(random_state=123, **params).fit(cv_train[features], cv_train.fare_amount)\r\n","    \r\n","        # Make predictions on the test data\r\n","        pred = gb.predict(cv_test[features])\r\n","    \r\n","        fold_score = np.sqrt(mean_squared_error(cv_test['fare_amount'], pred))\r\n","        rmse_scores.append(fold_score)\r\n","    \r\n","    return np.round(np.mean(rmse_scores) + np.std(rmse_scores), 5)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTNOb96vGe0q"},"source":["Instructions\r\n","<ul>\r\n","<li>Specify the grid for possible <code>max_depth</code> values with 3, 6, 9, 12 and 15.</li>\r\n","<li>Pass each hyperparameter candidate in the grid to the model <code>params</code> dictionary.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"NVJIWN-rGton","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614976458784,"user_tz":180,"elapsed":98247,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e6eb5601-f3c2-4ac0-e358-28161036ea97"},"source":["# Possible max depth values\r\n","max_depth_grid = [3, 6, 9, 12, 15]\r\n","results = {}\r\n","\r\n","# For each value in the grid\r\n","for max_depth_candidate in max_depth_grid:\r\n","    # Specify parameters for the model\r\n","    params = {'max_depth': max_depth_candidate}\r\n","\r\n","    # Calculate validation score for a particular hyperparameter\r\n","    validation_score = get_cv_score(train, params)\r\n","\r\n","    # Save the results for each max depth value\r\n","    results[max_depth_candidate] = validation_score   \r\n","print(results)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["{3: 5.67086, 6: 5.36931, 9: 5.35548, 12: 5.51582, 15: 5.71856}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CHfTo7taG2vQ"},"source":["**We have a validation score for each value in the grid. It's clear that the optimal max depth value is located somewhere between 3 and 6. The next step could be to use a smaller grid, for example [3, 4, 5, 6] and repeat the same process. Moving from larger to smaller grids allows us to find the most optimal values. Keep going to try optimizing 2 hyperparameters simultaneously!**"]},{"cell_type":"markdown","metadata":{"id":"NVOQT0j-HAbL"},"source":["### 2D grid search"]},{"cell_type":"markdown","metadata":{"id":"MmdMUoTmHC60"},"source":["<div class=\"\"><p>The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations. However, in such cases, the grid search space is rapidly expanding. For example, if we have 2 parameters with 10 possible values, it will yield 100 experiment runs.</p>\r\n","<p>Your goal is to find the best hyperparameter couple of <code>max_depth</code> and <code>subsample</code> for the Gradient Boosting model. <code>subsample</code> is a fraction of observations to be used for fitting the individual trees.</p>\r\n","<p>You're given a function <code>get_cv_score()</code>, which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"PJMEGxHpHEZ8"},"source":["Instructions\r\n","<ul>\r\n","<li>Specify the grids for possible <code>max_depth</code> and <code>subsample</code> values. For <code>max_depth</code>: 3, 5 and 7. For <code>subsample</code>: 0.8, 0.9 and 1.0.</li>\r\n","<li>Apply the <code>product()</code> function from the <code>itertools</code> package to the hyperparameter grids. It returns all possible combinations for these two grids.</li>\r\n","<li>Pass each hyperparameters candidate couple to the model <code>params</code> dictionary.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"Guu0p-ZmIPLw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614976552006,"user_tz":180,"elapsed":93217,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"99437cd2-698b-4d4c-de9b-78a15e52a419"},"source":["import itertools\r\n","\r\n","# Hyperparameter grids\r\n","max_depth_grid = [ 3, 5, 7]\r\n","subsample_grid = [0.8, 0.9, 1.0]\r\n","results = {}\r\n","\r\n","# For each couple in the grid\r\n","for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\r\n","    params = {'max_depth': max_depth_candidate,\r\n","              'subsample': subsample_candidate}\r\n","    validation_score = get_cv_score(train, params)\r\n","    # Save the results for each couple\r\n","    results[(max_depth_candidate, subsample_candidate)] = validation_score   \r\n","print(results)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["{(3, 0.8): 5.65813, (3, 0.9): 5.65228, (3, 1.0): 5.67086, (5, 0.8): 5.34925, (5, 0.9): 5.44507, (5, 1.0): 5.3132, (7, 0.8): 5.39502, (7, 0.9): 5.40612, (7, 1.0): 5.3591}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DtKeYx3JItI6"},"source":["**You can see that tuning multiple hyperparameters simultaneously achieves better results. In the previous exercise, tuning only the max_depth parameter gave the best RMSE of \\$6.50. With max_depth equal to 7 and subsample equal to 0.8, the best RMSE is now $6.16. However, do not spend too much time on the hyperparameter tuning at the beginning of the competition!**"]},{"cell_type":"markdown","metadata":{"id":"OL6szVNVI6BV"},"source":["## Model ensembling"]},{"cell_type":"markdown","metadata":{"id":"6Ob-xd72KKnx"},"source":["## Model blending"]},{"cell_type":"markdown","metadata":{"id":"noXRAzOLKNpZ"},"source":["<div class=\"\"><p>You will start creating model ensembles with a <strong>blending</strong> technique.</p>\r\n","<p>Your goal is to train 2 different models on the New York City Taxi competition data. Make predictions on the test data and then blend them using a simple arithmetic mean.</p>\r\n","<p>The <code>train</code> and <code>test</code> DataFrames are already available in your workspace. <code>features</code> is a list of columns to be used for training and it is also available in your workspace. The target variable name is \"fare_amount\".</p></div>"]},{"cell_type":"code","metadata":{"id":"-QSjlxYZKyok","executionInfo":{"status":"ok","timestamp":1614977783423,"user_tz":180,"elapsed":957,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["train = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/2-machine-learning-scientist-with-python/23-winning-a-kaggle-competition-in-python/datasets/taxi_chapter_4_cv.csv')\r\n","test = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/2-machine-learning-scientist-with-python/23-winning-a-kaggle-competition-in-python/datasets/taxi_chapter_4_cv_test.csv')\r\n","features = ['pickup_longitude',\r\n"," 'pickup_latitude',\r\n"," 'dropoff_longitude',\r\n"," 'dropoff_latitude',\r\n"," 'passenger_count',\r\n"," 'distance_km',\r\n"," 'hour']"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CILPG9blKPax"},"source":["Instructions\r\n","<ul>\r\n","<li>Train a Gradient Boosting model on the train data using <code>features</code> list, and the \"fare_amount\" column as a target variable.</li>\r\n","<li>Train a Random Forest model in the same manner.</li>\r\n","<li>Make predictions on the test data using both Gradient Boosting and Random Forest models.</li>\r\n","<li>Find the average of both models predictions.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"CogLz8TEKJRo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614977786432,"user_tz":180,"elapsed":1680,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"2b61ae91-942d-4714-cee9-c512ff8826cf"},"source":["from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\r\n","\r\n","# Train a Gradient Boosting model\r\n","gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\r\n","\r\n","# Train a Random Forest model\r\n","rf = RandomForestRegressor().fit(train[features], train.fare_amount)\r\n","\r\n","# Make predictions on the test data\r\n","test['gb_pred'] = gb.predict(test[features])\r\n","test['rf_pred'] = rf.predict(test[features])\r\n","\r\n","# Find mean of model predictions\r\n","test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\r\n","print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["    gb_pred  rf_pred     blend\n","0  9.661374    9.301  9.481187\n","1  9.304288    8.030  8.667144\n","2  5.795140    4.770  5.282570\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OLnDajYtK2WE"},"source":["**Blending allows you to get additional score improvements almost for free just by averaging multiple models predictions.**"]},{"cell_type":"markdown","metadata":{"id":"21SfvYicLZAH"},"source":["### Model stacking I"]},{"cell_type":"markdown","metadata":{"id":"rirUvORmLbVf"},"source":["<div class=\"\"><p>Now it's time for <strong>stacking</strong>. To implement the stacking approach, you will follow the 6 steps we've discussed in the previous video:</p>\r\n","<ol>\r\n","<li>Split train data into two parts</li>\r\n","<li>Train multiple models on Part 1</li>\r\n","<li>Make predictions on Part 2</li>\r\n","<li>Make predictions on the test data</li>\r\n","<li>Train a new model on Part 2 using predictions as features</li>\r\n","<li>Make predictions on the test data using the 2nd level model</li>\r\n","</ol>\r\n","<p><code>train</code> and <code>test</code> DataFrames are already available in your workspace. <code>features</code> is a list of columns to be used for training on the Part 1 data and it is also available in your workspace. Target variable name is \"fare_amount\".</p></div>"]},{"cell_type":"markdown","metadata":{"id":"5_rgPyUXLe--"},"source":["Instructions 1/2\r\n","<ul>\r\n","<li>Split the <code>train</code> DataFrame into two equal parts: <code>part_1</code> and <code>part_2</code>. Use the <code>train_test_split()</code> function with <code>test_size</code> equal to 0.5.</li>\r\n","<li>Train Gradient Boosting and Random Forest models on the <code>part_1</code> data.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"wunYxie8L0TH","executionInfo":{"status":"ok","timestamp":1614977898057,"user_tz":180,"elapsed":1160,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\r\n","\r\n","# Split train data into two parts\r\n","part_1, part_2 = train_test_split(train, test_size=.5, random_state=123)\r\n","\r\n","# Train a Gradient Boosting model on Part 1\r\n","gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\r\n","\r\n","# Train a Random Forest model on Part 1\r\n","rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xE2vOVGLe_F"},"source":["Instructions 2/2\r\n","<ul>\r\n","<li>Make Gradient Boosting and Random Forest predictions on the <code>part_2</code> data.</li>\r\n","<li>Make Gradient Boosting and Random Forest predictions on the <code>test</code> data.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"cS6QQJjeM1Em","executionInfo":{"status":"ok","timestamp":1614977901353,"user_tz":180,"elapsed":799,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Make predictions on the Part 2 data\r\n","part_2 = part_2.copy()\r\n","part_2['gb_pred'] = gb.predict(part_2[features])\r\n","part_2['rf_pred'] = rf.predict(part_2[features])\r\n","\r\n","# Make predictions on the test data\r\n","test = test.copy()\r\n","test['gb_pred'] = gb.predict(test[features])\r\n","test['rf_pred'] = rf.predict(test[features])"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKXy7Y5VM8o5"},"source":["**You've covered 4 out of 6 steps to create a stacking ensemble. The only steps left is to create a new model on Part 2 data using predictions as features and apply it to the test data.**"]},{"cell_type":"markdown","metadata":{"id":"TuivLO4iM-lc"},"source":["### Model stacking II"]},{"cell_type":"markdown","metadata":{"id":"btWZruVONBDc"},"source":["<div class=\"\"><p>OK, what you've done so far in the stacking implementation:</p>\r\n","<ol>\r\n","<li>Split train data into two parts</li>\r\n","<li>Train multiple models on Part 1</li>\r\n","<li>Make predictions on Part 2</li>\r\n","<li>Make predictions on the test data</li>\r\n","</ol>\r\n","<p>Now, your goal is to create a second level model using predictions from steps 3 and 4 as features. So, this model is trained on Part 2 data and then you can make stacking predictions on the test data.</p>\r\n","<p><code>part_2</code> and <code>test</code> DataFrames are already available in your workspace. Gradient Boosting and Random Forest predictions are stored in these DataFrames under the names \"gb_pred\" and \"rf_pred\", respectively.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"aMUIAduENl4u"},"source":["Instructions\r\n","<ul>\r\n","<li>Train a Linear Regression model on the Part 2 data using Gradient Boosting and Random Forest models predictions as features.</li>\r\n","<li>Make predictions on the test data using Gradient Boosting and Random Forest models predictions as features.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"jAZijRfUN9Hr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614977913790,"user_tz":180,"elapsed":794,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"f127ab95-ebba-46e3-8a93-e904c3ec6dae"},"source":["from sklearn.linear_model import LinearRegression\r\n","\r\n","# Create linear regression model without the intercept\r\n","lr = LinearRegression(fit_intercept=False)\r\n","\r\n","# Train 2nd level model on the Part 2 data\r\n","lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\r\n","\r\n","# Make stacking predictions on the test data\r\n","test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\r\n","\r\n","# Look at the model coefficients\r\n","print(lr.coef_)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["[0.35812797 0.64768928]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yqfYx7S3OMwJ"},"source":["\r\n","\r\n","```\r\n","[0.72504358 0.27647395]\r\n","```\r\n","\r\n","\r\n","**Usually, the 2nd level model is some simple model like Linear or Logistic Regressions. Also, note that you were not using intercept in the Linear Regression just to combine pure model predictions. Looking at the coefficients, it's clear that 2nd level model has more trust to the Gradient Boosting: 0.7 versus 0.3 for the Random Forest model.**"]},{"cell_type":"markdown","metadata":{"id":"xxc6AD8sOn2o"},"source":["## Final tips"]},{"cell_type":"markdown","metadata":{"id":"CChIgeTKa7dk"},"source":["### Testing Kaggle forum ideas"]},{"cell_type":"markdown","metadata":{"id":"3BIGhpAVa-Hk"},"source":["<div class=\"\"><p>Unfortunately, not all the Forum posts and Kernels are necessarily useful for your model. So instead of blindly incorporating ideas into your pipeline, you should test them first.</p>\r\n","<p>You're given a function <code>get_cv_score()</code>, which takes a train dataset as an argument and returns the overall validation root mean squared error over 3-fold cross-validation. The <code>train</code> DataFrame is already available in your workspace.</p>\r\n","<p>You should try different suggestions from the Kaggle Forum and check whether they improve your validation score.</p></div>"]},{"cell_type":"code","metadata":{"id":"cCex7Um9MO35","executionInfo":{"status":"ok","timestamp":1614978031093,"user_tz":180,"elapsed":819,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["def get_cv_score(train):\r\n","    features = ['pickup_longitude', 'pickup_latitude',\r\n","            'dropoff_longitude', 'dropoff_latitude',\r\n","            'passenger_count', 'distance_km', 'hour', 'weird_feature']\r\n","    \r\n","    features = [x for x in features if x in train.columns]\r\n","  \r\n","    # Create KFold object\r\n","    kf = KFold(n_splits=3, shuffle=True, random_state=123)\r\n","\r\n","    rmse_scores = []\r\n","    \r\n","    # Loop through each split\r\n","    for train_index, test_index in kf.split(train):\r\n","        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\r\n","    \r\n","        # Train a Gradient Boosting model\r\n","        gb = GradientBoostingRegressor(random_state=123).fit(cv_train[features], cv_train.fare_amount)\r\n","    \r\n","        # Make predictions on the test data\r\n","        pred = gb.predict(cv_test[features])\r\n","    \r\n","        fold_score = np.sqrt(mean_squared_error(cv_test['fare_amount'], pred))\r\n","        rmse_scores.append(fold_score)\r\n","    \r\n","    return np.round(np.mean(rmse_scores) + np.std(rmse_scores), 5)"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmO4hVg9bArO"},"source":["Instructions 1/2\r\n","<li><strong>Suggestion 1: the <code>passenger_count</code> feature is useless</strong>. Let's see! Drop this feature and compare the scores.</li>"]},{"cell_type":"code","metadata":{"id":"1x230eyKcFO2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614978036668,"user_tz":180,"elapsed":1682,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"42e65b7a-df93-42ad-e83c-e90d56c26899"},"source":["# Drop passenger_count column\r\n","new_train_1 = train.drop('passenger_count', axis=1)\r\n","\r\n","# Compare validation scores\r\n","initial_score = get_cv_score(train)\r\n","new_score = get_cv_score(new_train_1)\r\n","\r\n","print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Initial score is 6.49932 and the new score is 6.42315\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kp0LUw7xbArP"},"source":["Instructions 2/2\r\n","<li>This first suggestion worked. <strong>Suggestion 2: Sum of <code>pickup_latitude</code> and <code>distance_km</code> is a good feature</strong>. Let's try it!</li>"]},{"cell_type":"code","metadata":{"id":"vtmL858kcRIL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614978043690,"user_tz":180,"elapsed":1754,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"af27efe5-9a6c-4e46-b382-59dd445a80f0"},"source":["# Create copy of the initial train DataFrame\r\n","new_train_2 = train.copy()\r\n","\r\n","# Find sum of pickup latitude and ride distance\r\n","new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\r\n","\r\n","# Compare validation scores\r\n","initial_score = get_cv_score(train)\r\n","new_score = get_cv_score(new_train_2)\r\n","\r\n","print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Initial score is 6.49932 and the new score is 6.50495\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gednyj4Dcd0v"},"source":["**Be aware that not all the ideas shared publicly could work for you! In this particular case, dropping the \"passenger_count\" feature helped, while finding the sum of pickup latitude and ride distance did not. The last action you perform in any Kaggle competition is selecting final submissions.**"]},{"cell_type":"markdown","metadata":{"id":"_7hyK5c5cf85"},"source":["### Select final submissions"]},{"cell_type":"markdown","metadata":{"id":"awKTuc22cisI"},"source":["<div class=\"\"><p>The last action in every competition is selecting final submissions. Your goal is to select 2 final submissions based on the local validation and Public Leaderboard scores. Suppose that the competition metric is RMSE (the lower the metric the better). Keep up with a selection strategy we've discussed in the slides:</p>\r\n","<ol>\r\n","<li>Local validation: 1.25; Leaderboard: 1.35.</li>\r\n","<li>Local validation: 1.32; Leaderboard: 1.39.</li>\r\n","<li>Local validation: 1.10; Leaderboard: 1.29.</li>\r\n","<li>Local validation: 1.17; Leaderboard: 1.25.</li>\r\n","<li>Local validation: 1.21; Leaderboard: 1.32.</li>\r\n","</ol></div>"]},{"cell_type":"markdown","metadata":{"id":"pVtIaJPWczQ1"},"source":["<pre>\r\n","Possible Answers\r\n","1 and 2.\r\n","2 and 3.\r\n","<b>3 and 4.</b>\r\n","4 and 5.\r\n","1 and 5.\r\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"mL1uNk46c_GB"},"source":["**Submission 3 is the best on local validation and submission 4 is the best on Public Leaderboard. So, it's the best choice for the final submissions!**"]}]}