{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"3_feature-selection-ii,-selecting-for-model-accuracy.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMFaEnicS98lWAk/C/RnpQM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Feature selection II, selecting for model accuracy\r\n","> In this second chapter on feature selection, you'll learn how to let models help you find the most important features in a dataset for predicting a particular target feature. In the final lesson of this chapter, you'll combine the advice of multiple, different, models to decide on which features are worth keeping.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Datacamp]\r\n","- image: images/datacamp/___"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 3 exercises \"Dimensionality Reduction in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG"}},{"cell_type":"markdown","source":["## Selecting features for model performance"],"metadata":{"id":"5PNJq_0yZIh4"}},{"cell_type":"markdown","source":["### Building a diabetes classifier\r\n"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["<div class=\"\"><p>You'll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset. The data has been split into a training and test set and pre-loaded for you as <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, and <code>y_test</code>.</p>\r\n","<p>A <code>StandardScaler()</code> instance has been predefined as <code>scaler</code> and a <code>LogisticRegression()</code> one as <code>lr</code>.</p></div>"],"metadata":{"id":"ramm24GLZSJW"}},{"cell_type":"code","execution_count":null,"source":["diabetes_df = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/7-dimensionality-reduction-in-python/datasets/diabetes_df.csv')"],"outputs":[],"metadata":{"id":"3AxSzF8TC0ZC"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.preprocessing import StandardScaler\r\n","from sklearn.linear_model import LogisticRegression\r\n","from sklearn.metrics import accuracy_score"],"outputs":[],"metadata":{"id":"3Pdty0yFgnb9"}},{"cell_type":"code","execution_count":null,"source":["X, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n","\r\n","scaler = StandardScaler()\r\n","lr = LogisticRegression()"],"outputs":[],"metadata":{"id":"1bslMgE8bwvk"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Fit the scaler on the training features and transform these features in one go.</li>\r\n","<li>Fit the logistic regression model on the scaled training data.</li>\r\n","<li>Scale the test features.</li>\r\n","<li>Predict diabetes presence on the scaled test set.</li>\r\n","</ul>"],"metadata":{"id":"mzO-HqbyZUCu"}},{"cell_type":"code","execution_count":null,"source":["# Fit the scaler on the training features and transform these in one go\r\n","X_train_std = scaler.fit_transform(X_train)\r\n","\r\n","# Fit the logistic regression model on the scaled training data\r\n","lr.fit(X_train_std, y_train)\r\n","\r\n","# Scale the test features\r\n","X_test_std = scaler.transform(X_test)\r\n","\r\n","# Predict diabetes presence on the scaled test set\r\n","y_pred = lr.predict(X_test_std)\r\n","\r\n","# Prints accuracy metrics and feature coefficients\r\n","print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \r\n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"outputs":[{"output_type":"stream","name":"stdout","text":["79.6% accuracy on test set.\n","{'pregnant': 0.05, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.35, 'age': 0.34}\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5jB2hJ1aU8n","executionInfo":{"status":"ok","timestamp":1611410421219,"user_tz":180,"elapsed":612,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"185eebfc-230a-4a07-ff52-b5c991bc3330"}},{"cell_type":"markdown","source":["**We get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features.**"],"metadata":{"id":"0u2IPOgdaZZs"}},{"cell_type":"markdown","source":["### Manual Recursive Feature Elimination\r\n"],"metadata":{"id":"i1PkwSJJiBMM"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now that we've created a diabetes classifier, let's see if we can reduce the number of features without hurting the model accuracy too much.</p>\r\n","<p>On the second line of code the features are selected from the original dataframe. Adjust this selection.</p>\r\n","<p>A <code>StandardScaler()</code> instance has been predefined as <code>scaler</code> and a <code>LogisticRegression()</code> one as <code>lr</code>.</p>\r\n","<p>All necessary functions and packages have been pre-loaded too.</p></div>"],"metadata":{"id":"6zUtgXMCiRn5"}},{"cell_type":"markdown","source":["Instructions 1/3\r\n","<li>First, run the given code, then remove the feature with the lowest model coefficient from <code>X</code>.</li>"],"metadata":{"id":"EFvVVIOJiUIh"}},{"cell_type":"code","execution_count":null,"source":["# Remove the feature with the lowest model coefficient\r\n","X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\r\n","\r\n","# Performs a 25-75% train test split\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n","\r\n","# Scales features and fits the logistic regression model\r\n","lr.fit(scaler.fit_transform(X_train), y_train)\r\n","\r\n","# Calculates the accuracy on the test set and prints coefficients\r\n","acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\r\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) \r\n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"outputs":[{"output_type":"stream","name":"stdout","text":["80.6% accuracy on test set.\n","{'pregnant': 0.05, 'glucose': 1.24, 'triceps': 0.24, 'insulin': 0.2, 'bmi': 0.39, 'family': 0.34, 'age': 0.35}\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFreOAD_jPRy","executionInfo":{"status":"ok","timestamp":1611410525195,"user_tz":180,"elapsed":591,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"65d6532e-4269-4a33-8767-72bcb205eea8"}},{"cell_type":"markdown","source":["Instructions 2/3\r\n","<li>Run the code and remove 2 more features with the lowest model coefficients.</li>"],"metadata":{"id":"haSHaRw-jbsp"}},{"cell_type":"code","execution_count":null,"source":["# Remove the 2 features with the lowest model coefficients\r\n","X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\r\n","\r\n","# Performs a 25-75% train test split\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n","\r\n","# Scales features and fits the logistic regression model\r\n","lr.fit(scaler.fit_transform(X_train), y_train)\r\n","\r\n","# Calculates the accuracy on the test set and prints coefficients\r\n","acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\r\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) \r\n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"outputs":[{"output_type":"stream","name":"stdout","text":["79.6% accuracy on test set.\n","{'glucose': 1.13, 'triceps': 0.25, 'bmi': 0.34, 'family': 0.34, 'age': 0.37}\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_bYvU5KjtEH","executionInfo":{"status":"ok","timestamp":1611410630683,"user_tz":180,"elapsed":659,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"167cd12c-f5ab-489d-fb08-8c7ac6cd85db"}},{"cell_type":"markdown","source":["Instructions 2/3\r\n","<li>Run the code and only keep the feature with the highest coefficient.</li>"],"metadata":{"id":"ZTjUeOv7jzAW"}},{"cell_type":"code","execution_count":null,"source":["# Only keep the feature with the highest coefficient\r\n","X = diabetes_df[['glucose']]\r\n","\r\n","# Performs a 25-75% train test split\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n","\r\n","# Scales features and fits the logistic regression model to the data\r\n","lr.fit(scaler.fit_transform(X_train), y_train)\r\n","\r\n","# Calculates the accuracy on the test set and prints coefficients\r\n","acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\r\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) \r\n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"outputs":[{"output_type":"stream","name":"stdout","text":["75.5% accuracy on test set.\n","{'glucose': 1.28}\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nVPuod-Dj15u","executionInfo":{"status":"ok","timestamp":1611410683131,"user_tz":180,"elapsed":989,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"4ed20c97-91b7-4f54-9408-e9484c51ed71"}},{"cell_type":"markdown","source":["**Removing all but one feature only reduced the accuracy by a few percent.**"],"metadata":{"id":"y5am1U3ikChN"}},{"cell_type":"markdown","source":["### Automatic Recursive Feature Elimination\r\n"],"metadata":{"id":"9o87fZklkFM8"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now let's automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features.</p>\r\n","<p>All the necessary functions and packages have been pre-loaded and the features have been scaled for you.</p></div>"],"metadata":{"id":"3uR53nmZkJTL"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.feature_selection import RFE"],"outputs":[],"metadata":{"id":"NGWzGT4ylN18"}},{"cell_type":"code","execution_count":null,"source":["X, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n","\r\n","#lr = LogisticRegression()\r\n","X_train_std = scaler.fit_transform(X_train)\r\n","#lr.fit(X_train_std, y_train)\r\n","X_test_std = scaler.transform(X_test)"],"outputs":[],"metadata":{"id":"Tup1Atu6lyf_"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create the RFE with a <code>LogisticRegression()</code> estimator and 3 features to select.</li>\r\n","<li>Print the features and their ranking.</li>\r\n","<li>Print the features that are not eliminated.</li>\r\n","</ul>"],"metadata":{"id":"sV8tHMKfkLMT"}},{"cell_type":"code","execution_count":null,"source":["# Create the RFE with a LogisticRegression estimator and 3 features to select\r\n","rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\r\n","\r\n","# Fits the eliminator to the data\r\n","rfe.fit(X_train_std, y_train)\r\n","\r\n","# Print the features and their ranking (high = dropped early on)\r\n","print(dict(zip(X.columns, rfe.ranking_)))\r\n","\r\n","# Print the features that are not eliminated\r\n","print(X.columns[rfe.support_])\r\n","#abs(lr.coef_[0])\r\n","# Calculates the test set accuracy\r\n","acc = accuracy_score(y_test, rfe.predict(X_test_std))\r\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) "],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting estimator with 8 features.\n","Fitting estimator with 7 features.\n","Fitting estimator with 6 features.\n","Fitting estimator with 5 features.\n","Fitting estimator with 4 features.\n","{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}\n","Index(['glucose', 'bmi', 'age'], dtype='object')\n","80.6% accuracy on test set.\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OK5dr2u5k8TH","executionInfo":{"status":"ok","timestamp":1611411930799,"user_tz":180,"elapsed":635,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"4edec9fa-893e-46b9-bf2c-c77db9d9f4e4"}},{"cell_type":"markdown","source":["**When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set.**"],"metadata":{"id":"De1q5JKsk-b9"}},{"cell_type":"markdown","source":["## Tree-based feature selection\r\n"],"metadata":{"id":"00JS3yUEpB32"}},{"cell_type":"markdown","source":["### Building a random forest model\r\n"],"metadata":{"id":"Qq8SqyNBqJlm"}},{"cell_type":"markdown","source":["<div class=\"\"><p>You'll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You'll fit the model on the training data after performing the train-test split and consult the feature importance values.</p>\r\n","<p>The feature and target datasets have been pre-loaded for you as <code>X</code> and <code>y</code>. Same goes for the necessary packages and functions.</p></div>"],"metadata":{"id":"69r2KeT-qNUF"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.ensemble import RandomForestClassifier"],"outputs":[],"metadata":{"id":"_nX7JoqxrcU-"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Set a 25% test size to perform a 75%-25% train-test split.</li>\r\n","<li>Fit the random forest classifier to the training data.</li>\r\n","<li>Calculate the accuracy on the test set.</li>\r\n","<li>Print the feature importances per feature.</li>\r\n","</ul>"],"metadata":{"id":"mNnIiZ3pqQJt"}},{"cell_type":"code","execution_count":null,"source":["# Perform a 75% training and 25% test data split\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n","\r\n","# Fit the random forest model to the training data\r\n","rf = RandomForestClassifier(random_state=0, n_estimators = 10)\r\n","rf.fit(X_train, y_train)\r\n","\r\n","# Calculate the accuracy\r\n","acc = accuracy_score(y_test, rf.predict(X_test))\r\n","\r\n","# Print the importances per feature\r\n","print(dict(zip(X.columns, rf.feature_importances_.round(2))))\r\n","\r\n","# Print accuracy\r\n","print(\"{0:.1%} accuracy on test set.\".format(acc))"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'pregnant': 0.09, 'glucose': 0.21, 'diastolic': 0.08, 'triceps': 0.11, 'insulin': 0.13, 'bmi': 0.09, 'family': 0.12, 'age': 0.16}\n","77.6% accuracy on test set.\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BovIE2serYY0","executionInfo":{"status":"ok","timestamp":1611413625870,"user_tz":180,"elapsed":625,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"afe8e83f-b750-4dc3-9acb-8b3a30dc0788"}},{"cell_type":"markdown","source":["**The random forest model gets 78% accuracy on the test set and 'glucose' is the most important feature (0.21).**"],"metadata":{"id":"03kpBNPtrojS"}},{"cell_type":"markdown","source":["## Random forest for feature selection"],"metadata":{"id":"R-9iPTk_s_6I"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now lets use the fitted random model to select the most important features from our input dataset <code>X</code>.</p>\r\n","<p>The trained model from the previous exercise has been pre-loaded for you as <code>rf</code>.</p></div>"],"metadata":{"id":"NHdMB43etE6v"}},{"cell_type":"markdown","source":["Instructions 1/2\r\n","<li>Create a mask for features with an importance higher than 0.15.</li>"],"metadata":{"id":"tlNm0VSrtHHA"}},{"cell_type":"code","execution_count":null,"source":["# Create a mask for features importances above the threshold\r\n","mask = rf.feature_importances_ > 0.15\r\n","\r\n","# Prints out the mask\r\n","print(mask)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[False  True False False False False False  True]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wn2IBiratERf","executionInfo":{"status":"ok","timestamp":1611413311234,"user_tz":180,"elapsed":641,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"68441287-b4b2-4c86-cbf0-1aee71133b9b"}},{"cell_type":"markdown","source":["Instructions 2/2\r\n","<li>Sub-select the most important features by applying the mask to <code>X</code>.</li>"],"metadata":{"id":"od_2YTJTuTJm"}},{"cell_type":"code","execution_count":null,"source":["# Apply the mask to the feature dataset X\r\n","reduced_X = X.loc[:, mask]\r\n","\r\n","# prints out the selected column names\r\n","print(reduced_X.columns)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['glucose', 'age'], dtype='object')\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXxDFI1auJjH","executionInfo":{"status":"ok","timestamp":1611413371739,"user_tz":180,"elapsed":610,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"72fc13c7-e0ea-4919-ce78-533c6a4c41cd"}},{"cell_type":"markdown","source":["**Only the features 'glucose' and 'age' were considered sufficiently important.**"],"metadata":{"id":"OsNywFytuNQ-"}},{"cell_type":"markdown","source":["### Recursive Feature Elimination with random forests\r\n"],"metadata":{"id":"1bOaV42zudz1"}},{"cell_type":"markdown","source":["<div class=\"\"><p>You'll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others.</p>\r\n","<p>You'll need these pre-loaded datasets: <code>X</code>, <code>X_train</code>, <code>y_train</code>.</p>\r\n","<p>Functions and classes that have been pre-loaded for you are: <code>RandomForestClassifier()</code>, <code>RFE()</code>, <code>train_test_split()</code>.</p></div>"],"metadata":{"id":"wa1LY1Zsuhp0"}},{"cell_type":"markdown","source":["Instructions 1/4\r\n","<li>Create a recursive feature eliminator that will select the 2 most important features using a random forest model.</li>"],"metadata":{"id":"67hhy0_NusEz"}},{"cell_type":"code","execution_count":null,"source":["# Wrap the feature eliminator around the random forest model\r\n","rfe = RFE(estimator=RandomForestClassifier(n_estimators = 10), n_features_to_select=2, verbose=1)"],"outputs":[],"metadata":{"id":"JkQZyH9xu0TK"}},{"cell_type":"markdown","source":["Instructions 2/4\r\n","<li>Fit the recursive feature eliminator to the training data.</li>"],"metadata":{"id":"tVbHGKf4utIE"}},{"cell_type":"code","execution_count":null,"source":["# Fit the model to the training data\r\n","rfe.fit(X_train, y_train)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting estimator with 8 features.\n","Fitting estimator with 7 features.\n","Fitting estimator with 6 features.\n","Fitting estimator with 5 features.\n","Fitting estimator with 4 features.\n","Fitting estimator with 3 features.\n"]},{"output_type":"execute_result","data":{"text/plain":["RFE(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","            max_depth=None, max_features='auto', max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n","            oob_score=False, random_state=None, verbose=0,\n","            warm_start=False),\n","  n_features_to_select=2, step=1, verbose=1)"]},"metadata":{"tags":[]},"execution_count":22}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWjGyzdyu94q","executionInfo":{"status":"ok","timestamp":1611413702934,"user_tz":180,"elapsed":661,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"033ea3e3-afa5-4f29-c283-f8fed78d14dd"}},{"cell_type":"markdown","source":["Instructions 3/4\r\n","<li>Create a mask using the fitted eliminator, then apply it to the feature dataset <code>X</code>.</li>"],"metadata":{"id":"kULLHGCtuuVj"}},{"cell_type":"code","execution_count":null,"source":["# Create a mask using an attribute of rfe\r\n","mask = rfe.support_\r\n","\r\n","# Apply the mask to the feature dataset X and print the result\r\n","reduced_X = X.loc[:, mask]\r\n","print(reduced_X.columns)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['glucose', 'insulin'], dtype='object')\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDcv6BHFvz7z","executionInfo":{"status":"ok","timestamp":1611413801315,"user_tz":180,"elapsed":720,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"719485eb-a579-4681-a663-44087a4a66a6"}},{"cell_type":"markdown","source":["Instructions 4/4\r\n","<li>Change the settings of <code>RFE()</code> to eliminate 2 features at each <code>step</code>.</li>"],"metadata":{"id":"k7yFCyFbuvZa"}},{"cell_type":"code","execution_count":null,"source":["# Set the feature eliminator to remove 2 features on each step\r\n","rfe = RFE(estimator=RandomForestClassifier(n_estimators = 10), n_features_to_select=2, step=2, verbose=1)\r\n","\r\n","# Fit the model to the training data\r\n","rfe.fit(X_train, y_train)\r\n","\r\n","# Create a mask\r\n","mask = rfe.support_\r\n","\r\n","# Apply the mask to the feature dataset X and print the result\r\n","reduced_X = X.loc[:, mask]\r\n","print(reduced_X.columns)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting estimator with 8 features.\n","Fitting estimator with 6 features.\n","Fitting estimator with 4 features.\n","Index(['glucose', 'insulin'], dtype='object')\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eo6UP3v8wBrZ","executionInfo":{"status":"ok","timestamp":1611413927229,"user_tz":180,"elapsed":665,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e30e7792-a7ac-4924-c3bc-64a89855c3bb"}},{"cell_type":"markdown","source":["**Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different.**"],"metadata":{"id":"kRSLsLGSwVH8"}},{"cell_type":"markdown","source":["## Regularized linear regression"],"metadata":{"id":"_MCKym2RyPow"}},{"cell_type":"markdown","source":["### Creating a LASSO regressor\r\n","<div class=\"\"><p>You'll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the pre-imported <code>Lasso()</code> regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.</p>\r\n","<p>You'll standardize the data first using the <code>StandardScaler()</code> that has been instantiated for you as <code>scaler</code> to make sure all coefficients face a comparable regularizing force trying to bring them down.</p>\r\n","<p>All necessary functions and classes plus the input datasets <code>X</code> and <code>y</code> have been pre-loaded.</p></div>"],"metadata":{"id":"gJzhVCseySV4"}},{"cell_type":"code","execution_count":null,"source":["ansur_bmi = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/7-dimensionality-reduction-in-python/datasets/ansur_bmi.csv')\r\n","X, y = ansur_bmi.iloc[:, :-1], ansur_bmi.iloc[:, -1]\r\n","from sklearn.linear_model import Lasso"],"outputs":[],"metadata":{"id":"u6ShBKl-0O2q"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Set the test size to 30% to get a 70-30% train test split.</li>\r\n","<li>Fit the scaler on the training features and transform these in one go.</li>\r\n","<li>Create the Lasso model.</li>\r\n","<li>Fit it to the scaled training data.</li>\r\n","</ul>"],"metadata":{"id":"ODjEQthTybqX"}},{"cell_type":"code","execution_count":null,"source":["# Set the test size to 30% to get a 70-30% train test split\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\r\n","\r\n","# Fit the scaler on the training features and transform these in one go\r\n","X_train_std = scaler.fit_transform(X_train)\r\n","\r\n","# Create the Lasso model\r\n","la = Lasso()\r\n","\r\n","# Fit it to the standardized training data\r\n","la.fit(X_train_std, y_train)"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n","  return self.partial_fit(X, y)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n","  return self.fit(X, **fit_params).transform(X)\n"]},{"output_type":"execute_result","data":{"text/plain":["Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n","   normalize=False, positive=False, precompute=False, random_state=None,\n","   selection='cyclic', tol=0.0001, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":30}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xVBaoufyfXv","executionInfo":{"status":"ok","timestamp":1611415145688,"user_tz":180,"elapsed":625,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"304988f4-e6cc-4a31-a990-8d537d663a42"}},{"cell_type":"markdown","source":["**You've fitted the Lasso model to the standardized training data. Now let's look at the results!**"],"metadata":{"id":"vXpqKg1W1pDc"}},{"cell_type":"markdown","source":["### Lasso model results\r\n","<div class=\"\"><p>Now that you've trained the Lasso model, you'll score its predictive capacity (<mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"0\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D445 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R²</mi></msup></math></mjx-assistive-mml></mjx-container>) on the test set and count how many features are ignored because their coefficient is reduced to zero.</p>\r\n","<p>The <code>X_test</code> and <code>y_test</code> datasets have been pre-loaded for you.</p>\r\n","<p>The <code>Lasso()</code> model and <code>StandardScaler()</code> have been instantiated as <code>la</code> and <code>scaler</code> respectively and both were fitted to the training data.</p></div>"],"metadata":{"id":"cye4YD1C1vlV"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Transform the test set with the pre-fitted scaler.</li>\r\n","<li>Calculate the <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"1\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D445 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container> value on the scaled test data.</li>\r\n","<li>Create a list that has True values when coefficients equal 0.</li>\r\n","<li>Calculate the total number of features with a coefficient of 0.</li>\r\n","</ul>"],"metadata":{"id":"qnbSVGQ12VFx"}},{"cell_type":"code","execution_count":null,"source":["# Transform the test set with the pre-fitted scaler\r\n","X_test_std = scaler.transform(X_test)\r\n","\r\n","# Calculate the coefficient of determination (R squared) on X_test_std\r\n","r_squared = la.score(X_test_std, y_test)\r\n","print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\r\n","\r\n","# Create a list that has True values when coefficients equal 0\r\n","zero_coef = la.coef_ == 0\r\n","\r\n","# Calculate how many features have a zero coefficient\r\n","n_ignored = sum(zero_coef)\r\n","print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["The model can predict 84.7% of the variance in the test set.\n","The model has ignored 82 out of 91 features.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n","  \n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4NT0DPjP3nNv","executionInfo":{"status":"ok","timestamp":1611415846002,"user_tz":180,"elapsed":655,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e085f01f-daf6-4779-95d4-4c669a4e94ee"}},{"cell_type":"markdown","source":["**We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The R^2 could be higher though.**"],"metadata":{"id":"amSOf1o03sgP"}},{"cell_type":"markdown","source":["### Adjusting the regularization strength\r\n"],"metadata":{"id":"6b7XPvja32Ll"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Your current Lasso model has an <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"2\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D445 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R²</mi></msup></math></mjx-assistive-mml></mjx-container> score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.</p>\r\n","<p>Let's improve the balance between predictive power and model simplicity by tweaking the <code>alpha</code> parameter.</p></div>"],"metadata":{"id":"fV_lNoFO38CU"}},{"cell_type":"markdown","source":["Instructions\r\n","<li>Find the <strong>highest</strong> value for <code>alpha</code> that gives an <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"3\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D445 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container> value above 98% from the options: <code>1</code>, <code>0.5</code>, <code>0.1</code>, and <code>0.01</code>.</li>"],"metadata":{"id":"McpffsXT4CjD"}},{"cell_type":"code","execution_count":null,"source":["# Find the highest alpha value with R-squared above 98%\r\n","la = Lasso(0.1, random_state=0)\r\n","\r\n","# Fits the model and calculates performance stats\r\n","la.fit(X_train_std, y_train)\r\n","r_squared = la.score(X_test_std, y_test)\r\n","n_ignored_features = sum(la.coef_ == 0)\r\n","\r\n","# Print peformance stats \r\n","print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\r\n","print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["The model can predict 98.3% of the variance in the test set.\n","64 out of 91 features were ignored.\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXH66mOV4bRx","executionInfo":{"status":"ok","timestamp":1611416359630,"user_tz":180,"elapsed":840,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"4d52ab25-d626-488d-ec84-70495a4f6803"}},{"cell_type":"markdown","source":["**With this more appropriate regularization strength we can predict 98% of the variance in the BMI value while ignoring 2/3 of the features**"],"metadata":{"id":"PA5EGNgk4hyQ"}},{"cell_type":"markdown","source":["## Combining feature selectors\r\n"],"metadata":{"id":"fsdF2QnB7g9R"}},{"cell_type":"markdown","source":["### Creating a LassoCV regressor"],"metadata":{"id":"GBj77_j2dkSM"}},{"cell_type":"markdown","source":["<div class=\"\"><p>You'll be predicting biceps circumference on a subsample of the male ANSUR dataset using the <code>LassoCV()</code> regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation.</p>\r\n","<p>The standardized training and test data has been pre-loaded for you as <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>.</p></div>"],"metadata":{"id":"aYaNiwxWddpW"}},{"cell_type":"code","execution_count":null,"source":["biceps_df = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/7-dimensionality-reduction-in-python/datasets/biceps_df.csv')"],"outputs":[],"metadata":{"id":"T90y4qhAfzCd"}},{"cell_type":"code","execution_count":null,"source":["X, y = biceps_df.iloc[:, :-1], biceps_df.iloc[:, -1]\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\r\n","\r\n","scaler = StandardScaler()\r\n","X_train = scaler.fit_transform(X_train)\r\n","X_test = scaler.transform(X_test)"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.partial_fit(X, y)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.fit(X, **fit_params).transform(X)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  \n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xx55_1FPgBvx","executionInfo":{"status":"ok","timestamp":1611428817548,"user_tz":180,"elapsed":843,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d08b4458-ba9d-4271-8480-688608675132"}},{"cell_type":"markdown","source":["Instructions\r\n","\r\n","<ul>\r\n","<li>Create and fit the LassoCV model on the training set.</li>\r\n","<li>Calculate <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"6\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D445 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container> on the test set.</li>\r\n","<li>Create a mask for coefficients not equal to zero.</li>\r\n","</ul>"],"metadata":{"id":"3St-wKHAdnxF"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.linear_model import LassoCV\r\n","\r\n","# Create and fit the LassoCV model on the training set\r\n","lcv = LassoCV(cv = 3)\r\n","lcv.fit(X_train, y_train)\r\n","print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\r\n","\r\n","# Calculate R squared on the test set\r\n","r_squared = lcv.score(X_test, y_test)\r\n","print('The model explains {0:.1%} of the test set variance'.format(r_squared))\r\n","\r\n","# Create a mask for coefficients not equal to zero\r\n","lcv_mask = lcv.coef_ != 0\r\n","print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal alpha = 0.089\n","The model explains 88.2% of the test set variance\n","26 features out of 32 selected\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQUq-o_mfG44","executionInfo":{"status":"ok","timestamp":1611428820782,"user_tz":180,"elapsed":838,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"66de7633-cc22-492d-dfd4-97c9a1eab154"}},{"cell_type":"markdown","source":["**We got a decent R squared and removed 6 features. We'll save the lcv_mask for later on.**"],"metadata":{"id":"yIBdKHFffJLY"}},{"cell_type":"markdown","source":["### Ensemble models for extra votes\r\n"],"metadata":{"id":"k0gc7Px2h5Mk"}},{"cell_type":"markdown","source":["<div class=\"\"><p>The <code>LassoCV()</code> model selected 26 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let's use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE).</p>\r\n","<p>The standardized training and test data has been pre-loaded for you as <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>.</p></div>"],"metadata":{"id":"ZiH4g0ipiILR"}},{"cell_type":"markdown","source":["Instructions 1/4\r\n","<li>Select 10 features with RFE on a <code>GradientBoostingRegressor</code> and drop 3 features on each step.</li>"],"metadata":{"id":"v6GkSBaGiKh5"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.feature_selection import RFE\r\n","from sklearn.ensemble import GradientBoostingRegressor\r\n","\r\n","# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\r\n","rfe_gb = RFE(estimator=GradientBoostingRegressor(), \r\n","             n_features_to_select=10, step=3, verbose=1)\r\n","rfe_gb.fit(X_train, y_train)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting estimator with 32 features.\n","Fitting estimator with 29 features.\n","Fitting estimator with 26 features.\n","Fitting estimator with 23 features.\n","Fitting estimator with 20 features.\n","Fitting estimator with 17 features.\n","Fitting estimator with 14 features.\n","Fitting estimator with 11 features.\n"]},{"output_type":"execute_result","data":{"text/plain":["RFE(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n","             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n","             max_leaf_nodes=None, min_impurity_decrease=0.0,\n","             min_impurity_split=None, min_samples_leaf=1,\n","             min_sampl...=None, subsample=1.0, tol=0.0001,\n","             validation_fraction=0.1, verbose=0, warm_start=False),\n","  n_features_to_select=10, step=3, verbose=1)"]},"metadata":{"tags":[]},"execution_count":57}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHyQRlDXiHPR","executionInfo":{"status":"ok","timestamp":1611428825295,"user_tz":180,"elapsed":1583,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c5c8a46c-6f52-478b-f71c-2d866b62dafa"}},{"cell_type":"markdown","source":["Instructions 2/4\r\n","<li>Calculate the <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"7\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D445 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R²</mi></msup></math></mjx-assistive-mml></mjx-container> on the test set.</li>"],"metadata":{"id":"fiHnNUxYitKU"}},{"cell_type":"code","execution_count":null,"source":["# Calculate the R squared on the test set\r\n","r_squared = rfe_gb.score(X_test, y_test)\r\n","print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"],"outputs":[{"output_type":"stream","name":"stdout","text":["The model can explain 85.6% of the variance in the test set\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUqrsDhMi89L","executionInfo":{"status":"ok","timestamp":1611428829865,"user_tz":180,"elapsed":824,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"33cbfc55-fb5d-485d-9dad-814a4cef1e38"}},{"cell_type":"markdown","source":["Instructions 3/4\r\n","<li>Assign the support array of the fitted model to <code>gb_mask</code>.</li>"],"metadata":{"id":"EerqMb9pjNi5"}},{"cell_type":"code","execution_count":null,"source":["# Assign the support array to gb_mask\r\n","gb_mask = rfe_gb.support_"],"outputs":[],"metadata":{"id":"6S-OZsmDjMpC"}},{"cell_type":"markdown","source":["Instructions 4/4\r\n","<li>Modify the first step to select 10 features with RFE on a <strong><code>RandomForestRegressor()</code></strong> and drop 3 features on each step.</li>"],"metadata":{"id":"lbnqLVe4j0Lk"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.ensemble import RandomForestRegressor\r\n","\r\n","# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\r\n","rfe_rf = RFE(estimator=RandomForestRegressor(n_estimators = 10), \r\n","             n_features_to_select=10, step=3, verbose=1)\r\n","rfe_rf.fit(X_train, y_train)\r\n","\r\n","# Calculate the R squared on the test set\r\n","r_squared = rfe_rf.score(X_test, y_test)\r\n","print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\r\n","\r\n","# Assign the support array to gb_mask\r\n","rf_mask = rfe_rf.support_"],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting estimator with 32 features.\n","Fitting estimator with 29 features.\n","Fitting estimator with 26 features.\n","Fitting estimator with 23 features.\n","Fitting estimator with 20 features.\n","Fitting estimator with 17 features.\n","Fitting estimator with 14 features.\n","Fitting estimator with 11 features.\n","The model can explain 83.1% of the variance in the test set\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glUkHBCwkWxQ","executionInfo":{"status":"ok","timestamp":1611428846179,"user_tz":180,"elapsed":1687,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"a4aee5e5-092d-4321-f138-ffc940fdb8df"}},{"cell_type":"markdown","source":["**Inluding the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important.**"],"metadata":{"id":"w4l5iQyskfO3"}},{"cell_type":"markdown","source":["### Combining 3 feature selectors\r\n"],"metadata":{"id":"mLqfeYIsk9M0"}},{"cell_type":"markdown","source":["<div class=\"\"><p>We'll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.</p>\r\n","<p>The per model votes have been pre-loaded as <code>lcv_mask</code>, <code>rf_mask</code>, and <code>gb_mask</code> and the feature and target datasets as <code>X</code> and <code>y</code>.</p></div>"],"metadata":{"id":"IlpzaET5lA83"}},{"cell_type":"markdown","source":["Instructions 1/4\r\n","<li>Sum the votes of the three models using <code>np.sum()</code>.</li>"],"metadata":{"id":"5DkkRGMDlFJT"}},{"cell_type":"code","execution_count":null,"source":["# Sum the votes of the three models\r\n","votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\r\n","print(votes)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 3 3 0 1 0 3 1 1 1 3 1 1 2 2 0 1 1 2 0 1 3 1 0 3 2 1 2 1 2 3]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNtw2DtwlVpp","executionInfo":{"status":"ok","timestamp":1611428851099,"user_tz":180,"elapsed":946,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"22f0a9fc-2a0f-405e-934d-6b21d30f3ee2"}},{"cell_type":"markdown","source":["Instructions 2/4\r\n","<li>Create a mask for features selected by all 3 models.</li>"],"metadata":{"id":"vdfwu_BZlg-f"}},{"cell_type":"code","execution_count":null,"source":["# Create a mask for features selected by all 3 models\r\n","meta_mask = votes >= 3\r\n","print(meta_mask)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[False False  True  True False False False  True False False False  True\n"," False False False False False False False False False False  True False\n"," False  True False False False False False  True]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPfcvdOCmKPq","executionInfo":{"status":"ok","timestamp":1611428855546,"user_tz":180,"elapsed":959,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"df2c0bc3-c5b1-4081-c252-da3b01946e8d"}},{"cell_type":"markdown","source":["Instructions 3/4\r\n","<li>Apply the dimensionality reduction on X and print which features were selected.</li>"],"metadata":{"id":"wG5f3cW6lIZN"}},{"cell_type":"code","execution_count":null,"source":["# Apply the dimensionality reduction on X\r\n","X_reduced = X.loc[:, meta_mask]\r\n","print(X_reduced.columns)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference',\n","       'forearmcircumferenceflexed', 'shouldercircumference',\n","       'thighcircumference', 'BMI'],\n","      dtype='object')\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbauBL-_mpxo","executionInfo":{"status":"ok","timestamp":1611428863136,"user_tz":180,"elapsed":839,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"6e6499c5-b16c-49bc-c408-db73679e71a1"}},{"cell_type":"markdown","source":["Instructions 4/4\r\n","<li>Plug the reduced dataset into the code for simple linear regression that has been written for you.</li>"],"metadata":{"id":"-phySEUClJns"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.linear_model import LinearRegression\r\n","lm = LinearRegression()"],"outputs":[],"metadata":{"id":"MVz7Bs3ynQ6y"}},{"cell_type":"code","execution_count":null,"source":["# Plug the reduced dataset into a linear regression pipeline\r\n","X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\r\n","lm.fit(scaler.fit_transform(X_train), y_train)\r\n","r_squared = lm.score(scaler.transform(X_test), y_test)"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.partial_fit(X, y)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.fit(X, **fit_params).transform(X)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  after removing the cwd from sys.path.\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z95DSICXk9Cl","executionInfo":{"status":"ok","timestamp":1611428879461,"user_tz":180,"elapsed":846,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"7f430d72-5b61-4942-d43e-1027587113e8"}},{"cell_type":"code","execution_count":null,"source":["print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["The model can explain 86.7% of the variance in the test set using 7 features.\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXkAmRRyoLh0","executionInfo":{"status":"ok","timestamp":1611428882062,"user_tz":180,"elapsed":978,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c149f22a-1aa8-441a-b07a-8e919981dd18"}},{"cell_type":"markdown","source":["**Using the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy!**"],"metadata":{"id":"9jDh9Tg1pazF"}}]}