{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"3-fine-tuning-your-xgboost-model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMVP/qCi08av66iakqEvgXt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Fine-tuning your XGBoost model\r\n",">  This chapter will teach you how to make your XGBoost models as performant as possible. You'll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Datacamp, XGBoost]\r\n","- image: images/datacamp/___"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 3 exercises \"Extreme Gradient Boosting with XGBoost\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import xgboost as xgb\r\n","\r\n","plt.rcParams['figure.figsize'] = (8, 8)"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1612569627070,"user_tz":180,"elapsed":1061,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Why tune your model?"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### When is tuning your model a bad idea?"],"metadata":{"id":"dAFz5REFmkNn"}},{"cell_type":"markdown","source":["<p>Now that you've seen the effect that tuning has on the overall performance of your XGBoost model, let's turn the question on its head and see if you can figure out when tuning your model might not be the best idea. <strong>Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model</strong>?</p>"],"metadata":{"id":"mBlDW3tWmmw0"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","You have lots of examples from some dataset and very many features at your disposal.\r\n","\r\n","<b>You are very short on time before you must push an initial model to production and have little data to train your model on.</b>\r\n","\r\n","You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints.\r\n","\r\n","You must squeeze out every last bit of performance out of your xgboost model.\r\n","\r\n","</pre>"],"metadata":{"id":"q18AAvcNmpSQ"}},{"cell_type":"markdown","source":["**You cannot tune if you do not have time!**"],"metadata":{"id":"x2GXkrF4mvPR"}},{"cell_type":"markdown","source":["### Tuning the number of boosting rounds"],"metadata":{"id":"3x5mIyllm5Sj"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use <code>xgb.cv()</code> inside a <code>for</code> loop and build one model per <code>num_boost_round</code> parameter.</p>\r\n","<p>Here, you'll continue working with the Ames housing dataset. The features are available in the array <code>X</code>, and the target vector is contained in <code>y</code>.</p></div>"],"metadata":{"id":"dzYCw1pem7Z8"}},{"cell_type":"code","execution_count":2,"source":["df = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/05-extreme-gradient-boosting-with-xgboost/datasets/boston_1460x57.csv')"],"outputs":[],"metadata":{"id":"3AxSzF8TC0ZC","executionInfo":{"status":"ok","timestamp":1612569579550,"user_tz":180,"elapsed":1246,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"code","execution_count":3,"source":["X, y = df.iloc[:, :-1], df.iloc[:, -1]"],"outputs":[],"metadata":{"id":"EiorOtWIn6Bn","executionInfo":{"status":"ok","timestamp":1612569586951,"user_tz":180,"elapsed":876,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a <code>DMatrix</code> called <code>housing_dmatrix</code> from <code>X</code> and <code>y</code>.</li>\r\n","<li>Create a parameter dictionary called <code>params</code>, passing in the appropriate <code>\"objective\"</code> (<code>\"reg:linear\"</code>) and <code>\"max_depth\"</code> (set it to <code>3</code>).</li>\r\n","<li>Iterate over <code>num_rounds</code> inside a <code>for</code> loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (<code>curr_num_rounds</code>) to <code>xgb.cv()</code> as the argument to <code>num_boost_round</code>. </li>\r\n","<li>Append the final boosting round RMSE for each cross-validated XGBoost model to the <code>final_rmse_per_round</code> list.</li>\r\n","<li><code>num_rounds</code> and <code>final_rmse_per_round</code> have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!</li>\r\n","</ul>"],"metadata":{"id":"23cph3bQm9ml"}},{"cell_type":"code","execution_count":8,"source":["# Create the DMatrix: housing_dmatrix\r\n","housing_dmatrix = xgb.DMatrix(X, y)\r\n","\r\n","# Create the parameter dictionary for each tree: params \r\n","params = {\"objective\":\"reg:squarederror\", \"max_depth\":3} #reg:linear\r\n","\r\n","# Create list of number of boosting rounds\r\n","num_rounds = [5, 10, 15]\r\n","\r\n","# Empty list to store final round rmse per XGBoost model\r\n","final_rmse_per_round = []\r\n","\r\n","# Iterate over num_rounds and build one model per num_boost_round parameter\r\n","for curr_num_rounds in num_rounds:\r\n","\r\n","    # Perform cross-validation: cv_results\r\n","    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\r\n","    \r\n","    # Append final round RMSE\r\n","    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n","\r\n","# Print the resultant DataFrame\r\n","num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\r\n","print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["   num_boosting_rounds          rmse\n","0                    5  50903.299479\n","1                   10  34774.195313\n","2                   15  32895.096354\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgfoIe_lnDVX","executionInfo":{"status":"ok","timestamp":1612569650271,"user_tz":180,"elapsed":1015,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"058422a9-c34c-48d2-8c14-26f60e50de83"}},{"cell_type":"markdown","source":["**As you can see, increasing the number of boosting rounds decreases the RMSE.**"],"metadata":{"id":"Un8-P3eTpIW3"}},{"cell_type":"markdown","source":["### Automated boosting round selection using early_stopping"],"metadata":{"id":"CRdOjLRHpLoW"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within <code>xgb.cv()</code>. This is done using a technique called <strong>early stopping</strong>. </p>\r\n","<p><strong>Early stopping</strong> works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (<code>\"rmse\"</code> in our case) does not improve for a given number of rounds. Here you will use the <code>early_stopping_rounds</code> parameter in <code>xgb.cv()</code> with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when <code>num_boost_rounds</code> is reached, then early stopping does not occur.</p>\r\n","<p>Here, the <code>DMatrix</code> and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!</p></div>"],"metadata":{"id":"VBUaJ5BZpOEm"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Perform 3-fold cross-validation with early stopping and <code>\"rmse\"</code> as your metric. Use <code>10</code> early stopping rounds and <code>50</code> boosting rounds. Specify a <code>seed</code> of <code>123</code> and make sure the output is a <code>pandas</code> DataFrame. Remember to specify the other parameters such as <code>dtrain</code>, <code>params</code>, and <code>metrics</code>.</li>\r\n","<li>Print <code>cv_results</code>.</li>\r\n","</ul>"],"metadata":{"id":"juivABTvpQQc"}},{"cell_type":"code","execution_count":10,"source":["# Create your housing DMatrix: housing_dmatrix\r\n","housing_dmatrix = xgb.DMatrix(data=X, label=y)\r\n","\r\n","# Create the parameter dictionary for each tree: params\r\n","params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\r\n","\r\n","# Perform cross-validation with early stopping: cv_results\r\n","cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=10, num_boost_round=50, metrics=\"rmse\", as_pandas=True, seed=123)\r\n","    \r\n","\r\n","# Print cv_results\r\n","print(cv_results)"],"outputs":[{"output_type":"stream","name":"stdout","text":["    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n","0     141871.619792      403.625044   142640.651042     705.559164\n","1     103057.020833       73.773330   104907.661458     111.116005\n","2      75975.960937      253.731460    79262.057292     563.761707\n","3      57420.535156      521.648794    61620.139323    1087.688565\n","4      44552.957031      544.167830    50437.565104    1846.445052\n","5      35763.953125      681.798925    43035.660156    2034.472841\n","6      29861.465495      769.569137    38600.881510    2169.801278\n","7      25994.675781      756.523981    36071.817708    2109.795546\n","8      23306.836589      759.236242    34383.184896    1934.543836\n","9      21459.768880      745.624686    33509.141276    1887.375284\n","10     20148.722656      749.611655    32916.805990    1850.898680\n","11     19215.381510      641.388117    32197.832682    1734.456935\n","12     18627.388672      716.256238    31770.852213    1802.153746\n","13     17960.694010      557.041305    31482.781250    1779.125301\n","14     17559.735677      631.413474    31389.991537    1892.319150\n","15     17205.714518      590.170536    31302.882812    1955.164600\n","16     16876.569662      703.632344    31234.057943    1880.705610\n","17     16597.662435      703.677150    31318.347005    1828.859938\n","18     16330.460286      607.273919    31323.633464    1775.909418\n","19     16005.973959      520.470780    31204.137370    1739.076467\n","20     15814.302409      518.604369    31089.861328    1756.022288\n","21     15493.404297      505.618287    31047.997396    1624.671744\n","22     15270.734049      502.018038    31056.917969    1668.043415\n","23     15086.381836      503.912751    31024.986328    1548.984070\n","24     14917.608073      486.207907    30983.686198    1663.129317\n","25     14709.590169      449.667122    30989.474609    1686.665979\n","26     14457.286784      376.787440    30952.113281    1613.171766\n","27     14185.566732      383.100315    31066.901693    1648.534636\n","28     13934.066732      473.464654    31095.641276    1709.225654\n","29     13749.645183      473.671783    31103.887370    1778.880069\n","30     13549.836914      454.898141    30976.084635    1744.515271\n","31     13413.485026      399.603470    30938.470703    1746.053665\n","32     13275.915039      415.409554    30931.000000    1772.468197\n","33     13085.877604      493.792099    30929.057292    1765.542083\n","34     12947.181641      517.789025    30890.627604    1786.510976\n","35     12846.026693      547.733238    30884.492839    1769.731406\n","36     12702.378906      505.522629    30833.541667    1691.002487\n","37     12532.243490      508.297573    30856.688802    1771.444141\n","38     12384.054362      536.223530    30818.017578    1782.783711\n","39     12198.444010      545.164711    30839.391276    1847.327435\n","40     12054.583984      508.842426    30776.966146    1912.781988\n","41     11897.036133      477.177200    30794.701172    1919.674009\n","42     11756.221680      502.992782    30780.957031    1906.819991\n","43     11618.846354      519.839474    30783.754557    1951.258381\n","44     11484.079427      578.429413    30776.733073    1953.447688\n","45     11356.552409      565.368860    30758.543620    1947.457714\n","46     11193.558268      552.298848    30729.974609    1985.702549\n","47     11071.316081      604.089610    30732.661458    1966.997835\n","48     10950.778646      574.861783    30712.244141    1957.750653\n","49     10824.865560      576.666974    30720.854167    1950.511063\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5b3ZRwUpTuU","executionInfo":{"status":"ok","timestamp":1612569750112,"user_tz":180,"elapsed":1008,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"93bfdd3a-6961-4857-83dc-ea6e4ca63da1"}},{"cell_type":"markdown","source":["### Overview of XGBoost's hyperparameters"],"metadata":{"id":"2t0-yPXDp4ls"}},{"cell_type":"markdown","source":["### Tuning eta"],"metadata":{"id":"pxLOVjoEuCMo"}},{"cell_type":"markdown","source":["<div class=\"\"><p>It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the <code>\"eta\"</code>, also known as the learning rate.</p>\r\n","<p>The learning rate in XGBoost is a parameter that can range between <code>0</code> and <code>1</code>, with higher values of <code>\"eta\"</code> penalizing feature weights more strongly, causing much stronger regularization.</p></div>"],"metadata":{"id":"HyrgQ9yruG0x"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a list called <code>eta_vals</code> to store the following <code>\"eta\"</code> values: <code>0.001</code>, <code>0.01</code>, and <code>0.1</code>.</li>\r\n","<li>Iterate over your <code>eta_vals</code> list using a <code>for</code> loop.</li>\r\n","<li>In each iteration of the <code>for</code> loop, set the <code>\"eta\"</code> key of <code>params</code> to be equal to <code>curr_val</code>. Then, perform 3-fold cross-validation with early stopping (<code>5</code> rounds), <code>10</code> boosting rounds, a metric of <code>\"rmse\"</code>, and a <code>seed</code> of <code>123</code>. Ensure the output is a DataFrame.</li>\r\n","<li>Append the final round RMSE to the <code>best_rmse</code> list.</li>\r\n","</ul>"],"metadata":{"id":"IchTXOGwuI1q"}},{"cell_type":"code","execution_count":12,"source":["# Create your housing DMatrix: housing_dmatrix\r\n","housing_dmatrix = xgb.DMatrix(data=X, label=y)\r\n","\r\n","# Create the parameter dictionary for each tree (boosting round)\r\n","params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\r\n","\r\n","# Create list of eta values and empty list to store final round rmse per xgboost model\r\n","eta_vals = [0.001, 0.01, 0.1]\r\n","best_rmse = []\r\n","\r\n","# Systematically vary the eta \r\n","for curr_val in eta_vals:\r\n","\r\n","    params[\"eta\"] = curr_val\r\n","    \r\n","    # Perform cross-validation: cv_results\r\n","    cv_results = cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=5, num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\r\n","    \r\n","    \r\n","    \r\n","    \r\n","    # Append the final round rmse to best_rmse\r\n","    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n","\r\n","# Print the resultant DataFrame\r\n","print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["     eta      best_rmse\n","0  0.001  195736.416667\n","1  0.010  179932.161458\n","2  0.100   79759.416667\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tq1NBsRsuLMg","executionInfo":{"status":"ok","timestamp":1612571032937,"user_tz":180,"elapsed":1130,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"25c338d5-a558-479b-c6d9-c211d6f19e3d"}},{"cell_type":"markdown","source":["### Tuning max_depth"],"metadata":{"id":"ClkngHs_uYiH"}},{"cell_type":"markdown","source":["<p>In this exercise, your job is to tune <code>max_depth</code>, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.</p>"],"metadata":{"id":"NMzLDDY_ua2o"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a list called <code>max_depths</code> to store the following <code>\"max_depth\"</code> values: <code>2</code>, <code>5</code>, <code>10</code>, and <code>20</code>.</li>\r\n","<li>Iterate over your <code>max_depths</code> list using a <code>for</code> loop.</li>\r\n","<li>Systematically vary <code>\"max_depth\"</code> in each iteration of the <code>for</code> loop and perform 2-fold cross-validation with early stopping (<code>5</code> rounds), <code>10</code> boosting rounds, a metric of <code>\"rmse\"</code>, and a <code>seed</code> of <code>123</code>. Ensure the output is a DataFrame.</li>\r\n","</ul>"],"metadata":{"id":"9IN7JAL8ucrw"}},{"cell_type":"code","execution_count":14,"source":["# Create your housing DMatrix\r\n","housing_dmatrix = xgb.DMatrix(data=X,label=y)\r\n","\r\n","# Create the parameter dictionary\r\n","params = {\"objective\":\"reg:squarederror\"}\r\n","\r\n","# Create list of max_depth values\r\n","max_depths = [2, 5, 10, 20]\r\n","best_rmse = []\r\n","\r\n","# Systematically vary the max_depth\r\n","for curr_val in max_depths:\r\n","\r\n","    params[\"max_depth\"] = curr_val\r\n","    \r\n","    # Perform cross-validation\r\n","    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, early_stopping_rounds=5, num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\r\n","    \r\n","    \r\n","    \r\n","    \r\n","    # Append the final round rmse to best_rmse\r\n","    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n","\r\n","# Print the resultant DataFrame\r\n","print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["   max_depth     best_rmse\n","0          2  37957.472657\n","1          5  35596.595703\n","2         10  36065.548829\n","3         20  36739.576172\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i27crcDLugDh","executionInfo":{"status":"ok","timestamp":1612571103744,"user_tz":180,"elapsed":994,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"bab02c4d-c74e-47f7-8cf3-65d11ae3f622"}},{"cell_type":"markdown","source":["### Tuning colsample_bytree"],"metadata":{"id":"67VbzUq7upjU"}},{"cell_type":"markdown","source":["<p>Now, it's time to tune <code>\"colsample_bytree\"</code>. You've already seen this if you've ever worked with scikit-learn's <code>RandomForestClassifier</code> or <code>RandomForestRegressor</code>, where it just was called <code>max_features</code>. In both <code>xgboost</code> and <code>sklearn</code>, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In <code>xgboost</code>, <code>colsample_bytree</code> must be specified as a float between 0 and 1.</p>"],"metadata":{"id":"mlXdnTGOurg3"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a list called <code>colsample_bytree_vals</code> to store the values <code>0.1</code>, <code>0.5</code>, <code>0.8</code>, and <code>1</code>.</li>\r\n","<li>Systematically vary <code>\"colsample_bytree\"</code> and perform cross-validation, exactly as you did with <code>max_depth</code> and <code>eta</code> previously.</li>\r\n","</ul>"],"metadata":{"id":"Tesk1Mroutbs"}},{"cell_type":"code","execution_count":16,"source":["# Create your housing DMatrix\r\n","housing_dmatrix = xgb.DMatrix(data=X,label=y)\r\n","\r\n","# Create the parameter dictionary\r\n","params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\r\n","\r\n","# Create list of hyperparameter values: colsample_bytree_vals\r\n","colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\r\n","best_rmse = []\r\n","\r\n","# Systematically vary the hyperparameter value \r\n","for curr_val in colsample_bytree_vals:\r\n","\r\n","    params[\"colsample_bytree\"] = curr_val\r\n","    \r\n","    # Perform cross-validation\r\n","    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\r\n","                 num_boost_round=10, early_stopping_rounds=5,\r\n","                 metrics=\"rmse\", as_pandas=True, seed=123)\r\n","    \r\n","    # Append the final round rmse to best_rmse\r\n","    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n","\r\n","# Print the resultant DataFrame\r\n","print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["   colsample_bytree     best_rmse\n","0               0.1  48193.453125\n","1               0.5  36013.542969\n","2               0.8  35932.960938\n","3               1.0  35836.044922\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XstmyP8-uxqg","executionInfo":{"status":"ok","timestamp":1612571175541,"user_tz":180,"elapsed":1858,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"6d0664bc-44af-46b7-fd76-72b6120900a1"}},{"cell_type":"markdown","source":["**There are several other individual parameters that you can tune, such as \"subsample\", which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!**"],"metadata":{"id":"4KcUIaqDu7_D"}},{"cell_type":"markdown","source":["## Review of grid search and random search"],"metadata":{"id":"DwM5g_Jou_f6"}},{"cell_type":"markdown","source":["### Grid search with XGBoost"],"metadata":{"id":"JH-kjBGhvCZD"}},{"cell_type":"markdown","source":["<p>Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's <code>GridSearch</code> and <code>RandomizedSearch</code> capabilities with internal cross-validation using the <code>GridSearchCV</code> and <code>RandomizedSearchCV</code> functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with <code>GridSearchCV</code>!</p>"],"metadata":{"id":"8q61oNzkvFZc"}},{"cell_type":"code","execution_count":18,"source":["from sklearn.model_selection import GridSearchCV"],"outputs":[],"metadata":{"id":"wB6Roav7vNlf","executionInfo":{"status":"ok","timestamp":1612571272073,"user_tz":180,"elapsed":1246,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a parameter grid called <code>gbm_param_grid</code> that contains a list of <code>\"colsample_bytree\"</code> values (<code>0.3</code>, <code>0.7</code>), a list with a single value for <code>\"n_estimators\"</code> (<code>50</code>), and a list of 2 <code>\"max_depth\"</code> (<code>2</code>, <code>5</code>) values.</li>\r\n","<li>Instantiate an <code>XGBRegressor</code> object called <code>gbm</code>.</li>\r\n","<li>Create a <code>GridSearchCV</code> object called <code>grid_mse</code>, passing in: the parameter grid to <code>param_grid</code>, the <code>XGBRegressor</code> to <code>estimator</code>, <code>\"neg_mean_squared_error\"</code> to <code>scoring</code>, and <code>4</code> to <code>cv</code>. Also specify <code>verbose=1</code> so you can better understand the output.</li>\r\n","<li>Fit the <code>GridSearchCV</code> object to <code>X</code> and <code>y</code>.</li>\r\n","<li>Print the best parameter values and lowest RMSE, using the <code>.best_params_</code> and <code>.best_score_</code> attributes, respectively, of <code>grid_mse</code>.</li>\r\n","</ul>"],"metadata":{"id":"B_PNP6RYvHhN"}},{"cell_type":"code","execution_count":20,"source":["gbm"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n","             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]},"execution_count":20}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDV9s8eGvmR2","executionInfo":{"status":"ok","timestamp":1612571372141,"user_tz":180,"elapsed":716,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e52ab3c3-41cb-4a61-ae80-4326fbb4c166"}},{"cell_type":"code","execution_count":29,"source":["# Create the parameter grid: gbm_param_grid\r\n","gbm_param_grid = {\r\n","    'colsample_bytree': [0.3, 0.7],\r\n","    'n_estimators': [50],\r\n","    'max_depth': [2, 5]\r\n","}\r\n","\r\n","# Instantiate the regressor: gbm\r\n","gbm = xgb.XGBRegressor(objective='reg:squarederror')\r\n","\r\n","# Perform grid search: grid_mse\r\n","grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\r\n","scoring='neg_mean_squared_error', cv=4, verbose=1, n_jobs=-1)\r\n","\r\n","# Fit grid_mse to the data\r\n","grid_mse.fit(X, y)\r\n","\r\n","# Print the best parameters and lowest RMSE\r\n","print(\"Best parameters found: \", grid_mse.best_params_)\r\n","print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"]},{"output_type":"stream","name":"stdout","text":["Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n","Lowest RMSE found:  29916.562522854438\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    1.0s finished\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UBH02pzvJ21","executionInfo":{"status":"ok","timestamp":1612571634627,"user_tz":180,"elapsed":2069,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"188fb52e-1054-42fa-b5af-75f47935a647"}},{"cell_type":"markdown","source":["### Random search with XGBoost"],"metadata":{"id":"aaJmMcUdwxPE"}},{"cell_type":"markdown","source":["<p>Often, <code>GridSearchCV</code> can be really time consuming, so in practice, you may want to use <code>RandomizedSearchCV</code> instead, as you will do in this exercise. The good news is you only have to make a few modifications to your <code>GridSearchCV</code> code to do <code>RandomizedSearchCV</code>. The key difference is you have to specify a <code>param_distributions</code> parameter instead of a <code>param_grid</code> parameter.</p>"],"metadata":{"id":"QNMYUcNIwzYX"}},{"cell_type":"code","execution_count":31,"source":["from sklearn.model_selection import RandomizedSearchCV"],"outputs":[],"metadata":{"id":"08M6gPOyxBEZ","executionInfo":{"status":"ok","timestamp":1612571744497,"user_tz":180,"elapsed":793,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a parameter grid called <code>gbm_param_grid</code> that contains a list with a single value for <code>'n_estimators'</code> (<code>25</code>), and a list of <code>'max_depth'</code> values between <code>2</code> and <code>11</code> for <code>'max_depth'</code> - use <code>range(2, 12)</code> for this. </li>\r\n","<li>Create a <code>RandomizedSearchCV</code> object called <code>randomized_mse</code>, passing in: the parameter grid to <code>param_distributions</code>, the <code>XGBRegressor</code> to <code>estimator</code>, <code>\"neg_mean_squared_error\"</code> to <code>scoring</code>, <code>5</code> to <code>n_iter</code>, and <code>4</code> to <code>cv</code>. Also specify <code>verbose=1</code> so you can better understand the output.</li>\r\n","<li>Fit the <code>RandomizedSearchCV</code> object to <code>X</code> and <code>y</code>.</li>\r\n","</ul>"],"metadata":{"id":"Ot2kVDnEw1QN"}},{"cell_type":"code","execution_count":35,"source":["# Create the parameter grid: gbm_param_grid \r\n","gbm_param_grid = {\r\n","    'n_estimators': [25],\r\n","    'max_depth': range(2, 12)\r\n","}\r\n","\r\n","# Instantiate the regressor: gbm\r\n","gbm = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10)\r\n","\r\n","# Perform random search: grid_mse\r\n","randomized_mse = RandomizedSearchCV(estimator=gbm,param_distributions=gbm_param_grid, n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1, n_jobs=-1)\r\n","\r\n","\r\n","# Fit randomized_mse to the data\r\n","randomized_mse.fit(X,y)\r\n","\r\n","# Print the best parameters and lowest RMSE\r\n","print(\"Best parameters found: \", randomized_mse.best_params_)\r\n","print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"]},{"output_type":"stream","name":"stdout","text":["Best parameters found:  {'n_estimators': 25, 'max_depth': 8}\n","Lowest RMSE found:  37418.52437954323\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.5s finished\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdPKmW58w3dM","executionInfo":{"status":"ok","timestamp":1612571785552,"user_tz":180,"elapsed":2403,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"77fe86f8-540c-40a5-b468-177959046c47"}},{"cell_type":"markdown","source":["## Limits of grid search and random search"],"metadata":{"id":"6bSy7U_UxOyi"}},{"cell_type":"markdown","source":["### When should you use grid search and random search?"],"metadata":{"id":"TmIMXgyUxRRE"}},{"cell_type":"markdown","source":["<p>Now that you've seen some of the drawbacks of grid search and random search, which of the following most accurately describes why both random search and grid search are non-ideal search hyperparameter tuning strategies in all scenarios?</p>"],"metadata":{"id":"8fe44OK-xUXm"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Grid Search and Random Search both take a very long time to perform, regardless of the number of parameters you want to tune.\r\n","\r\n","Grid Search and Random Search both scale exponentially in the number of hyperparameters you want to tune.\r\n","\r\n","<b>The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run.</b>\r\n","\r\n","Grid Search and Random Search require that you have some idea of where the ideal values for hyperparameters reside.\r\n","</pre>"],"metadata":{"id":"8zT7_vDoxXTP"}},{"cell_type":"markdown","source":["**This is why random search and grid search should not always be used.**"],"metadata":{"id":"WySWVASMxiGr"}}]}