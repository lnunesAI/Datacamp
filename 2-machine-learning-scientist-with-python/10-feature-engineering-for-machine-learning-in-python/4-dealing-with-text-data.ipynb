{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"4-dealing-with-text-data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZ00IaHZO0mCRFXGL0wdos"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Dealing with Text Data\r\n",">  Finally, in this chapter, you will work with unstructured text data, understanding ways in which you can engineer columnar features out of a text corpus. You will compare how different approaches may impact how much context is being extracted from a text, and how to balance the need for context, without too many features being created.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Python, Datacamp, Machine Learning]\r\n","- image: images/datacamp/1_supervised_learning_with_scikit_learn/2_regression.png"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 4 exercises \"Feature Engineering for Machine Learning in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1612274407722,"user_tz":180,"elapsed":1896,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Encoding text"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### Cleaning up your text"],"metadata":{"id":"saQfZhYdAYhD"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline. </p>\r\n","<p>In this chapter you will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as <code>speech_df</code>, with the speeches stored in the <code>text</code> column.</p></div>"],"metadata":{"id":"XLeZ72uQAbhC"}},{"cell_type":"code","execution_count":2,"source":["speech_df = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/10-feature-engineering-for-machine-learning-in-python/datasets/speech_df.csv')"],"outputs":[],"metadata":{"id":"of4eD__tCrPL","executionInfo":{"status":"ok","timestamp":1612274410498,"user_tz":180,"elapsed":2071,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions 1/2\r\n","<p>Print the first 5 rows of the <code>text</code> column to see the free text fields.</p>"],"metadata":{"id":"35CtzC9SAd9l"}},{"cell_type":"code","execution_count":3,"source":["# Print the first 5 rows of the text column\r\n","print(speech_df['text'].head())"],"outputs":[{"output_type":"stream","name":"stdout","text":["0    Fellow-Citizens of the Senate and of the House...\n","1    Fellow Citizens:  I AM again called upon by th...\n","2    WHEN it was first perceived, in early times, t...\n","3    Friends and Fellow-Citizens:  CALLED upon to u...\n","4    PROCEEDING, fellow-citizens, to that qualifica...\n","Name: text, dtype: object\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccE6K3X9Anyw","executionInfo":{"status":"ok","timestamp":1612274414063,"user_tz":180,"elapsed":1180,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"438741e6-cf3c-4536-ebe7-36844685d9a0"}},{"cell_type":"markdown","source":["Instructions 2/2\r\n","<ul>\r\n","<li>Replace all non letter characters in the <code>text</code> column with a whitespace.</li>\r\n","<li>Make all characters in the newly created <code>text_clean</code> column lower case.</li>\r\n","</ul>"],"metadata":{"id":"2qzyIDjMAd9m"}},{"cell_type":"code","execution_count":4,"source":["# Replace all non letter characters with a whitespace\r\n","speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\r\n","\r\n","# Change to lower case\r\n","speech_df['text_clean'] = speech_df['text_clean'].str.lower()\r\n","\r\n","# Print the first 5 rows of the text_clean column\r\n","print(speech_df['text_clean'].head())"],"outputs":[{"output_type":"stream","name":"stdout","text":["0    fellow citizens of the senate and of the house...\n","1    fellow citizens   i am again called upon by th...\n","2    when it was first perceived  in early times  t...\n","3    friends and fellow citizens   called upon to u...\n","4    proceeding  fellow citizens  to that qualifica...\n","Name: text_clean, dtype: object\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aCtiGP9B2H5","executionInfo":{"status":"ok","timestamp":1612274452658,"user_tz":180,"elapsed":1409,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"27f15fd2-6b37-49ca-ee69-1df55dc6867e"}},{"cell_type":"markdown","source":["**now your text strings have been standardized and cleaned up. You can now use this new column (text_clean) to extract information about the speeches.**"],"metadata":{"id":"pzEZWU71B_kG"}},{"cell_type":"markdown","source":["### High level text features"],"metadata":{"id":"OMvMgTj1DBmI"}},{"cell_type":"markdown","source":["<p>Once the text has been cleaned and standardized you can begin creating features from the data. The most fundamental information you can calculate about free form text is its size, such as its length and number of words. In this exercise (and the rest of this chapter), you will focus on the cleaned/transformed text column (<code>text_clean</code>) you created in the last exercise.</p>"],"metadata":{"id":"aDcuqAGtDFoP"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Record the character length of each speech in the <code>char_count</code> column.</li>\r\n","<li>Record the word count of each speech in the <code>word_count</code> column.</li>\r\n","<li>Record the average word length of each speech in the <code>avg_word_length</code> column.</li>\r\n","</ul>"],"metadata":{"id":"rLmwX66UDHSY"}},{"cell_type":"code","execution_count":null,"source":["# Find the length of each text\r\n","speech_df['char_cnt'] = speech_df['text_clean'].str.len()\r\n","\r\n","# Count the number of words in each text\r\n","speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\r\n","\r\n","# Find the average length of word\r\n","speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\r\n","\r\n","# Print the first 5 rows of these columns\r\n","print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"],"outputs":[],"metadata":{"id":"8bBZgg85EVTO"}},{"cell_type":"markdown","source":["**These features may appear basic but can be quite useful in ML models.**"],"metadata":{"id":"1tUABVkfEZUN"}},{"cell_type":"markdown","source":["## Word counts"],"metadata":{"id":"trg5oHGSFsI8"}},{"cell_type":"markdown","source":["### Counting words (I)"],"metadata":{"id":"GuDlpEIKFpI8"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Once high level information has been recorded you can begin creating features based on the actual content of each text. One way to do this is to approach it in a similar way to how you worked with categorical variables in the earlier lessons. </p>\r\n","<ul>\r\n","<li>For each unique word in the dataset a column is created. </li>\r\n","<li>For each entry, the number of times this word occurs is counted and the count value is entered into the respective column.  </li>\r\n","</ul>\r\n","<p>These \"count\" columns can then be used to train machine learning models.</p></div>"],"metadata":{"id":"9qYwrNG8F6pq"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import <code>CountVectorizer</code> from <code>sklearn.feature_extraction.text</code>.  </li>\r\n","<li>Instantiate <code>CountVectorizer</code> and assign it to <code>cv</code>. </li>\r\n","<li>Fit the vectorizer to the <code>text_clean</code> column. </li>\r\n","<li>Print the feature names generated by the vectorizer.</li>\r\n","</ul>"],"metadata":{"id":"uNnPr6XrF8g6"}},{"cell_type":"code","execution_count":24,"source":["# Import CountVectorizer\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","# Instantiate CountVectorizer\r\n","cv = CountVectorizer()\r\n","\r\n","# Fit the vectorizer\r\n","cv.fit(speech_df['text_clean'])\r\n","\r\n","# Print feature names\r\n","cv.get_feature_names()[:5]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['abandon', 'abandoned', 'abandonment', 'abate', 'abdicated']"]},"metadata":{"tags":[]},"execution_count":24}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMi6g8dFGXF_","executionInfo":{"status":"ok","timestamp":1612278439210,"user_tz":180,"elapsed":640,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b324249c-e028-4381-e245-7c2428086d4a"}},{"cell_type":"markdown","source":["**this vectorizer can be applied to both the text it was trained on, and new texts.**"],"metadata":{"id":"MgV0RH2IGcSe"}},{"cell_type":"markdown","source":["### Counting words (II)"],"metadata":{"id":"ciQSt97tGrw8"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. This array will have a row per block of text and a column for each of the features generated by the vectorizer that you observed in the last exercise. </p>\r\n","<p>The vectorizer to you fit in the last exercise (<code>cv</code>) is available in your workspace.</p></div>"],"metadata":{"id":"GhgsPf3FGuHc"}},{"cell_type":"markdown","source":["Instructions 1/2\r\n","<ul>\r\n","<li>Apply the vectorizer to the <code>text_clean</code> column. </li>\r\n","<li>Convert this transformed (sparse) array into a numpy array with counts.</li>\r\n","</ul>"],"metadata":{"id":"oyr9HbMyGwaL"}},{"cell_type":"code","execution_count":9,"source":["# Apply the vectorizer\r\n","cv_transformed = cv.transform(speech_df['text_clean'])\r\n","\r\n","# Print the full array\r\n","cv_array = cv_transformed.toarray()\r\n","print(cv_array)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 1 0 ... 0 0 0]\n"," ...\n"," [0 1 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IgycxBEGHKkZ","executionInfo":{"status":"ok","timestamp":1612275561736,"user_tz":180,"elapsed":638,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ef9d08a8-0e08-4f7b-bdbb-c33404407453"}},{"cell_type":"markdown","source":["Instructions 2/2\r\n","<p>Print the dimensions of this numpy array.</p>"],"metadata":{"id":"fGafSEzjGwaM"}},{"cell_type":"code","execution_count":10,"source":["# Print the shape of cv_array\r\n","print(cv_array.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(58, 9043)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SPZkyBSHUTP","executionInfo":{"status":"ok","timestamp":1612275605831,"user_tz":180,"elapsed":983,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"624c358c-15d5-434f-842e-dd5e0ddf2ade"}},{"cell_type":"markdown","source":["**The speeches have 9043 unique words, which is a lot! In the next exercise, you will see how to create a limited set of features.**"],"metadata":{"id":"FgH-1E5KHYsO"}},{"cell_type":"markdown","source":["### Limiting your features"],"metadata":{"id":"-PFocTtpHamn"}},{"cell_type":"markdown","source":["<div class=\"\"><p>As you have seen, using the <code>CountVectorizer</code> with its default settings creates a feature for every single word in your corpus. This can create far too many features, often including ones that will provide very little analytical value.</p>\r\n","<p>For this purpose <code>CountVectorizer</code> has parameters that you can set to reduce the number of features:  </p>\r\n","<ul>\r\n","<li><code>min_df</code> : Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts.  </li>\r\n","<li><code>max_df</code> : Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as \"and\" or \"the\".</li>\r\n","</ul></div>"],"metadata":{"id":"IeyEVHmaHcwf"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Limit the number of features in the CountVectorizer by setting the minimum number of documents a word can appear to 20% and the maximum to 80%.</li>\r\n","<li>Fit and apply the vectorizer on <code>text_clean</code> column in one step. </li>\r\n","<li>Convert this transformed (sparse) array into a numpy array with counts. </li>\r\n","<li>Print the dimensions of the new reduced array.</li>\r\n","</ul>"],"metadata":{"id":"YggLuLkqHete"}},{"cell_type":"code","execution_count":11,"source":["# Import CountVectorizer\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","# Specify arguements to limit the number of features generated\r\n","cv = CountVectorizer(min_df=0.2, max_df=0.8)\r\n","\r\n","# Fit, transform, and convert into array\r\n","cv_transformed = cv.fit_transform(speech_df['text_clean'])\r\n","cv_array = cv_transformed.toarray()\r\n","\r\n","# Print the array shape\r\n","print(cv_array.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(58, 818)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvEb6WaHH72r","executionInfo":{"status":"ok","timestamp":1612275781798,"user_tz":180,"elapsed":888,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"05702dcf-25da-4c2f-8232-70358fecc1a2"}},{"cell_type":"markdown","source":["**Did you notice that the number of features (unique words) greatly reduced from 9043 to 818?**"],"metadata":{"id":"jC_NALz5IAOT"}},{"cell_type":"markdown","source":["### Text to DataFrame"],"metadata":{"id":"nVHXPi39IEsi"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now that you have generated these count based features in an array you will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a pandas DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame.</p>\r\n","<p>The numpy array (<code>cv_array</code>) and the vectorizer (<code>cv</code>) you fit in the last exercise are available in your workspace.</p></div>"],"metadata":{"id":"bzHFehnbIHY6"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a DataFrame <code>cv_df</code> containing the <code>cv_array</code> as the values and the feature names as the column names. </li>\r\n","<li>Add the prefix <code>Counts_</code> to the column names for ease of identification. </li>\r\n","<li>Concatenate this DataFrame (<code>cv_df</code>) to the original DataFrame (<code>speech_df</code>) column wise.</li>\r\n","</ul>"],"metadata":{"id":"yQdEEOi6IJAa"}},{"cell_type":"code","execution_count":14,"source":["# Create a DataFrame with these features\r\n","cv_df = pd.DataFrame(cv_array, \r\n","                     columns=cv.get_feature_names()).add_prefix('Counts_')\r\n","\r\n","# Add the new columns to the original DataFrame\r\n","speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\r\n","speech_df_new.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Inaugural Address</th>\n","      <th>Date</th>\n","      <th>text</th>\n","      <th>text_clean</th>\n","      <th>Counts_abiding</th>\n","      <th>Counts_ability</th>\n","      <th>Counts_able</th>\n","      <th>Counts_about</th>\n","      <th>Counts_above</th>\n","      <th>Counts_abroad</th>\n","      <th>Counts_accept</th>\n","      <th>Counts_accomplished</th>\n","      <th>Counts_achieve</th>\n","      <th>Counts_across</th>\n","      <th>Counts_act</th>\n","      <th>Counts_action</th>\n","      <th>Counts_acts</th>\n","      <th>Counts_add</th>\n","      <th>Counts_adequate</th>\n","      <th>Counts_administration</th>\n","      <th>Counts_adopted</th>\n","      <th>Counts_advance</th>\n","      <th>Counts_advantage</th>\n","      <th>Counts_affairs</th>\n","      <th>Counts_afford</th>\n","      <th>Counts_after</th>\n","      <th>Counts_again</th>\n","      <th>Counts_against</th>\n","      <th>Counts_age</th>\n","      <th>Counts_ago</th>\n","      <th>Counts_agriculture</th>\n","      <th>Counts_aid</th>\n","      <th>Counts_alike</th>\n","      <th>Counts_almighty</th>\n","      <th>Counts_almost</th>\n","      <th>Counts_alone</th>\n","      <th>Counts_along</th>\n","      <th>Counts_already</th>\n","      <th>Counts_also</th>\n","      <th>...</th>\n","      <th>Counts_vital</th>\n","      <th>Counts_voice</th>\n","      <th>Counts_want</th>\n","      <th>Counts_war</th>\n","      <th>Counts_wars</th>\n","      <th>Counts_washington</th>\n","      <th>Counts_waste</th>\n","      <th>Counts_way</th>\n","      <th>Counts_ways</th>\n","      <th>Counts_weak</th>\n","      <th>Counts_wealth</th>\n","      <th>Counts_weight</th>\n","      <th>Counts_welfare</th>\n","      <th>Counts_were</th>\n","      <th>Counts_what</th>\n","      <th>Counts_whatever</th>\n","      <th>Counts_where</th>\n","      <th>Counts_wherever</th>\n","      <th>Counts_whether</th>\n","      <th>Counts_while</th>\n","      <th>Counts_whole</th>\n","      <th>Counts_whom</th>\n","      <th>Counts_whose</th>\n","      <th>Counts_willing</th>\n","      <th>Counts_wisdom</th>\n","      <th>Counts_wise</th>\n","      <th>Counts_wisely</th>\n","      <th>Counts_wish</th>\n","      <th>Counts_within</th>\n","      <th>Counts_without</th>\n","      <th>Counts_women</th>\n","      <th>Counts_words</th>\n","      <th>Counts_work</th>\n","      <th>Counts_wrong</th>\n","      <th>Counts_year</th>\n","      <th>Counts_years</th>\n","      <th>Counts_yet</th>\n","      <th>Counts_you</th>\n","      <th>Counts_young</th>\n","      <th>Counts_your</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>George Washington</td>\n","      <td>First Inaugural Address</td>\n","      <td>Thursday, April 30, 1789</td>\n","      <td>Fellow-Citizens of the Senate and of the House...</td>\n","      <td>fellow citizens of the senate and of the house...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>George Washington</td>\n","      <td>Second Inaugural Address</td>\n","      <td>Monday, March 4, 1793</td>\n","      <td>Fellow Citizens:  I AM again called upon by th...</td>\n","      <td>fellow citizens   i am again called upon by th...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>John Adams</td>\n","      <td>Inaugural Address</td>\n","      <td>Saturday, March 4, 1797</td>\n","      <td>WHEN it was first perceived, in early times, t...</td>\n","      <td>when it was first perceived  in early times  t...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Thomas Jefferson</td>\n","      <td>First Inaugural Address</td>\n","      <td>Wednesday, March 4, 1801</td>\n","      <td>Friends and Fellow-Citizens:  CALLED upon to u...</td>\n","      <td>friends and fellow citizens   called upon to u...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Thomas Jefferson</td>\n","      <td>Second Inaugural Address</td>\n","      <td>Monday, March 4, 1805</td>\n","      <td>PROCEEDING, fellow-citizens, to that qualifica...</td>\n","      <td>proceeding  fellow citizens  to that qualifica...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 823 columns</p>\n","</div>"],"text/plain":["                Name         Inaugural Address  ... Counts_young Counts_your\n","0  George Washington   First Inaugural Address  ...            0           9\n","1  George Washington  Second Inaugural Address  ...            0           1\n","2         John Adams         Inaugural Address  ...            0           1\n","3   Thomas Jefferson   First Inaugural Address  ...            0           7\n","4   Thomas Jefferson  Second Inaugural Address  ...            0           4\n","\n","[5 rows x 823 columns]"]},"metadata":{"tags":[]},"execution_count":14}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"id":"20r9OWPvI0tN","executionInfo":{"status":"ok","timestamp":1612276080888,"user_tz":180,"elapsed":649,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c431423e-b5ca-4057-ee19-a95913acfed1"}},{"cell_type":"markdown","source":["**With the new features combined with the orginial DataFrame they can be now used for ML models or analysis.**"],"metadata":{"id":"gL983WbLI3h0"}},{"cell_type":"markdown","source":["## Term frequency-inverse document frequency"],"metadata":{"id":"1fK1VBgvNWPq"}},{"cell_type":"markdown","source":["### Tf-idf"],"metadata":{"id":"IbP_sN1gNQyz"}},{"cell_type":"markdown","source":["<p>While counts of occurrences of words can be useful to build models, words that occur many times may skew the results undesirably. To limit these common words from overpowering your model a form of normalization can be used. In this lesson you will be using Term frequency-inverse document frequency (Tf-idf) as was discussed in the video. Tf-idf has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents.</p>"],"metadata":{"id":"IqR-HDdFNgWJ"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import <code>TfidfVectorizer</code> from <code>sklearn.feature_extraction.text</code>.  </li>\r\n","<li>Instantiate <code>TfidfVectorizer</code> while limiting the number of features to 100 and removing English stop words. </li>\r\n","<li>Fit and apply the vectorizer on <code>text_clean</code> column in one step. </li>\r\n","<li>Create a DataFrame <code>tv_df</code> containing the weights of the words and the feature names as the column names.</li>\r\n","</ul>"],"metadata":{"id":"5x3zsY_wNjzY"}},{"cell_type":"code","execution_count":16,"source":["# Import TfidfVectorizer\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","\r\n","# Instantiate TfidfVectorizer\r\n","tv = TfidfVectorizer(max_features=100, stop_words='english')\r\n","\r\n","# Fit the vectroizer and transform the data\r\n","tv_transformed =  tv.fit_transform(speech_df['text_clean'])\r\n","\r\n","# Create a DataFrame with these features\r\n","tv_df = pd.DataFrame(tv_transformed.toarray(), \r\n","                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\r\n","tv_df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TFIDF_action</th>\n","      <th>TFIDF_administration</th>\n","      <th>TFIDF_america</th>\n","      <th>TFIDF_american</th>\n","      <th>TFIDF_americans</th>\n","      <th>TFIDF_believe</th>\n","      <th>TFIDF_best</th>\n","      <th>TFIDF_better</th>\n","      <th>TFIDF_change</th>\n","      <th>TFIDF_citizens</th>\n","      <th>TFIDF_come</th>\n","      <th>TFIDF_common</th>\n","      <th>TFIDF_confidence</th>\n","      <th>TFIDF_congress</th>\n","      <th>TFIDF_constitution</th>\n","      <th>TFIDF_country</th>\n","      <th>TFIDF_day</th>\n","      <th>TFIDF_duties</th>\n","      <th>TFIDF_duty</th>\n","      <th>TFIDF_equal</th>\n","      <th>TFIDF_executive</th>\n","      <th>TFIDF_faith</th>\n","      <th>TFIDF_far</th>\n","      <th>TFIDF_federal</th>\n","      <th>TFIDF_fellow</th>\n","      <th>TFIDF_force</th>\n","      <th>TFIDF_foreign</th>\n","      <th>TFIDF_free</th>\n","      <th>TFIDF_freedom</th>\n","      <th>TFIDF_future</th>\n","      <th>TFIDF_general</th>\n","      <th>TFIDF_god</th>\n","      <th>TFIDF_good</th>\n","      <th>TFIDF_government</th>\n","      <th>TFIDF_great</th>\n","      <th>TFIDF_high</th>\n","      <th>TFIDF_history</th>\n","      <th>TFIDF_home</th>\n","      <th>TFIDF_hope</th>\n","      <th>TFIDF_human</th>\n","      <th>...</th>\n","      <th>TFIDF_need</th>\n","      <th>TFIDF_new</th>\n","      <th>TFIDF_office</th>\n","      <th>TFIDF_old</th>\n","      <th>TFIDF_order</th>\n","      <th>TFIDF_party</th>\n","      <th>TFIDF_peace</th>\n","      <th>TFIDF_people</th>\n","      <th>TFIDF_place</th>\n","      <th>TFIDF_policy</th>\n","      <th>TFIDF_political</th>\n","      <th>TFIDF_power</th>\n","      <th>TFIDF_powers</th>\n","      <th>TFIDF_present</th>\n","      <th>TFIDF_president</th>\n","      <th>TFIDF_principles</th>\n","      <th>TFIDF_progress</th>\n","      <th>TFIDF_prosperity</th>\n","      <th>TFIDF_public</th>\n","      <th>TFIDF_purpose</th>\n","      <th>TFIDF_right</th>\n","      <th>TFIDF_rights</th>\n","      <th>TFIDF_secure</th>\n","      <th>TFIDF_service</th>\n","      <th>TFIDF_shall</th>\n","      <th>TFIDF_spirit</th>\n","      <th>TFIDF_state</th>\n","      <th>TFIDF_states</th>\n","      <th>TFIDF_strength</th>\n","      <th>TFIDF_support</th>\n","      <th>TFIDF_things</th>\n","      <th>TFIDF_time</th>\n","      <th>TFIDF_today</th>\n","      <th>TFIDF_union</th>\n","      <th>TFIDF_united</th>\n","      <th>TFIDF_war</th>\n","      <th>TFIDF_way</th>\n","      <th>TFIDF_work</th>\n","      <th>TFIDF_world</th>\n","      <th>TFIDF_years</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.133415</td>\n","      <td>0.000000</td>\n","      <td>0.105388</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.229644</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.111079</td>\n","      <td>0.000000</td>\n","      <td>0.060755</td>\n","      <td>0.229644</td>\n","      <td>0.115098</td>\n","      <td>0.064225</td>\n","      <td>0.238637</td>\n","      <td>0.063036</td>\n","      <td>0.147280</td>\n","      <td>0.000000</td>\n","      <td>0.178978</td>\n","      <td>0.000000</td>\n","      <td>0.147528</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.098352</td>\n","      <td>0.000000</td>\n","      <td>0.101797</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.147528</td>\n","      <td>0.367430</td>\n","      <td>0.133183</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.051787</td>\n","      <td>0.126073</td>\n","      <td>...</td>\n","      <td>0.00000</td>\n","      <td>0.049176</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.141458</td>\n","      <td>0.070729</td>\n","      <td>0.000000</td>\n","      <td>0.174590</td>\n","      <td>0.056532</td>\n","      <td>0.138691</td>\n","      <td>0.000000</td>\n","      <td>0.050898</td>\n","      <td>0.065448</td>\n","      <td>0.315182</td>\n","      <td>0.061880</td>\n","      <td>0.063036</td>\n","      <td>0.000000</td>\n","      <td>0.064225</td>\n","      <td>0.333237</td>\n","      <td>0.0</td>\n","      <td>0.055540</td>\n","      <td>0.050898</td>\n","      <td>0.000000</td>\n","      <td>0.063036</td>\n","      <td>0.145021</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.103573</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.045929</td>\n","      <td>0.0</td>\n","      <td>0.136012</td>\n","      <td>0.203593</td>\n","      <td>0.000000</td>\n","      <td>0.060755</td>\n","      <td>0.000000</td>\n","      <td>0.045929</td>\n","      <td>0.052694</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.261016</td>\n","      <td>0.266097</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.179712</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.217318</td>\n","      <td>0.000000</td>\n","      <td>0.237725</td>\n","      <td>0.179712</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.192418</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.179712</td>\n","      <td>0.000000</td>\n","      <td>0.233437</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.242128</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.170786</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.246652</td>\n","      <td>0.242128</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.567446</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.199157</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.092436</td>\n","      <td>0.157058</td>\n","      <td>0.073018</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.026112</td>\n","      <td>0.060460</td>\n","      <td>0.000000</td>\n","      <td>0.106072</td>\n","      <td>0.0</td>\n","      <td>0.056125</td>\n","      <td>0.025654</td>\n","      <td>0.196017</td>\n","      <td>0.224501</td>\n","      <td>0.212143</td>\n","      <td>0.026582</td>\n","      <td>0.029665</td>\n","      <td>0.055113</td>\n","      <td>0.058233</td>\n","      <td>0.068028</td>\n","      <td>0.082669</td>\n","      <td>0.027556</td>\n","      <td>0.000000</td>\n","      <td>0.068143</td>\n","      <td>0.000000</td>\n","      <td>0.246496</td>\n","      <td>0.045428</td>\n","      <td>0.000000</td>\n","      <td>0.023510</td>\n","      <td>0.133321</td>\n","      <td>0.0</td>\n","      <td>0.136285</td>\n","      <td>0.339429</td>\n","      <td>0.102528</td>\n","      <td>0.027556</td>\n","      <td>0.029116</td>\n","      <td>0.000000</td>\n","      <td>0.023920</td>\n","      <td>0.058233</td>\n","      <td>...</td>\n","      <td>0.00000</td>\n","      <td>0.022714</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.130678</td>\n","      <td>0.130678</td>\n","      <td>0.121696</td>\n","      <td>0.403213</td>\n","      <td>0.026112</td>\n","      <td>0.000000</td>\n","      <td>0.027556</td>\n","      <td>0.117549</td>\n","      <td>0.030230</td>\n","      <td>0.058233</td>\n","      <td>0.000000</td>\n","      <td>0.058233</td>\n","      <td>0.000000</td>\n","      <td>0.059331</td>\n","      <td>0.153921</td>\n","      <td>0.0</td>\n","      <td>0.025654</td>\n","      <td>0.023510</td>\n","      <td>0.030230</td>\n","      <td>0.058233</td>\n","      <td>0.089313</td>\n","      <td>0.153921</td>\n","      <td>0.090691</td>\n","      <td>0.215280</td>\n","      <td>0.000000</td>\n","      <td>0.116465</td>\n","      <td>0.032030</td>\n","      <td>0.021214</td>\n","      <td>0.0</td>\n","      <td>0.062823</td>\n","      <td>0.070529</td>\n","      <td>0.024339</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.063643</td>\n","      <td>0.073018</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.092693</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.090942</td>\n","      <td>0.117831</td>\n","      <td>0.045471</td>\n","      <td>0.053335</td>\n","      <td>0.223369</td>\n","      <td>0.0</td>\n","      <td>0.084421</td>\n","      <td>0.154348</td>\n","      <td>0.000000</td>\n","      <td>0.084421</td>\n","      <td>0.127639</td>\n","      <td>0.039983</td>\n","      <td>0.089243</td>\n","      <td>0.000000</td>\n","      <td>0.175183</td>\n","      <td>0.051163</td>\n","      <td>0.082899</td>\n","      <td>0.041449</td>\n","      <td>0.059596</td>\n","      <td>0.239161</td>\n","      <td>0.048179</td>\n","      <td>0.000000</td>\n","      <td>0.102498</td>\n","      <td>0.171970</td>\n","      <td>0.035363</td>\n","      <td>0.100268</td>\n","      <td>0.0</td>\n","      <td>0.170829</td>\n","      <td>0.382918</td>\n","      <td>0.030844</td>\n","      <td>0.124348</td>\n","      <td>0.087591</td>\n","      <td>0.045471</td>\n","      <td>0.035980</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.042992</td>\n","      <td>0.000000</td>\n","      <td>0.049140</td>\n","      <td>0.000000</td>\n","      <td>0.183051</td>\n","      <td>0.060650</td>\n","      <td>0.039277</td>\n","      <td>0.000000</td>\n","      <td>0.124348</td>\n","      <td>0.141450</td>\n","      <td>0.045471</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.131387</td>\n","      <td>0.000000</td>\n","      <td>0.044621</td>\n","      <td>0.154348</td>\n","      <td>0.0</td>\n","      <td>0.154348</td>\n","      <td>0.070725</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.201512</td>\n","      <td>0.000000</td>\n","      <td>0.090942</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.131387</td>\n","      <td>0.048179</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.094497</td>\n","      <td>0.000000</td>\n","      <td>0.036610</td>\n","      <td>0.000000</td>\n","      <td>0.039277</td>\n","      <td>0.095729</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.041334</td>\n","      <td>0.039761</td>\n","      <td>0.000000</td>\n","      <td>0.031408</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.067393</td>\n","      <td>0.039011</td>\n","      <td>0.091514</td>\n","      <td>0.273760</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.033105</td>\n","      <td>0.000000</td>\n","      <td>0.217280</td>\n","      <td>0.109504</td>\n","      <td>0.034302</td>\n","      <td>0.153126</td>\n","      <td>0.142240</td>\n","      <td>0.075146</td>\n","      <td>0.043893</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.234492</td>\n","      <td>0.000000</td>\n","      <td>0.159045</td>\n","      <td>0.029311</td>\n","      <td>0.073768</td>\n","      <td>0.060676</td>\n","      <td>0.043011</td>\n","      <td>0.0</td>\n","      <td>0.087934</td>\n","      <td>0.082128</td>\n","      <td>0.026461</td>\n","      <td>0.000000</td>\n","      <td>0.075146</td>\n","      <td>0.039011</td>\n","      <td>0.000000</td>\n","      <td>0.075146</td>\n","      <td>...</td>\n","      <td>0.10864</td>\n","      <td>0.029311</td>\n","      <td>0.000000</td>\n","      <td>0.040535</td>\n","      <td>0.126475</td>\n","      <td>0.000000</td>\n","      <td>0.125634</td>\n","      <td>0.000000</td>\n","      <td>0.067393</td>\n","      <td>0.000000</td>\n","      <td>0.035560</td>\n","      <td>0.091014</td>\n","      <td>0.039011</td>\n","      <td>0.112719</td>\n","      <td>0.000000</td>\n","      <td>0.112719</td>\n","      <td>0.036884</td>\n","      <td>0.000000</td>\n","      <td>0.463464</td>\n","      <td>0.0</td>\n","      <td>0.033105</td>\n","      <td>0.091014</td>\n","      <td>0.039011</td>\n","      <td>0.037573</td>\n","      <td>0.201694</td>\n","      <td>0.066209</td>\n","      <td>0.312084</td>\n","      <td>0.123470</td>\n","      <td>0.078021</td>\n","      <td>0.075146</td>\n","      <td>0.082667</td>\n","      <td>0.164256</td>\n","      <td>0.0</td>\n","      <td>0.121605</td>\n","      <td>0.030338</td>\n","      <td>0.094225</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.054752</td>\n","      <td>0.062817</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 100 columns</p>\n","</div>"],"text/plain":["   TFIDF_action  TFIDF_administration  ...  TFIDF_world  TFIDF_years\n","0      0.000000              0.133415  ...     0.045929     0.052694\n","1      0.000000              0.261016  ...     0.000000     0.000000\n","2      0.000000              0.092436  ...     0.063643     0.073018\n","3      0.000000              0.092693  ...     0.095729     0.000000\n","4      0.041334              0.039761  ...     0.054752     0.062817\n","\n","[5 rows x 100 columns]"]},"metadata":{"tags":[]},"execution_count":16}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"3dbOiqEjNFZ9","executionInfo":{"status":"ok","timestamp":1612277284601,"user_tz":180,"elapsed":664,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"af5ff0e7-db1f-4514-9fc4-ebb56896bc1b"}},{"cell_type":"markdown","source":["**Did you notice that counting the word occurences and calculating the Tf-idf weights are very similar? This is one of the reasons scikit-learn is very popular, a consistent API.**"],"metadata":{"id":"A_o3AaYTNOvT"}},{"cell_type":"markdown","source":["### Inspecting Tf-idf values"],"metadata":{"id":"aKmIDW_7N7Jl"}},{"cell_type":"markdown","source":["<div class=\"\"><p>After creating Tf-idf features you will often want to understand what are the most highest scored words for each corpus. This can be achieved by isolating the row you want to examine and then sorting the the scores from high to low. </p>\r\n","<p>The DataFrame from the last exercise (<code>tv_df</code>) is available in your workspace.</p></div>"],"metadata":{"id":"pIadxUgrN9qd"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Assign the first row of <code>tv_df</code> to <code>sample_row</code>. </li>\r\n","<li><code>sample_row</code> is now a series of weights assigned to words. Sort these values to print the top 5 highest-rated words.</li>\r\n","</ul>"],"metadata":{"id":"atwhNPfZN_L-"}},{"cell_type":"code","execution_count":17,"source":["# Isolate the row to be examined\r\n","sample_row = tv_df.iloc[0]\r\n","\r\n","# Print the top 5 words of the sorted output\r\n","print(sample_row.sort_values(ascending=False).head())"],"outputs":[{"output_type":"stream","name":"stdout","text":["TFIDF_government    0.367430\n","TFIDF_public        0.333237\n","TFIDF_present       0.315182\n","TFIDF_duty          0.238637\n","TFIDF_citizens      0.229644\n","Name: 0, dtype: float64\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCFzvbHKOSWr","executionInfo":{"status":"ok","timestamp":1612277460453,"user_tz":180,"elapsed":672,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"bd9b87d9-c0ce-4a5f-e110-6313e18fcf3c"}},{"cell_type":"markdown","source":["**Do you think these scores make sense for the corresponding words?**"],"metadata":{"id":"DCM20jc_OZRq"}},{"cell_type":"markdown","source":["### Transforming unseen data"],"metadata":{"id":"YVYiS63aOfMZ"}},{"cell_type":"markdown","source":["<div class=\"\"><p>When creating vectors from text, any transformations that you perform before training a machine learning model, you also need to apply on the new unseen (test) data. To achieve this follow the same approach from the last chapter: <em>fit the vectorizer only on the training data, and apply it to the test data.</em></p>\r\n","<p>For this exercise the <code>speech_df</code> DataFrame has been split in two:</p>\r\n","<ul>\r\n","<li><code>train_speech_df</code>: The training set consisting of the first 45 speeches.</li>\r\n","<li><code>test_speech_df</code>: The test set consisting of the remaining speeches.</li>\r\n","</ul></div>"],"metadata":{"id":"JRrshL95OhGq"}},{"cell_type":"code","execution_count":18,"source":["train_speech_df = speech_df.iloc[:45]\r\n","test_speech_df = speech_df.iloc[45:]"],"outputs":[],"metadata":{"id":"PInY7-0DPRUE","executionInfo":{"status":"ok","timestamp":1612277687686,"user_tz":180,"elapsed":658,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Instantiate <code>TfidfVectorizer</code>. </li>\r\n","<li>Fit the vectorizer and apply it to the <code>text_clean</code> column. </li>\r\n","<li>Apply the same vectorizer on the <code>text_clean</code> column of the test data. </li>\r\n","<li>Create a DataFrame of these new features from the test set.</li>\r\n","</ul>"],"metadata":{"id":"rghLXSiHOimp"}},{"cell_type":"code","execution_count":20,"source":["# Instantiate TfidfVectorizer\r\n","tv = TfidfVectorizer(max_features=100, stop_words='english')\r\n","\r\n","# Fit the vectroizer and transform the data\r\n","tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\r\n","\r\n","# Transform test data\r\n","test_tv_transformed = tv.transform(test_speech_df['text_clean'])\r\n","\r\n","# Create new features for the test set\r\n","test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \r\n","                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\r\n","test_tv_df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TFIDF_action</th>\n","      <th>TFIDF_administration</th>\n","      <th>TFIDF_america</th>\n","      <th>TFIDF_american</th>\n","      <th>TFIDF_authority</th>\n","      <th>TFIDF_best</th>\n","      <th>TFIDF_business</th>\n","      <th>TFIDF_citizens</th>\n","      <th>TFIDF_commerce</th>\n","      <th>TFIDF_common</th>\n","      <th>TFIDF_confidence</th>\n","      <th>TFIDF_congress</th>\n","      <th>TFIDF_constitution</th>\n","      <th>TFIDF_constitutional</th>\n","      <th>TFIDF_country</th>\n","      <th>TFIDF_day</th>\n","      <th>TFIDF_duties</th>\n","      <th>TFIDF_duty</th>\n","      <th>TFIDF_equal</th>\n","      <th>TFIDF_executive</th>\n","      <th>TFIDF_faith</th>\n","      <th>TFIDF_far</th>\n","      <th>TFIDF_federal</th>\n","      <th>TFIDF_fellow</th>\n","      <th>TFIDF_force</th>\n","      <th>TFIDF_foreign</th>\n","      <th>TFIDF_free</th>\n","      <th>TFIDF_freedom</th>\n","      <th>TFIDF_future</th>\n","      <th>TFIDF_general</th>\n","      <th>TFIDF_given</th>\n","      <th>TFIDF_god</th>\n","      <th>TFIDF_good</th>\n","      <th>TFIDF_government</th>\n","      <th>TFIDF_great</th>\n","      <th>TFIDF_high</th>\n","      <th>TFIDF_hope</th>\n","      <th>TFIDF_human</th>\n","      <th>TFIDF_important</th>\n","      <th>TFIDF_institutions</th>\n","      <th>...</th>\n","      <th>TFIDF_order</th>\n","      <th>TFIDF_ought</th>\n","      <th>TFIDF_party</th>\n","      <th>TFIDF_peace</th>\n","      <th>TFIDF_people</th>\n","      <th>TFIDF_policy</th>\n","      <th>TFIDF_political</th>\n","      <th>TFIDF_power</th>\n","      <th>TFIDF_powers</th>\n","      <th>TFIDF_present</th>\n","      <th>TFIDF_principle</th>\n","      <th>TFIDF_principles</th>\n","      <th>TFIDF_progress</th>\n","      <th>TFIDF_proper</th>\n","      <th>TFIDF_prosperity</th>\n","      <th>TFIDF_protection</th>\n","      <th>TFIDF_public</th>\n","      <th>TFIDF_purpose</th>\n","      <th>TFIDF_question</th>\n","      <th>TFIDF_republic</th>\n","      <th>TFIDF_revenue</th>\n","      <th>TFIDF_right</th>\n","      <th>TFIDF_rights</th>\n","      <th>TFIDF_secure</th>\n","      <th>TFIDF_self</th>\n","      <th>TFIDF_service</th>\n","      <th>TFIDF_shall</th>\n","      <th>TFIDF_spirit</th>\n","      <th>TFIDF_state</th>\n","      <th>TFIDF_states</th>\n","      <th>TFIDF_subject</th>\n","      <th>TFIDF_support</th>\n","      <th>TFIDF_time</th>\n","      <th>TFIDF_union</th>\n","      <th>TFIDF_united</th>\n","      <th>TFIDF_war</th>\n","      <th>TFIDF_way</th>\n","      <th>TFIDF_work</th>\n","      <th>TFIDF_world</th>\n","      <th>TFIDF_years</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.029540</td>\n","      <td>0.233954</td>\n","      <td>0.082703</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.022577</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.026350</td>\n","      <td>0.000000</td>\n","      <td>0.026950</td>\n","      <td>0.0</td>\n","      <td>0.022577</td>\n","      <td>0.029540</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.065003</td>\n","      <td>0.0</td>\n","      <td>0.031720</td>\n","      <td>0.056409</td>\n","      <td>0.000000</td>\n","      <td>0.049296</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.049296</td>\n","      <td>0.066626</td>\n","      <td>0.026350</td>\n","      <td>0.000000</td>\n","      <td>0.030968</td>\n","      <td>0.195008</td>\n","      <td>0.024111</td>\n","      <td>0.115378</td>\n","      <td>0.110450</td>\n","      <td>0.055135</td>\n","      <td>0.079050</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.034158</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.316200</td>\n","      <td>0.302600</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.025767</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.030968</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.029540</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.030242</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.086457</td>\n","      <td>0.165406</td>\n","      <td>0.000000</td>\n","      <td>0.024648</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.115378</td>\n","      <td>0.000000</td>\n","      <td>0.024648</td>\n","      <td>0.079050</td>\n","      <td>0.033313</td>\n","      <td>0.000000</td>\n","      <td>0.299983</td>\n","      <td>0.134749</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.547457</td>\n","      <td>0.036862</td>\n","      <td>0.000000</td>\n","      <td>0.036036</td>\n","      <td>0.000000</td>\n","      <td>0.015094</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.017617</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.045283</td>\n","      <td>0.019750</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.021730</td>\n","      <td>0.0</td>\n","      <td>0.084830</td>\n","      <td>0.037714</td>\n","      <td>0.000000</td>\n","      <td>0.016479</td>\n","      <td>0.043459</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.089089</td>\n","      <td>0.052851</td>\n","      <td>0.000000</td>\n","      <td>0.020704</td>\n","      <td>0.086919</td>\n","      <td>0.016120</td>\n","      <td>0.154278</td>\n","      <td>0.132920</td>\n","      <td>0.018431</td>\n","      <td>0.035234</td>\n","      <td>0.040438</td>\n","      <td>0.043459</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.022837</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.334722</td>\n","      <td>0.086705</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.017227</td>\n","      <td>0.018857</td>\n","      <td>0.000000</td>\n","      <td>0.024041</td>\n","      <td>0.000000</td>\n","      <td>0.103522</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.019750</td>\n","      <td>0.024685</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.108108</td>\n","      <td>0.016120</td>\n","      <td>0.020219</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.101155</td>\n","      <td>0.036862</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.019296</td>\n","      <td>0.092567</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.052851</td>\n","      <td>0.066817</td>\n","      <td>0.078999</td>\n","      <td>0.277701</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.126987</td>\n","      <td>0.134669</td>\n","      <td>0.000000</td>\n","      <td>0.131652</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.046997</td>\n","      <td>0.042907</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.036763</td>\n","      <td>0.048102</td>\n","      <td>0.045927</td>\n","      <td>0.0</td>\n","      <td>0.052924</td>\n","      <td>0.0</td>\n","      <td>0.103304</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.049244</td>\n","      <td>0.040136</td>\n","      <td>0.216981</td>\n","      <td>0.085814</td>\n","      <td>0.000000</td>\n","      <td>0.100853</td>\n","      <td>0.052924</td>\n","      <td>0.078521</td>\n","      <td>0.150301</td>\n","      <td>0.071941</td>\n","      <td>0.089780</td>\n","      <td>0.085814</td>\n","      <td>0.246220</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.042907</td>\n","      <td>0.211174</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.093993</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.043884</td>\n","      <td>0.117781</td>\n","      <td>0.000000</td>\n","      <td>0.054245</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.269339</td>\n","      <td>0.000000</td>\n","      <td>0.040136</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.075151</td>\n","      <td>0.000000</td>\n","      <td>0.080272</td>\n","      <td>0.042907</td>\n","      <td>0.054245</td>\n","      <td>0.096203</td>\n","      <td>0.225452</td>\n","      <td>0.043884</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.037094</td>\n","      <td>0.067428</td>\n","      <td>0.267012</td>\n","      <td>0.031463</td>\n","      <td>0.039990</td>\n","      <td>0.061516</td>\n","      <td>0.050085</td>\n","      <td>0.077301</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.039990</td>\n","      <td>0.030758</td>\n","      <td>0.0</td>\n","      <td>0.077301</td>\n","      <td>0.134856</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.074188</td>\n","      <td>0.0</td>\n","      <td>0.108607</td>\n","      <td>0.032190</td>\n","      <td>0.183116</td>\n","      <td>0.084393</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.056262</td>\n","      <td>0.304162</td>\n","      <td>0.090220</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.185469</td>\n","      <td>0.027517</td>\n","      <td>0.421380</td>\n","      <td>0.100845</td>\n","      <td>0.031463</td>\n","      <td>0.060146</td>\n","      <td>0.034515</td>\n","      <td>0.037094</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.038984</td>\n","      <td>0.0</td>\n","      <td>0.038984</td>\n","      <td>0.060146</td>\n","      <td>0.222015</td>\n","      <td>0.0</td>\n","      <td>0.092274</td>\n","      <td>0.029408</td>\n","      <td>0.032190</td>\n","      <td>0.094389</td>\n","      <td>0.000000</td>\n","      <td>0.032940</td>\n","      <td>0.070687</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.029408</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.039990</td>\n","      <td>0.0</td>\n","      <td>0.030758</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.076040</td>\n","      <td>0.0</td>\n","      <td>0.024668</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.112524</td>\n","      <td>0.0</td>\n","      <td>0.098819</td>\n","      <td>0.210690</td>\n","      <td>0.000000</td>\n","      <td>0.056262</td>\n","      <td>0.030073</td>\n","      <td>0.038020</td>\n","      <td>0.235998</td>\n","      <td>0.237026</td>\n","      <td>0.061516</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.221561</td>\n","      <td>0.156644</td>\n","      <td>0.028442</td>\n","      <td>0.087505</td>\n","      <td>0.000000</td>\n","      <td>0.109959</td>\n","      <td>0.0</td>\n","      <td>0.023428</td>\n","      <td>0.021389</td>\n","      <td>0.028442</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.018327</td>\n","      <td>0.143872</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.026383</td>\n","      <td>0.0</td>\n","      <td>0.077246</td>\n","      <td>0.000000</td>\n","      <td>0.162799</td>\n","      <td>0.060023</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.060023</td>\n","      <td>0.378580</td>\n","      <td>0.042778</td>\n","      <td>0.025138</td>\n","      <td>0.025138</td>\n","      <td>0.211061</td>\n","      <td>0.019571</td>\n","      <td>0.337164</td>\n","      <td>0.089656</td>\n","      <td>0.000000</td>\n","      <td>0.042778</td>\n","      <td>0.220934</td>\n","      <td>0.026383</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.027727</td>\n","      <td>0.171114</td>\n","      <td>0.298266</td>\n","      <td>0.0</td>\n","      <td>0.043752</td>\n","      <td>0.041832</td>\n","      <td>0.000000</td>\n","      <td>0.022378</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.150826</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.023979</td>\n","      <td>0.059941</td>\n","      <td>0.028442</td>\n","      <td>0.0</td>\n","      <td>0.087505</td>\n","      <td>0.019571</td>\n","      <td>0.024548</td>\n","      <td>0.108166</td>\n","      <td>0.0</td>\n","      <td>0.035090</td>\n","      <td>0.044755</td>\n","      <td>0.023428</td>\n","      <td>0.060023</td>\n","      <td>0.0</td>\n","      <td>0.023428</td>\n","      <td>0.187313</td>\n","      <td>0.131913</td>\n","      <td>0.040016</td>\n","      <td>0.021389</td>\n","      <td>0.081124</td>\n","      <td>0.119894</td>\n","      <td>0.299701</td>\n","      <td>0.153133</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 100 columns</p>\n","</div>"],"text/plain":["   TFIDF_action  TFIDF_administration  ...  TFIDF_world  TFIDF_years\n","0      0.000000              0.029540  ...     0.299983     0.134749\n","1      0.000000              0.000000  ...     0.277701     0.126126\n","2      0.000000              0.000000  ...     0.225452     0.043884\n","3      0.037094              0.067428  ...     0.237026     0.061516\n","4      0.000000              0.000000  ...     0.299701     0.153133\n","\n","[5 rows x 100 columns]"]},"metadata":{"tags":[]},"execution_count":20}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"EA6mBFloPITN","executionInfo":{"status":"ok","timestamp":1612277715736,"user_tz":180,"elapsed":694,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"db194522-2c0b-41e6-9f82-1c7da4f355d1"}},{"cell_type":"markdown","source":["**the vectorizer should only be fit on the train set, never on your test set.**"],"metadata":{"id":"AECdVM6EPLCX"}},{"cell_type":"markdown","source":["## N-grams"],"metadata":{"id":"b9JMxkvcPcie"}},{"cell_type":"markdown","source":["### Using longer n-grams"],"metadata":{"id":"-zyGT56PQ4Cw"}},{"cell_type":"markdown","source":["<div class=\"\"><p>So far you have created features based on individual words in each of the texts. This can be quite powerful when used in a machine learning model but you may be concerned that by looking at words individually a lot of the context is being ignored. To deal with this when creating models you can use n-grams which are sequence of n words grouped together. For example:</p>\r\n","<ul>\r\n","<li>bigrams: Sequences of two consecutive words</li>\r\n","<li>trigrams: Sequences of two consecutive words   </li>\r\n","</ul>\r\n","<p>These can be automatically created in your dataset by specifying the <code>ngram_range</code> argument as a tuple <code>(n1, n2)</code> where all n-grams in the <code>n1</code> to <code>n2</code> range are included.</p></div>"],"metadata":{"id":"r2ENKbUnQ7Gh"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import <code>CountVectorizer</code> from <code>sklearn.feature_extraction.text</code>.  </li>\r\n","<li>Instantiate <code>CountVectorizer</code> while considering only trigrams.  </li>\r\n","<li>Fit the vectorizer and apply it to the <code>text_clean</code> column in one step.  </li>\r\n","<li>Print the feature names generated by the vectorizer.</li>\r\n","</ul>"],"metadata":{"id":"xXI-zuVfQ89X"}},{"cell_type":"code","execution_count":23,"source":["# Import CountVectorizer\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","# Instantiate a trigram vectorizer\r\n","cv_trigram_vec = CountVectorizer(max_features=100, \r\n","                                 stop_words='english', \r\n","                                 ngram_range = (3,3))\r\n","\r\n","# Fit and apply trigram vectorizer\r\n","cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\r\n","\r\n","# Print the trigram features\r\n","cv_trigram_vec.get_feature_names()[:5]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ability preserve protect',\n"," 'agriculture commerce manufactures',\n"," 'america ideal freedom',\n"," 'amity mutual concession',\n"," 'anchor peace home']"]},"metadata":{"tags":[]},"execution_count":23}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWnLSuKmR5VY","executionInfo":{"status":"ok","timestamp":1612278400449,"user_tz":180,"elapsed":710,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"3ba348d5-faae-4809-c91e-4e8e09f2012a"}},{"cell_type":"markdown","source":["**Here you can see that by taking sequential word pairings, some context is preserved.**"],"metadata":{"id":"V4ecuqEDSELW"}},{"cell_type":"markdown","source":["### Finding the most common words"],"metadata":{"id":"fxfSefNUSQb1"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Its always advisable once you have created your features to inspect them to ensure that they are as you would expect. This will allow you to catch errors early, and perhaps influence what further feature engineering you will need to do.   </p>\r\n","<p>The vectorizer (<code>cv</code>) you fit in the last exercise and the sparse array consisting of word counts (<code>cv_trigram</code>) is available in your workspace.</p></div>"],"metadata":{"id":"_CE6ROk5STX8"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a DataFrame of the features (word counts). </li>\r\n","<li>Add the counts of word occurrences and print the top 5 most occurring words.</li>\r\n","</ul>"],"metadata":{"id":"0-ITwrXESVo1"}},{"cell_type":"code","execution_count":25,"source":["# Create a DataFrame of the features\r\n","cv_tri_df = pd.DataFrame(cv_trigram.toarray(), \r\n","                 columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\r\n","\r\n","# Print the top 5 words in the sorted output\r\n","print(cv_tri_df.sum().sort_values(ascending=False).head())"],"outputs":[{"output_type":"stream","name":"stdout","text":["Counts_constitution united states    20\n","Counts_people united states          13\n","Counts_preserve protect defend       10\n","Counts_mr chief justice              10\n","Counts_president united states        8\n","dtype: int64\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNsIxF4pSkZ7","executionInfo":{"status":"ok","timestamp":1612278564242,"user_tz":180,"elapsed":655,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b186da03-f4eb-42f3-e943-59f154158620"}},{"cell_type":"markdown","source":["**that the most common trigram is constitution united states makes a lot of sense for US presidents speeches.**"],"metadata":{"id":"xJu84OU5SnWC"}}]}