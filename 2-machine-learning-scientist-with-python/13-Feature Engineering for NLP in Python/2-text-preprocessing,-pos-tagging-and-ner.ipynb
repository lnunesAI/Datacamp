{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2-text-preprocessing,-pos-tagging-and-ner.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM5MYjlSMYH92JKnV5AoPIz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Text preprocessing, POS tagging and NER\r\n",">  In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Python, Datacamp, Machine Learning]\r\n","- image: images/datacamp/1_supervised_learning_with_scikit_learn/2_regression.png"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 2 exercises \"Feature Engineering for NLP in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import spacy\r\n","plt.rcParams['figure.figsize'] = (8, 8)"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1612808466851,"user_tz":180,"elapsed":1409,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Tokenization and Lemmatization"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### Identifying lemmas"],"metadata":{"id":"-731z7jtESuq"}},{"cell_type":"markdown","source":["<p>Identify the list of words from the choices which do not have the same lemma.</p>"],"metadata":{"id":"c75hHcKgEWhG"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","He, She, I, They\r\n","\r\n","\r\n","Am, Are, Is, Was\r\n","\r\n","\r\n","Increase, Increases, Increasing, Increased\r\n","\r\n","\r\n","<b>Car, Bike, Truck, Bus</b>\r\n","\r\n","</pre>"],"metadata":{"id":"utV_scS9EoY7"}},{"cell_type":"markdown","source":["**Although all these words refer to vehicles, they are words with distinct base forms.**"],"metadata":{"id":"dwzHdo5XEmLL"}},{"cell_type":"markdown","source":["### Tokenizing the Gettysburg Address"],"metadata":{"id":"V_EaM_ktFuTA"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.</p>\r\n","<p>The entire speech is available as a string named <code>gettysburg</code>.</p></div>"],"metadata":{"id":"mV6LffmEFuNQ"}},{"cell_type":"code","execution_count":null,"source":["gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\""],"outputs":[],"metadata":{"id":"rP79MjM4F7YL"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Load the <code>en_core_web_sm</code> model using <code>spacy.load()</code>.</li>\r\n","<li>Create a Doc object <code>doc</code> for the <code>gettysburg</code> string.</li>\r\n","<li>Using list comprehension, loop over <code>doc</code> to generate the token texts.</li>\r\n","</ul>"],"metadata":{"id":"pP4S81jwFycK"}},{"cell_type":"code","execution_count":null,"source":["# Load the en_core_web_sm model\r\n","nlp = spacy.load('en_core_web_sm')\r\n","\r\n","# Create a Doc object\r\n","doc = nlp(gettysburg)\r\n","\r\n","# Generate the tokens\r\n","tokens = [token.text for token in doc]\r\n","print(tokens)"],"outputs":[{"output_type":"stream","name":"stdout","text":["['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Nf0qmwtGbHe","executionInfo":{"status":"ok","timestamp":1612795524382,"user_tz":180,"elapsed":2348,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d1ca5495-92c3-466b-e3b1-eda1cb736e42"}},{"cell_type":"markdown","source":["**You now know how to tokenize a piece of text. In the next exercise, we will perform similar steps and conduct lemmatization.**"],"metadata":{"id":"GGhRY6vyGeEH"}},{"cell_type":"markdown","source":["### Lemmatizing the Gettysburg address"],"metadata":{"id":"CjXFeW-FGu2L"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, we will perform lemmatization on the same <code>gettysburg</code> address from before. </p>\r\n","<p>However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly.</p></div>"],"metadata":{"id":"Hj3z2Ia1GxPd"}},{"cell_type":"markdown","source":["Instructions 1/3\r\n","<p>Print the gettysburg address to the console.</p>"],"metadata":{"id":"8ed3KFBnGzOD"}},{"cell_type":"code","execution_count":null,"source":["# Print the gettysburg address\r\n","print(gettysburg)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"syeEKdDVHA-h","executionInfo":{"status":"ok","timestamp":1612795615341,"user_tz":180,"elapsed":1147,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"4dbceedf-2f17-4f5c-d75c-a33ab27272ec"}},{"cell_type":"markdown","source":["Instructions 2/3\r\n","<p>Loop over <code>doc</code> and extract the lemma for each token of <code>gettysburg</code>.</p>"],"metadata":{"id":"7iGA8QoEGzOJ"}},{"cell_type":"code","execution_count":null,"source":["# Create a Doc object\r\n","doc = nlp(gettysburg)\r\n","\r\n","# Generate lemmas\r\n","lemmas = [token.lemma_ for token in doc]"],"outputs":[],"metadata":{"id":"G5Eenx5_HcEq"}},{"cell_type":"markdown","source":["Instructions 3/3\r\n","<p>Convert <code>lemmas</code> into a string using <code>join</code>.</p>"],"metadata":{"id":"jGmkPfmVGzOJ"}},{"cell_type":"code","execution_count":null,"source":["# Convert lemmas into a string\r\n","print(' '.join(lemmas))"],"outputs":[{"output_type":"stream","name":"stdout","text":["four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooSLXcQ6HszI","executionInfo":{"status":"ok","timestamp":1612795794303,"user_tz":180,"elapsed":1104,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"81c38b71-82e5-4b8c-f299-0303f3c17a51"}},{"cell_type":"markdown","source":["**You're now proficient at performing lemmatization using spaCy. Observe the lemmatized version of the speech. It isn't very readable to humans but it is in a much more convenient format for a machine to process.**"],"metadata":{"id":"0gMZ_-TNH8oq"}},{"cell_type":"markdown","source":["## Text cleaning"],"metadata":{"id":"mHJYhq7uH_Ac"}},{"cell_type":"markdown","source":["### Cleaning a blog post"],"metadata":{"id":"t9c7V7KCJAih"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.</p>\r\n","<p>The excerpt is available as a string <code>blog</code> and has been printed to the console. The list of stopwords are available as <code>stopwords</code>.</p></div>"],"metadata":{"id":"HhoAh_Q9JEdB"}},{"cell_type":"code","execution_count":null,"source":["stopwords = spacy.lang.en.stop_words.STOP_WORDS\r\n","blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'"],"outputs":[],"metadata":{"id":"3S0D7PqVJh0J"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Using list comprehension, loop through <code>doc</code> to extract the <code>lemma_</code> of each token.</li>\r\n","<li>Remove stopwords and non-alphabetic tokens using <code>stopwords</code> and <code>isalpha()</code>.</li>\r\n","</ul>"],"metadata":{"id":"Iiuj4E1gJGNi"}},{"cell_type":"code","execution_count":null,"source":["# Load model and create Doc object\r\n","nlp = spacy.load('en_core_web_sm')\r\n","doc = nlp(blog)\r\n","\r\n","# Generate lemmatized tokens\r\n","lemmas = [token.lemma_ for token in doc]\r\n","\r\n","# Remove stopwords and non-alphabetic tokens\r\n","a_lemmas = [lemma for lemma in lemmas \r\n","            if lemma.isalpha() and lemma not in stopwords]\r\n","\r\n","# Print string after text cleaning\r\n","print(' '.join(a_lemmas))"],"outputs":[{"output_type":"stream","name":"stdout","text":["century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-3ckrjLJdJ4","executionInfo":{"status":"ok","timestamp":1612796329077,"user_tz":180,"elapsed":1062,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ca7f78d8-2705-4a6c-eea9-63d13692a3ca"}},{"cell_type":"markdown","source":["**Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases.**"],"metadata":{"id":"vFgFl596JefB"}},{"cell_type":"markdown","source":["### Cleaning TED talks in a dataframe"],"metadata":{"id":"8UU7ywTkKFb8"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe <code>ted</code> consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function <code>preprocess</code> and applying it to the <code>transcript</code> feature of the dataframe. </p>\r\n","<p>The stopwords list is available as <code>stopwords</code>.</p></div>"],"metadata":{"id":"pGOdMUAnKHid"}},{"cell_type":"code","execution_count":null,"source":["ted = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/13-Feature%20Engineering%20for%20NLP%20in%20Python/datasets/ted_500x1.csv')"],"outputs":[],"metadata":{"id":"T-VRRcvJKWvb"}},{"cell_type":"code","execution_count":null,"source":["ted = ted[:20]"],"outputs":[],"metadata":{"id":"ki7z42_AKmlG"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Generate the Doc object for <code>text</code>. Ignore the <code>disable</code> argument for now.</li>\r\n","<li>Generate lemmas using list comprehension using the <code>lemma_</code> attribute.</li>\r\n","<li>Remove non-alphabetic characters using <code>isalpha()</code> in the if condition.</li>\r\n","</ul>"],"metadata":{"id":"vlPqkE04KJUm"}},{"cell_type":"code","execution_count":null,"source":["# Function to preprocess text\r\n","def preprocess(text):\r\n","  \t# Create Doc object\r\n","    doc = nlp(text, disable=['ner', 'parser'])\r\n","    # Generate lemmas\r\n","    lemmas = [token.lemma_ for token in doc]\r\n","    # Remove stopwords and non-alphabetic characters\r\n","    a_lemmas = [lemma for lemma in lemmas \r\n","            if lemma.isalpha() and lemma not in stopwords]\r\n","    \r\n","    return ' '.join(a_lemmas)\r\n","  \r\n","# Apply preprocess to ted['transcript']\r\n","ted['transcript'] = ted['transcript'].apply(preprocess)\r\n","print(ted['transcript'])"],"outputs":[{"output_type":"stream","name":"stdout","text":["0     talk new lecture TED illusion create TED try r...\n","1     representation brain brain break left half log...\n","2     great honor today share Digital Universe creat...\n","3     passion music technology thing combination thi...\n","4     use want computer new program programming requ...\n","5     neuroscientist mixed background physics medici...\n","6     Pat Mitchell day January begin like work love ...\n","7     Taylor Wilson year old nuclear physicist littl...\n","8     grow Northern Ireland right north end absolute...\n","9     publish article New York Times Modern Love col...\n","10    Joseph Member Parliament Kenya picture Maasai ...\n","11    hi talk little bit music machine life specific...\n","12    hi let ask audience question lie child raise h...\n","13    historical record allow know ancient Greeks dr...\n","14    good morning little boy experience change life...\n","15    slide year ago time short slide morning time w...\n","16    like world like share year old love story poor...\n","17    fail woman fail feminist passionate opinion ge...\n","18    revolution century significant longevity revol...\n","19    today baffle lady observe shell soul dwellsAnd...\n","Name: transcript, dtype: object\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KQKS97JKV_R","executionInfo":{"status":"ok","timestamp":1612796596784,"user_tz":180,"elapsed":2782,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ce790f25-0a3e-4b6a-adbd-74d92d4b6214"}},{"cell_type":"markdown","source":["**You have preprocessed all the TED talk transcripts contained in ted and it is now in a good shape to perform operations such as vectorization (as we will soon see how). You now have a good understanding of how text preprocessing works and why it is important. In the next lessons, we will move on to generating word level features for our texts.**"],"metadata":{"id":"09RStn8zKZfK"}},{"cell_type":"markdown","source":["## Part-of-speech tagging"],"metadata":{"id":"XHSL-v14LLPh"}},{"cell_type":"markdown","source":["### POS tagging in Lord of the Flies"],"metadata":{"id":"-pEKI4MjMbdZ"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, <em>Lord of the Flies</em>, authored by William Golding.</p>\r\n","<p>The passage is available as <code>lotf</code> and has already been printed to the console.</p></div>"],"metadata":{"id":"ZbsDq96CMeOL"}},{"cell_type":"code","execution_count":null,"source":["lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'"],"outputs":[],"metadata":{"id":"c6WWQ1jQMlKj"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Load the <code>en_core_web_sm</code> model.</li>\r\n","<li>Create a doc object for <code>lotf</code> using <code>nlp()</code>.</li>\r\n","<li>Using the <code>text</code> and <code>pos_</code> attributes, generate tokens and their corresponding POS tags.</li>\r\n","</ul>"],"metadata":{"id":"UMRLn8OYMfy7"}},{"cell_type":"code","execution_count":null,"source":["# Load the en_core_web_sm model\r\n","nlp = spacy.load('en_core_web_sm')\r\n","\r\n","# Create a Doc object\r\n","doc = nlp(lotf)\r\n","\r\n","# Generate tokens and pos tags\r\n","pos = [(token.text, token.pos_) for token in doc]\r\n","print(pos)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzeyOGN5MtMf","executionInfo":{"status":"ok","timestamp":1612797116972,"user_tz":180,"elapsed":1300,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c46bcd6a-cea2-4dc0-a842-74345e661d91"}},{"cell_type":"markdown","source":["**Examine the various POS tags attached to each token and evaluate if they make intuitive sense to you. You will notice that they are indeed labelled correctly according to the standard rules of English grammar.**"],"metadata":{"id":"ZhvJLL4BM0Sx"}},{"cell_type":"markdown","source":["### Counting nouns in a piece of text"],"metadata":{"id":"vJMZxxf9M6er"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, we will write two functions, <code>nouns()</code> and <code>proper_nouns()</code> that will count the number of other nouns and proper nouns in a piece of text respectively.</p>\r\n","<p>These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news. </p>\r\n","<p>The <code>en_core_web_sm</code> model has already been loaded as <code>nlp</code> in this exercise.</p></div>"],"metadata":{"id":"zX71RcyNM9W7"}},{"cell_type":"markdown","source":["Instructions 1/2\r\n","<p>Using the list <code>count</code> method, count the number of proper nouns (annotated as <code>PROPN</code>) in the <code>pos</code> list.</p>"],"metadata":{"id":"F9kkC8H-NADG"}},{"cell_type":"code","execution_count":null,"source":["nlp = spacy.load('en_core_web_sm')\r\n","\r\n","# Returns number of proper nouns\r\n","def proper_nouns(text, model=nlp):\r\n","  \t# Create doc object\r\n","    doc = model(text)\r\n","    # Generate list of POS tags\r\n","    pos = [token.pos_ for token in doc]\r\n","    \r\n","    # Return number of proper nouns\r\n","    return pos.count('PROPN')\r\n","\r\n","print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"],"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIeMJyp_NQLp","executionInfo":{"status":"ok","timestamp":1612797408577,"user_tz":180,"elapsed":1215,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"a99e1c1a-d79d-4fe6-b640-8946a0d1bc12"}},{"cell_type":"markdown","source":["Instructions 2/2\r\n","<p>Using the list <code>count</code> method, count the number of other nouns (annotated as <code>NOUN</code>) in the <code>pos</code> list.</p>"],"metadata":{"id":"V4B-KuEiNADO"}},{"cell_type":"code","execution_count":null,"source":["# Returns number of other nouns\r\n","def nouns(text, model=nlp):\r\n","  \t# Create doc object\r\n","    doc = model(text)\r\n","    # Generate list of POS tags\r\n","    pos = [token.pos_ for token in doc]\r\n","    \r\n","    # Return number of other nouns\r\n","    return pos.count('NOUN')\r\n","\r\n","print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"],"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5z8hqkFTNuJb","executionInfo":{"status":"ok","timestamp":1612797376032,"user_tz":180,"elapsed":606,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ce577af6-5c01-4d44-84be-5d2822c007ed"}},{"cell_type":"markdown","source":["**You now know how to write functions that compute the number of instances of a particulat POS tag in a given piece of text. In the next exercise, we will use these functions to generate features from text in a dataframe.**"],"metadata":{"id":"odSrqj-dN0YG"}},{"cell_type":"markdown","source":["### Noun usage in fake news"],"metadata":{"id":"Yck56fVuN7C2"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, you have been given a dataframe <code>headlines</code> that contains news headlines that are either fake or real. Your task is to generate two new features <code>num_propn</code> and <code>num_noun</code> that represent the number of proper nouns and other nouns contained in the <code>title</code> feature of <code>headlines</code>.</p>\r\n","<p>Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the <code>num_propn</code> and <code>num_noun</code> features in fake news detectors will improve its performance.</p>\r\n","<p>To accomplish this task, the functions <code>proper_nouns</code> and <code>nouns</code> that you had built in the previous exercise have already been made available to you.</p></div>"],"metadata":{"id":"WaRaFm0XN9t_"}},{"cell_type":"code","execution_count":null,"source":["headlines = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/13-Feature%20Engineering%20for%20NLP%20in%20Python/datasets/headlines_100x3.csv')"],"outputs":[],"metadata":{"id":"yBROiq38PWzi"}},{"cell_type":"markdown","source":["Instructions 1/2\r\n","<ul>\r\n","<li>Create a new feature <code>num_propn</code> by applying <code>proper_nouns</code> to <code>headlines['title']</code>.</li>\r\n","<li>Filter <code>headlines</code> to compute the mean number of proper nouns in fake news using the <code>mean</code> method.</li>\r\n","</ul>"],"metadata":{"id":"PvzaWNQtOAMu"}},{"cell_type":"code","execution_count":null,"source":["headlines['num_propn'] = headlines['title'].apply(proper_nouns)\r\n","\r\n","# Compute mean of proper nouns\r\n","real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\r\n","fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\r\n","\r\n","# Print results\r\n","print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mean no. of proper nouns in real and fake headlines are 2.37 and 4.35 respectively\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YczKWr9BOPp9","executionInfo":{"status":"ok","timestamp":1612797840413,"user_tz":180,"elapsed":1623,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"75468164-4030-4790-ecb4-4cec69dd596f"}},{"cell_type":"markdown","source":["Instructions 2/2\r\n","<li>Repeat the process for other nous: create a feature <code>'num_noun'</code> using <code>nouns</code> and compute the mean of other nouns</li>"],"metadata":{"id":"8lYlpj-wOAM9"}},{"cell_type":"code","execution_count":null,"source":["headlines['num_noun'] = headlines['title'].apply(nouns)\r\n","\r\n","# Compute mean of other nouns\r\n","real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\r\n","fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\r\n","\r\n","# Print results\r\n","print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mean no. of other nouns in real and fake headlines are 2.32 and 1.84 respectively\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJQ52cDgOl3l","executionInfo":{"status":"ok","timestamp":1612797836229,"user_tz":180,"elapsed":1647,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"6fb0b67d-de71-4d1e-f2b2-10435fe8273d"}},{"cell_type":"markdown","source":["**You now know to construct features using POS tags information. Notice how the mean number of proper nouns is considerably higher for fake news than it is for real news. The opposite seems to be true in the case of other nouns. This fact can be put to great use in designing fake news detectors.**"],"metadata":{"id":"5PpUlbReOnok"}},{"cell_type":"markdown","source":["### Named entity recognition"],"metadata":{"id":"eaZlbdfcQesg"}},{"cell_type":"markdown","source":["### Named entities in a sentence"],"metadata":{"id":"8wVgP-6H3fAZ"}},{"cell_type":"markdown","source":["<p>In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models. We will also verify the veracity of these labels.</p>"],"metadata":{"id":"oVPkPoWp3h0B"}},{"cell_type":"code","execution_count":4,"source":["# Load the required model\r\n","nlp = spacy.load('en_core_web_sm')\r\n","\r\n","# Create a Doc instance \r\n","text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\r\n","doc = nlp(text)\r\n","\r\n","# Print all named entities and their labels\r\n","for ent in doc.ents:\r\n","    print(ent.text, ent.label_)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Sundar Pichai PERSON\n","Google ORG\n","Mountain View GPE\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5fbdDDG34iJ","executionInfo":{"status":"ok","timestamp":1612808473660,"user_tz":180,"elapsed":1454,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"6e06661f-8975-4a2e-b82b-290dc8944257"}},{"cell_type":"markdown","source":["**Notice how the model correctly predicted the labels of Google and Mountain View but mislabeled Sundar Pichai as an organization. As discussed in the video, the predictions of the model depend strongly on the data it is trained on. It is possible to train spaCy models on your custom data. You will learn to do this in more advanced NLP courses.**"],"metadata":{"id":"ajTkxzzQ4Ik-"}},{"cell_type":"markdown","source":["### Identifying people mentioned in a news article"],"metadata":{"id":"ueNk-mpk4wgI"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, you have been given an excerpt from a news article published in <em>TechCrunch</em>. Your task is to write a function <code>find_people</code> that identifies the names of people that have been mentioned in a particular piece of text. You will then use <code>find_people</code> to identify the people of interest in the article.</p>\r\n","<p>The article is available as the string <code>tc</code> and has been printed to the console. The required spacy model has also been already loaded as <code>nlp</code>.</p></div>"],"metadata":{"id":"r7dElFr34y6i"}},{"cell_type":"code","execution_count":5,"source":["tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\""],"outputs":[],"metadata":{"id":"PUO3ey7-57Ef","executionInfo":{"status":"ok","timestamp":1612808966933,"user_tz":180,"elapsed":637,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a Doc object for <code>text</code>.</li>\r\n","<li>Using list comprehension, loop through <code>doc.ents</code> and create a list of named entities whose label is <code>PERSON</code>.</li>\r\n","<li>Using <code>find_persons()</code>, print the people mentioned in <code>tc</code>.</li>\r\n","</ul>"],"metadata":{"id":"222Q0hQA41yK"}},{"cell_type":"code","execution_count":29,"source":["def find_persons(text):\r\n","  # Create Doc object\r\n","  doc = nlp(text)\r\n","  \r\n","  # Identify the persons\r\n","  persons = [ent.text  for ent in doc.ents if ent.label_ == 'PERSON']\r\n","  \r\n","  # Return persons\r\n","  return persons\r\n","\r\n","print(find_persons(tc))"],"outputs":[{"output_type":"stream","name":"stdout","text":["['Sheryl Sandberg', 'Mark Zuckerberg']\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ume_PBXB55zv","executionInfo":{"status":"ok","timestamp":1612809188062,"user_tz":180,"elapsed":786,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"5d5f6381-370a-4b51-ee0b-33987b73fa5e"}},{"cell_type":"markdown","source":["**The article was related to Facebook and our function correctly identified both the people mentioned. You can now see how NER could be used in a variety of applications. Publishers may use a technique like this to classify news articles by the people mentioned in them. A question answering system could also use something like this to answer questions such as 'Who are the people mentioned in this passage?'. With this, we come to an end of this chapter. In the next, we will learn how to conduct vectorization on documents.**"],"metadata":{"id":"_diSvCFt54SP"}}]}