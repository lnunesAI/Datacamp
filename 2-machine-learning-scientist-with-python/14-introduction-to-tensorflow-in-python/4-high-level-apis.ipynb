{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"4-high-level-apis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPGjOz6nqHOx2Bvk+yINY+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# High Level APIs\r\n",">  In the final chapter, you'll use high-level APIs in TensorFlow 2 to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, make predictions with, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process, and to avoid errors.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Datacamp]\r\n","- image: images/datacamp/___"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 4 exercises \"Introduction to TensorFlow in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["import h5py\r\n","import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import tensorflow as tf\r\n","from tensorflow import Variable, matmul, ones, keras, constant, float32, random\r\n","plt.rcParams['figure.figsize'] = (8, 8)"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1613137317658,"user_tz":180,"elapsed":584,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Defining neural networks with Keras"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### The sequential model in Keras"],"metadata":{"id":"Ho1eZTss8AvH"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In chapter 3, we used components of the <code>keras</code> API in <code>tensorflow</code> to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the <code>keras</code> sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the <code>.summary()</code> method to print the model's architecture, including the shape and number of parameters associated with each layer.</p>\r\n","<p>Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that <code>keras</code> has been imported from <code>tensorflow</code> for you.</p></div>"],"metadata":{"id":"PqMj_lKMUeYi"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Define a <code>keras</code> sequential model named <code>model</code>.</li>\r\n","<li>Set the first layer to be <code>Dense()</code> and to have 16 nodes and a <code>relu</code> activation.</li>\r\n","<li>Define the second layer to be <code>Dense()</code> and to have 8 nodes and a <code>relu</code> activation.</li>\r\n","<li>Set the output layer to have 4 nodes and use a <code>softmax</code> activation function.</li>\r\n","</ul>"],"metadata":{"id":"L8kZeHcJUg4J"}},{"cell_type":"code","execution_count":null,"source":["# Define a Keras sequential model\r\n","model = keras.Sequential()\r\n","\r\n","# Define the first dense layer\r\n","model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\r\n","\r\n","# Define the second dense layer\r\n","model.add(keras.layers.Dense(8, activation='relu'))\r\n","\r\n","# Define the output layer\r\n","model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","# Print the model architecture\r\n","print(model.summary())"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 16)                12560     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 8)                 136       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 36        \n","=================================================================\n","Total params: 12,732\n","Trainable params: 12,732\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"metadata":{"id":"w48_1NTN8ASP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613084577238,"user_tz":180,"elapsed":1557,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"46f8a410-63d7-4f2f-d82c-6ecb79b23609"}},{"cell_type":"markdown","source":["**Notice that we've defined a model, but we haven't compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model's architecture.**"],"metadata":{"id":"yfJjdfDaVbmU"}},{"cell_type":"markdown","source":["### Compiling a sequential model"],"metadata":{"id":"_CHKh90ZVdhb"}},{"cell_type":"markdown","source":["<p>In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the <code>adam</code> optimizer and the <code>categorical_crossentropy</code> loss. You will also use a method in <code>keras</code> to summarize your model's architecture. Note that <code>keras</code> has been imported from <code>tensorflow</code> for you and a sequential <code>keras</code> model has been defined as <code>model</code>.</p>"],"metadata":{"id":"_c67l7rTVfxE"}},{"cell_type":"code","execution_count":null,"source":["model = keras.Sequential()"],"outputs":[],"metadata":{"id":"34upfJ71WRHr"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>In the first dense layer, set the number of nodes to 16, the activation to <code>sigmoid</code>, and the <code>input_shape</code> to (784,).</li>\r\n","<li>Apply dropout at a rate of 25% to the first layer's output.</li>\r\n","<li>Set the output layer to be dense, have 4 nodes, and use a <code>softmax</code> activation function.</li>\r\n","<li>Compile the model using an <code>adam</code> optimizer and <code>categorical_crossentropy</code> loss function.</li>\r\n","</ul>"],"metadata":{"id":"GlB3kDGxVhkk"}},{"cell_type":"code","execution_count":null,"source":["# Define the first dense layer\r\n","model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\r\n","\r\n","# Apply dropout to the first layer's output\r\n","model.add(keras.layers.Dropout(0.25))\r\n","\r\n","# Define the output layer\r\n","model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","# Compile the model\r\n","model.compile('adam', loss='categorical_crossentropy')\r\n","\r\n","# Print a model summary\r\n","print(model.summary())"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_5 (Dense)              (None, 16)                12560     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 16)                0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 4)                 68        \n","=================================================================\n","Total params: 12,628\n","Trainable params: 12,628\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYixijEKV-Pr","executionInfo":{"status":"ok","timestamp":1613084828021,"user_tz":180,"elapsed":846,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c7a6d500-4f56-421a-f7bf-6fbb06ba1102"}},{"cell_type":"markdown","source":["**You've now defined and compiled a neural network using the keras sequential model. Notice that printing the .summary() method shows the layer type, output shape, and number of parameters of each layer.**"],"metadata":{"id":"l7hnJ7y5WZVT"}},{"cell_type":"markdown","source":["### Defining a multiple input model"],"metadata":{"id":"cQwlwpRfWcT_"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the <code>.summary()</code> method to examine the joint model's architecture.</p>\r\n","<p>Note that <code>keras</code> has been imported from <code>tensorflow</code> for you. Additionally, the input layers of the first and second models have been defined as <code>m1_inputs</code> and <code>m2_inputs</code>, respectively. Note that the two models have the same architecture, but one of them uses a <code>sigmoid</code> activation in the first layer and the other uses a <code>relu</code>.</p></div>"],"metadata":{"id":"poVCpb2wWeUH"}},{"cell_type":"code","execution_count":null,"source":["m1_inputs = keras.Input(shape=(784,))\r\n","m2_inputs = keras.Input(shape=(784,))\r\n","keras.backend.clear_session()"],"outputs":[],"metadata":{"id":"o2u0sOpoacsK"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Pass model 1's input layer to its first layer and model 1's first layer to its second layer.</li>\r\n","<li>Pass model 2's input layer to its first layer and model 2's first layer to its second layer.</li>\r\n","<li>Use the <code>add()</code> operation to combine the second layers of model 1 and model 2.</li>\r\n","<li>Complete the functional model definition.</li>\r\n","</ul>"],"metadata":{"id":"ybanawP2Wf0_"}},{"cell_type":"code","execution_count":null,"source":["# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\r\n","m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\r\n","m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\r\n","\r\n","# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\r\n","m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\r\n","m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\r\n","\r\n","# Merge model outputs and define a functional model\r\n","merged = keras.layers.add([m1_layer2, m2_layer2])\r\n","model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\r\n","\r\n","# Print a model summary\r\n","print(model.summary())"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 784)]        0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 784)]        0                                            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 12)           9420        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 12)           9420        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 4)            52          dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 4)            52          dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 4)            0           dense_1[0][0]                    \n","                                                                 dense_3[0][0]                    \n","==================================================================================================\n","Total params: 18,944\n","Trainable params: 18,944\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvMUAafiaVgY","executionInfo":{"status":"ok","timestamp":1613086380910,"user_tz":180,"elapsed":856,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d7a0e177-8008-4cdd-f0a2-201b07da44be"}},{"cell_type":"markdown","source":["**Notice that the .summary() method yields a new column: connected to. This column tells you how layers connect to each other within the network. We can see that dense_2, for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3.**"],"metadata":{"id":"eojVRLGoaYJB"}},{"cell_type":"markdown","source":["## Training and validation with Keras"],"metadata":{"id":"6xegy1hYceVg"}},{"cell_type":"markdown","source":["### Training with Keras"],"metadata":{"id":"dqEAdGSFmQny"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.</p>\r\n","<p>Note that <code>keras</code> has been imported from <code>tensorflow</code> for you. Additionally, the features are available as <code>sign_language_features</code> and the targets are available as <code>sign_language_labels</code>.</p></div>"],"metadata":{"id":"n_ckxbj9mOl6"}},{"cell_type":"code","execution_count":null,"source":["%%capture\r\n","!wget https://github.com/lnunesAI/Datacamp/raw/main/2-machine-learning-scientist-with-python/14-introduction-to-tensorflow-in-python/datasets/letters_images.h5\r\n","h5f = h5py.File('letters_images.h5','r')\r\n","sign_language_features = h5f['sign_language_features'][:]\r\n","sign_language_labels = h5f['sign_language_labels'][:]\r\n","h5f.close()"],"outputs":[],"metadata":{"id":"a4WB7yD3tyqw"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Define a sequential model named <code>model</code>.</li>\r\n","<li>Set the output layer to be dense, have 4 nodes, and use a <code>softmax</code> activation function.</li>\r\n","<li>Compile the model with the <code>SGD</code> optimizer and <code>categorical_crossentropy</code> loss.</li>\r\n","<li>Complete the fitting operation and set the number of epochs to 5.</li>\r\n","</ul>"],"metadata":{"id":"3QkLzwO8mTXr"}},{"cell_type":"code","execution_count":null,"source":["# Define a sequential model\r\n","model = keras.Sequential()\r\n","\r\n","# Define a hidden layer\r\n","model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\r\n","\r\n","# Define the output layer\r\n","model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","# Compile the model\r\n","model.compile('SGD', loss='categorical_crossentropy')\r\n","\r\n","# Complete the fitting operation\r\n","model.fit(sign_language_features, sign_language_labels, epochs=5)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","32/32 [==============================] - 0s 1ms/step - loss: 1.4010\n","Epoch 2/5\n","32/32 [==============================] - 0s 1ms/step - loss: 1.2036\n","Epoch 3/5\n","32/32 [==============================] - 0s 1ms/step - loss: 1.0344\n","Epoch 4/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.9183\n","Epoch 5/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.7867\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f78bac10630>"]},"metadata":{"tags":[]},"execution_count":113}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLjC_e-Umxdx","executionInfo":{"status":"ok","timestamp":1613093259156,"user_tz":180,"elapsed":1186,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"069e39f8-fb31-41e2-aa40-0bf01ead8d76"}},{"cell_type":"markdown","source":["**You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise.**"],"metadata":{"id":"V0PRfD4lmzcU"}},{"cell_type":"markdown","source":["### Metrics and validation with Keras"],"metadata":{"id":"4RBfE0rs3Ow6"}},{"cell_type":"markdown","source":["<div class=\"\"><p>We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation.</p>\r\n","<p>Note that <code>keras</code> has been imported for you from <code>tensorflow</code>.</p></div>"],"metadata":{"id":"bg06OktZ3eZ3"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Set the first dense layer to have 32 nodes, use a <code>sigmoid</code> activation function, and have an input shape of (784,).</li>\r\n","<li>Use the root mean square propagation optimizer, a categorical crossentropy loss, and the accuracy metric.</li>\r\n","<li>Set the number of epochs to 10 and use 10% of the dataset for validation.</li>\r\n","</ul>"],"metadata":{"id":"LBtLuaiy3gMv"}},{"cell_type":"code","execution_count":null,"source":["# Define sequential model\r\n","model = keras.Sequential()\r\n","\r\n","# Define the first layer\r\n","model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\r\n","\r\n","# Add activation function to classifier\r\n","model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","# Set the optimizer, loss function, and metrics\r\n","model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\n","\r\n","# Add the number of epochs and the validation split\r\n","model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=.1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","29/29 [==============================] - 1s 9ms/step - loss: 1.3699 - accuracy: 0.3603 - val_loss: 1.3646 - val_accuracy: 0.2800\n","Epoch 2/10\n","29/29 [==============================] - 0s 2ms/step - loss: 1.0493 - accuracy: 0.6165 - val_loss: 1.0644 - val_accuracy: 0.5600\n","Epoch 3/10\n","29/29 [==============================] - 0s 3ms/step - loss: 0.9206 - accuracy: 0.7223 - val_loss: 0.9979 - val_accuracy: 0.5300\n","Epoch 4/10\n","29/29 [==============================] - 0s 2ms/step - loss: 0.7874 - accuracy: 0.8076 - val_loss: 0.7938 - val_accuracy: 0.6100\n","Epoch 5/10\n","29/29 [==============================] - 0s 3ms/step - loss: 0.6995 - accuracy: 0.8249 - val_loss: 0.6022 - val_accuracy: 0.8700\n","Epoch 6/10\n","29/29 [==============================] - 0s 7ms/step - loss: 0.5811 - accuracy: 0.9085 - val_loss: 0.7554 - val_accuracy: 0.6500\n","Epoch 7/10\n","29/29 [==============================] - 0s 3ms/step - loss: 0.5209 - accuracy: 0.8898 - val_loss: 0.5172 - val_accuracy: 0.8400\n","Epoch 8/10\n","29/29 [==============================] - 0s 3ms/step - loss: 0.4529 - accuracy: 0.9262 - val_loss: 0.4013 - val_accuracy: 0.9800\n","Epoch 9/10\n","29/29 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.9384 - val_loss: 0.5370 - val_accuracy: 0.7400\n","Epoch 10/10\n","29/29 [==============================] - 0s 3ms/step - loss: 0.3604 - accuracy: 0.9307 - val_loss: 0.3826 - val_accuracy: 0.9400\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f78babed940>"]},"metadata":{"tags":[]},"execution_count":114}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWA89S2D4MGF","executionInfo":{"status":"ok","timestamp":1613093721615,"user_tz":180,"elapsed":2415,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"9c9588a5-df2f-4727-b649-0d8f3d5b8990"}},{"cell_type":"markdown","source":["**With the keras API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of over 90% in the validation sample!**"],"metadata":{"id":"FBVTR1rj4gry"}},{"cell_type":"markdown","source":["### Overfitting detection"],"metadata":{"id":"Sexr_F9d452r"}},{"cell_type":"markdown","source":["<div class=\"\"><p>In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.</p>\r\n","<p>You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting.</p>\r\n","<p>Note that <code>keras</code> has been imported from <code>tensorflow</code>.</p></div>"],"metadata":{"id":"6Da4UfKA49UL"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Define a sequential model in <code>keras</code> named <code>model</code>.</li>\r\n","<li>Add a first dense layer with 1024 nodes, a <code>relu</code> activation, and an input shape of (784,).</li>\r\n","<li>Set the learning rate to 0.001.</li>\r\n","<li>Set the <code>fit()</code> operation to iterate over the full sample 50 times and use 50% of the sample for validation purposes.</li>\r\n","</ul>"],"metadata":{"id":"DwD1AUlm4_xb"}},{"cell_type":"code","execution_count":null,"source":["# Define sequential model\r\n","model = keras.Sequential()\r\n","\r\n","# Define the first layer\r\n","model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\r\n","\r\n","# Add activation function to classifier\r\n","model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","# Finish the model compilation\r\n","model.compile(optimizer=keras.optimizers.Adam(lr=0.001), \r\n","              loss='categorical_crossentropy', metrics=['accuracy'])\r\n","\r\n","# Complete the model fit operation\r\n","model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=.5)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","16/16 [==============================] - 1s 20ms/step - loss: 2.5896 - accuracy: 0.3237 - val_loss: 1.2024 - val_accuracy: 0.5160\n","Epoch 2/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.9984 - accuracy: 0.5884 - val_loss: 0.6594 - val_accuracy: 0.7520\n","Epoch 3/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.6003 - accuracy: 0.8000 - val_loss: 0.5584 - val_accuracy: 0.7320\n","Epoch 4/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.5112 - accuracy: 0.7955 - val_loss: 0.5951 - val_accuracy: 0.7460\n","Epoch 5/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.5120 - accuracy: 0.7862 - val_loss: 0.3491 - val_accuracy: 0.9540\n","Epoch 6/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.3254 - accuracy: 0.9403 - val_loss: 0.2831 - val_accuracy: 0.9820\n","Epoch 7/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.2844 - accuracy: 0.9274 - val_loss: 0.2831 - val_accuracy: 0.8960\n","Epoch 8/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.2412 - accuracy: 0.9554 - val_loss: 0.2537 - val_accuracy: 0.9280\n","Epoch 9/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.2155 - accuracy: 0.9657 - val_loss: 0.2215 - val_accuracy: 0.9620\n","Epoch 10/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1727 - accuracy: 0.9745 - val_loss: 0.1624 - val_accuracy: 0.9800\n","Epoch 11/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1402 - accuracy: 0.9888 - val_loss: 0.1365 - val_accuracy: 0.9940\n","Epoch 12/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1178 - accuracy: 0.9939 - val_loss: 0.1209 - val_accuracy: 0.9980\n","Epoch 13/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1107 - accuracy: 0.9949 - val_loss: 0.1027 - val_accuracy: 0.9940\n","Epoch 14/50\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0892 - accuracy: 0.9954 - val_loss: 0.0964 - val_accuracy: 0.9960\n","Epoch 15/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0814 - accuracy: 0.9891 - val_loss: 0.1241 - val_accuracy: 0.9800\n","Epoch 16/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0793 - accuracy: 0.9971 - val_loss: 0.0775 - val_accuracy: 0.9960\n","Epoch 17/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0635 - accuracy: 0.9989 - val_loss: 0.0789 - val_accuracy: 0.9980\n","Epoch 18/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9960\n","Epoch 19/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0501 - accuracy: 0.9990 - val_loss: 0.0529 - val_accuracy: 0.9980\n","Epoch 20/50\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0437 - accuracy: 0.9970 - val_loss: 0.0511 - val_accuracy: 0.9980\n","Epoch 21/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0373 - accuracy: 0.9994 - val_loss: 0.0435 - val_accuracy: 0.9980\n","Epoch 22/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0371 - accuracy: 0.9990 - val_loss: 0.0400 - val_accuracy: 0.9980\n","Epoch 23/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9996 - val_loss: 0.0394 - val_accuracy: 0.9980\n","Epoch 24/50\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9980\n","Epoch 25/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0321 - accuracy: 0.9942 - val_loss: 0.0306 - val_accuracy: 0.9980\n","Epoch 26/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9980\n","Epoch 27/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 0.9980\n","Epoch 28/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 0.9980\n","Epoch 29/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 0.9980\n","Epoch 30/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9980\n","Epoch 31/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000\n","Epoch 32/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 0.9980\n","Epoch 33/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 0.9880\n","Epoch 34/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n","Epoch 35/50\n","16/16 [==============================] - 0s 19ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9980\n","Epoch 36/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 0.9980\n","Epoch 37/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 0.9980\n","Epoch 38/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 0.9980\n","Epoch 39/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 0.9980\n","Epoch 40/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n","Epoch 41/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 0.9980\n","Epoch 42/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 0.9980\n","Epoch 43/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0246 - val_accuracy: 0.9880\n","Epoch 44/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 0.9980\n","Epoch 45/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n","Epoch 46/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n","Epoch 47/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 0.9980\n","Epoch 48/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9980\n","Epoch 49/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n","Epoch 50/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 0.9980\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f78c2b38e48>"]},"metadata":{"tags":[]},"execution_count":115}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y51RSN5d5TIp","executionInfo":{"status":"ok","timestamp":1613094020195,"user_tz":180,"elapsed":10822,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"dcd911bd-1d09-4b8f-e733-6fc410804228"}},{"cell_type":"markdown","source":["**You may have noticed that the validation loss, val_loss, was substantially higher than the training loss, loss. Furthermore, if val_loss started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to try decreasing the number of epochs.**"],"metadata":{"id":"xN-UJuO05iCO"}},{"cell_type":"markdown","source":["### Evaluating models"],"metadata":{"id":"H-d4F5-w50oM"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Two models have been trained and are available: <code>large_model</code>, which has many parameters; and <code>small_model</code>, which has fewer parameters. Both models have been trained using <code>train_features</code> and <code>train_labels</code>, which are available to you. A separate test set, which consists of <code>test_features</code> and <code>test_labels</code>, is also available.</p>\r\n","<p>Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating <code>large_model</code> and <code>small_model</code> on both the train and test sets. For each model, you can do this by applying the <code>.evaluate(x, y)</code> method to compute the loss for features <code>x</code> and labels <code>y</code>. You will then compare the four losses generated.</p></div>"],"metadata":{"id":"gbPPLW2a53OM"}},{"cell_type":"code","execution_count":null,"source":["#small_model.optimizer.get_config()\r\n","small_model = keras.Sequential()\r\n","\r\n","small_model.add(keras.layers.Dense(8, activation='relu', input_shape=(784,)))\r\n","small_model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","small_model.compile(optimizer=keras.optimizers.SGD(lr=0.01), \r\n","                    loss='categorical_crossentropy', \r\n","                    metrics=['accuracy'])\r\n","\r\n","large_model = keras.Sequential()\r\n","\r\n","large_model.add(keras.layers.Dense(64, activation='sigmoid', input_shape=(784,)))\r\n","large_model.add(keras.layers.Dense(4, activation='softmax'))\r\n","\r\n","large_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, \r\n","                                                       beta_1=0.9, beta_2=0.999),\r\n","                   loss='categorical_crossentropy', metrics=['accuracy'])"],"outputs":[],"metadata":{"id":"5Kns9oxD74ID"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\r\n","train_features, test_features, train_labels, test_labels = train_test_split(sign_language_features, \r\n","                                                                            sign_language_labels,\r\n","                                                                            test_size=0.5)\r\n","small_model.fit(train_features, train_labels, epochs=50, verbose=False)\r\n","large_model.fit(train_features, train_labels, epochs=50, verbose=False)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f78aacb1cc0>"]},"metadata":{"tags":[]},"execution_count":143}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1Fb178E7ZZh","executionInfo":{"status":"ok","timestamp":1613095427228,"user_tz":180,"elapsed":3239,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"6f891dbe-7757-470a-de61-e6787da30d76"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Evaluate the small model using the train data.</li>\r\n","<li>Evaluate the small model using the test data.</li>\r\n","<li>Evaluate the large model using the train data.</li>\r\n","<li>Evaluate the large model using the test data.</li>\r\n","</ul>"],"metadata":{"id":"q3rqh-iK55Al"}},{"cell_type":"code","execution_count":null,"source":["# Evaluate the small model using the train data\r\n","small_train = small_model.evaluate(train_features, train_labels)\r\n","\r\n","# Evaluate the small model using the test data\r\n","small_test = small_model.evaluate(test_features, test_labels)\r\n","\r\n","# Evaluate the large model using the train data\r\n","large_train = large_model.evaluate(train_features, train_labels)\r\n","\r\n","# Evaluate the large model using the test data\r\n","large_test = large_model.evaluate(test_features, test_labels)\r\n","\r\n","# Print losses\r\n","print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\r\n","print('Large - Train: {}, Test: {}'.format(large_train, large_test))"],"outputs":[{"output_type":"stream","name":"stdout","text":["16/16 [==============================] - 0s 1ms/step - loss: 0.3449 - accuracy: 0.9280\n","16/16 [==============================] - 0s 1ms/step - loss: 0.3493 - accuracy: 0.9180\n","16/16 [==============================] - 0s 1ms/step - loss: 0.0403 - accuracy: 1.0000\n","16/16 [==============================] - 0s 1ms/step - loss: 0.0407 - accuracy: 1.0000\n","\n"," Small - Train: [0.34488919377326965, 0.9279999732971191], Test: [0.34929773211479187, 0.9179999828338623]\n","Large - Train: [0.040271367877721786, 1.0], Test: [0.04072785750031471, 1.0]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RC3QCCHX6jML","executionInfo":{"status":"ok","timestamp":1613095428576,"user_tz":180,"elapsed":1341,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b326162f-d33d-4cae-bfe9-e46b922bc9d1"}},{"cell_type":"markdown","source":["\r\n","\r\n","```\r\n","Small - Train: 0.16981545090675354, Test: 0.2848725914955139\r\n","Large - Train: 0.03957206755876541, Test: 0.1454352587461471\r\n","```\r\n","\r\n","\r\n","**Notice that the gap between the test and train set losses is high for large_model, suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for large_model. This suggests that we may want to use large_model, but reduce the number of training epochs.**"],"metadata":{"id":"49zNok8S691x"}},{"cell_type":"markdown","source":["## Training models with the Estimators API"],"metadata":{"id":"BRbLV3Q2-9qd"}},{"cell_type":"markdown","source":["### Preparing to train with Estimators"],"metadata":{"id":"WvFffDdYbjm4"}},{"cell_type":"markdown","source":["<div class=\"\"><p>For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the <code>estimator</code> API. </p>\r\n","<p>Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade <code>estimator</code>. Note that <code>feature_column</code> has been imported for you from <code>tensorflow</code>. Additionally, <code>numpy</code> has been imported as <code>np</code>, and the Kings County housing dataset is available as a <code>pandas</code> <code>DataFrame</code>: <code>housing</code>.</p></div>"],"metadata":{"id":"tlSFUpb4bo9Q"}},{"cell_type":"code","execution_count":8,"source":["housing = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/2-machine-learning-scientist-with-python/14-introduction-to-tensorflow-in-python/datasets/housing.csv')"],"outputs":[],"metadata":{"id":"tLWwhJaDfadC","executionInfo":{"status":"ok","timestamp":1613137564169,"user_tz":180,"elapsed":1539,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Complete the feature column for <code>bedrooms</code> and add another numeric feature column for <code>bathrooms</code>. Use <code>bedrooms</code> and <code>bathrooms</code> as the keys.</li>\r\n","<li>Create a list of the feature columns, <code>feature_list</code>, in the order in which they were defined.</li>\r\n","<li>Set <code>labels</code> to be equal to the <code>price</code> column in <code>housing</code>.</li>\r\n","<li>Complete the <code>bedrooms</code> entry of the <code>features</code> dictionary and add another entry for <code>bathrooms</code>.</li>\r\n","</ul>"],"metadata":{"id":"XY0dDU8MbqtR"}},{"cell_type":"code","execution_count":12,"source":["# Define feature columns for bedrooms and bathrooms\r\n","bedrooms = tf.feature_column.numeric_column(\"bedrooms\")\r\n","bathrooms = tf.feature_column.numeric_column(\"bathrooms\")\r\n","\r\n","# Define the list of feature columns\r\n","feature_list = [bedrooms, bathrooms]\r\n","\r\n","def input_fn():\r\n","\t# Define the labels\r\n","\tlabels = np.array(housing['price'])\r\n","\t# Define the features\r\n","\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \r\n","                'bathrooms':np.array(housing['bathrooms'])}\r\n","\treturn features, labels"],"outputs":[],"metadata":{"id":"knEb3Qozco1D","executionInfo":{"status":"ok","timestamp":1613137609031,"user_tz":180,"elapsed":589,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["**In the next exercise, we'll use the feature columns and data input function to define and train an estimator.**"],"metadata":{"id":"z7UrrQSVctd8"}},{"cell_type":"markdown","source":["### Defining Estimators"],"metadata":{"id":"8T0zNo-Kc373"}},{"cell_type":"markdown","source":["<p>In the previous exercise, you defined a list of feature columns, <code>feature_list</code>, and a data input function, <code>input_fn()</code>. In this exercise, you will build on that work by defining an <code>estimator</code> that makes use of input data.</p>"],"metadata":{"id":"SM5rvEPGc6uY"}},{"cell_type":"markdown","source":["Instructions 1/2\r\n","<p>Use a deep neural network regressor with 2 nodes in both the first and second hidden layers and 1 training step.</p>"],"metadata":{"id":"XWhSFXmpc8y4"}},{"cell_type":"code","execution_count":13,"source":["# Define the model and set the number of steps\r\n","model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\r\n","model.train(input_fn, steps=1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Using default config.\n","WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpoy00ujbs\n","INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpoy00ujbs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n","INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpoy00ujbs/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n","INFO:tensorflow:loss = 426471400000.0, step = 0\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1...\n","INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpoy00ujbs/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1...\n","INFO:tensorflow:Loss for final step: 426471400000.0.\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow_estimator.python.estimator.canned.dnn.DNNRegressorV2 at 0x7f288d6f35c0>"]},"metadata":{"tags":[]},"execution_count":13}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQ6fx2Dwd3Ur","executionInfo":{"status":"ok","timestamp":1613137615741,"user_tz":180,"elapsed":1974,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"58960c47-6a16-4262-fc8a-f5dc71700bdf"}},{"cell_type":"markdown","source":["Instructions 2/2\r\n","<p>Modify the code to use a <code>LinearRegressor()</code>, remove the <code>hidden_units</code>, and set the number of <code>steps</code> to 2.</p>"],"metadata":{"id":"-qNwrHulc8zE"}},{"cell_type":"code","execution_count":15,"source":["# Define the model and set the number of steps\r\n","model = tf.estimator.LinearRegressor(feature_columns=feature_list)\r\n","model.train(input_fn, steps=2)"],"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Using default config.\n","WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpuflvxayi\n","INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpuflvxayi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Calling model_fn.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n","  warnings.warn('`layer.add_variable` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n","INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpuflvxayi/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n","INFO:tensorflow:loss = 426471400000.0, step = 0\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2...\n","INFO:tensorflow:Saving checkpoints for 2 into /tmp/tmpuflvxayi/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2...\n","INFO:tensorflow:Loss for final step: 426469850000.0.\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 at 0x7f288ae1cba8>"]},"metadata":{"tags":[]},"execution_count":15}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KVIkkc0eHWH","executionInfo":{"status":"ok","timestamp":1613137630671,"user_tz":180,"elapsed":1659,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"88cabe3e-aeef-4746-c737-7a21d8979f54"}},{"cell_type":"markdown","source":["**Note that you have other premade estimator options, such as BoostedTreesRegressor(), and can also create your own custom estimators**"],"metadata":{"id":"ENclCjYteKM2"}}]}