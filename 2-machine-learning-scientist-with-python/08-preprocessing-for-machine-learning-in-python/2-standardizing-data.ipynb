{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"2-standardizing-data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhS/Hutq6t7WcIlkPQ7rVn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Standardizing Data\r\n","> This chapter is all about standardizing data. Often a model will make some assumptions about the distribution or scale of your features. Standardization is a way to make your data fit these assumptions and improve the algorithm's performance.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Python, Datacamp, Machine Learning]\r\n","- image: images/batman.jpg"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 2 exercises \"Preprocessing for Machine Learning in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1611499330813,"user_tz":180,"elapsed":1579,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Standardizing Data"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### When to standardize\r\n"],"metadata":{"id":"yM1Rlo--y4ua"}},{"cell_type":"markdown","source":["Now that you've learned when it is appropriate to standardize your data, which of these scenarios would you NOT want to standardize?\r\n","\r\n"],"metadata":{"id":"6kAAEf7By7JJ"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","A column you want to use for modeling has extremely high variance.\r\n","\r\n","You have a dataset with several continuous columns on different scales and you'd like to use a linear model to train the data.\r\n","\r\n","The models you're working with use some sort of distance metric in a linear space, like the Euclidean metric.\r\n","\r\n","<b>Your dataset is comprised of categorical data.</b>\r\n","\r\n","</pre>"],"metadata":{"id":"uCFCTD_7y_Lo"}},{"cell_type":"markdown","source":["**Standardization is a preprocessing task performed on numerical, continuous data.**"],"metadata":{"id":"7r0knPBAzOei"}},{"cell_type":"markdown","source":["### Modeling without normalizing\r\n"],"metadata":{"id":"MdOUOjv2zZtN"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the <code>wine</code> dataset. One of the columns, <code>Proline</code>, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.</p>\r\n","<p>The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (<code>knn</code>) as well as the <code>X</code> and <code>y</code> sets you need to fit and score on.</p></div>"],"metadata":{"id":"bk-GVNZLzdSN"}},{"cell_type":"code","execution_count":43,"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.neighbors import KNeighborsClassifier\r\n","\r\n","wine_subset = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/wine_subset.csv')\r\n","\r\n","y = wine_subset['Type']\r\n","X = wine_subset.drop('Type', 1)\r\n","\r\n","knn = KNeighborsClassifier()"],"outputs":[],"metadata":{"id":"7ZWuIviH1-O7","executionInfo":{"status":"ok","timestamp":1611501729519,"user_tz":180,"elapsed":750,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Split up the <code>X</code> and <code>y</code> sets into training and test sets using <code>train_test_split()</code>.</li>\r\n","<li>Use the <code>knn</code> model's <code>fit()</code> method on the <code>X_train</code> data and <code>y_train</code> labels, to fit the model to the data.</li>\r\n","<li>Print out the <code>knn</code> model's <code>score()</code> on the <code>X_test</code> data and <code>y_test</code> labels to evaluate the model.</li>\r\n","</ul>"],"metadata":{"id":"Q7QXQn8izfSM"}},{"cell_type":"code","execution_count":10,"source":["# Split the dataset and labels into training and test sets\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\r\n","\r\n","# Fit the k-nearest neighbors model to the training data\r\n","knn.fit(X_train, y_train)\r\n","\r\n","# Score the model on the test data\r\n","print(knn.score(X_test, y_test))"],"outputs":[{"output_type":"stream","name":"stdout","text":["0.5555555555555556\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORflbNtGz9c6","executionInfo":{"status":"ok","timestamp":1611499435767,"user_tz":180,"elapsed":845,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b98bbfcc-f362-4d2e-aa71-96c9f05d6e4d"}},{"cell_type":"markdown","source":["**You can see that the accuracy score is pretty low. Let's explore methods to improve this score.**"],"metadata":{"id":"kAxneLgD0ARp"}},{"cell_type":"markdown","source":["##  Log normalization"],"metadata":{"id":"KWqkLno12qBq"}},{"cell_type":"markdown","source":["### Checking the variance\r\n"],"metadata":{"id":"QnxcFqrE3_9K"}},{"cell_type":"markdown","source":["<p>Check the variance of the columns in the <code>wine</code> dataset. Out of the four columns listed in the multiple choice section, which column is a candidate for normalization?</p>"],"metadata":{"id":"h4nZwfNY4Ceq"}},{"cell_type":"code","execution_count":45,"source":["wine = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/wine.csv')"],"outputs":[],"metadata":{"id":"8jsu5af34NGp","executionInfo":{"status":"ok","timestamp":1611501744506,"user_tz":180,"elapsed":779,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Alcohol\r\n","\r\n","<b>Proline</b>\r\n","\r\n","Proanthocyanins\r\n","\r\n","Ash\r\n","</pre>"],"metadata":{"id":"VhGTFHS74Epa"}},{"cell_type":"code","execution_count":12,"source":["wine.var()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["Type                                0.600679\n","Alcohol                             0.659062\n","Malic acid                          1.248015\n","Ash                                 0.075265\n","Alcalinity of ash                  11.152686\n","Magnesium                         203.989335\n","Total phenols                       0.391690\n","Flavanoids                          0.997719\n","Nonflavanoid phenols                0.015489\n","Proanthocyanins                     0.327595\n","Color intensity                     5.374449\n","Hue                                 0.052245\n","OD280/OD315 of diluted wines        0.504086\n","Proline                         99166.717355\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":12}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUNUviY94Uui","executionInfo":{"status":"ok","timestamp":1611499924717,"user_tz":180,"elapsed":749,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"3195438f-4256-47ba-ca02-9095cd6b50e7"}},{"cell_type":"markdown","source":["**The Proline column has an extremely high variance.**"],"metadata":{"id":"eP7664zi4ZQI"}},{"cell_type":"markdown","source":["### Log normalization in Python\r\n"],"metadata":{"id":"X-V_LAP94bpP"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now that we know that the <code>Proline</code> column in our wine dataset has a large amount of variance, let's log normalize it.</p>\r\n","<p><code>Numpy</code> has been imported as <code>np</code> in your workspace.</p></div>"],"metadata":{"id":"DNGEcawU4epu"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Print out the variance of the <code>Proline</code> column for reference.</li>\r\n","<li>Use the <code>np.log()</code> function on the <code>Proline</code> column to create a new, log-normalized column named <code>Proline_log</code>.</li>\r\n","<li>Print out the variance of the <code>Proline_log</code> column to see the difference.</li>\r\n","</ul>"],"metadata":{"id":"TQv0ekML4g3G"}},{"cell_type":"code","execution_count":14,"source":["# Print out the variance of the Proline column\r\n","print(wine['Proline'].var())\r\n","\r\n","# Apply the log normalization function to the Proline column\r\n","wine['Proline_log'] = np.log(wine['Proline'])\r\n","\r\n","# Check the variance of the normalized Proline column\r\n","print(wine['Proline_log'].var())"],"outputs":[{"output_type":"stream","name":"stdout","text":["99166.71735542428\n","0.17231366191842018\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jm9afRNQ5IyL","executionInfo":{"status":"ok","timestamp":1611500184911,"user_tz":180,"elapsed":956,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"a96401ff-fdad-4b30-9463-9a1dec5e95e0"}},{"cell_type":"markdown","source":["**The np.log() function is an easy way to log normalize a column.**"],"metadata":{"id":"tEva5_Bd5YpQ"}},{"cell_type":"markdown","source":["## Scaling data for feature comparison"],"metadata":{"id":"-mkNQz425bmX"}},{"cell_type":"markdown","source":["### Scaling data - investigating columns\r\n"],"metadata":{"id":"8u12qncV56xE"}},{"cell_type":"markdown","source":["<p>We want to use the <code>Ash</code>, <code>Alcalinity of ash</code>, and <code>Magnesium</code> columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. Using <code>describe()</code> to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?</p>"],"metadata":{"id":"P7-i7yqZ5-vV"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162.\r\n","\r\n","The means of Ash and Alcalinity of ash are less than 20, while the mean of Magnesium is greater than 90.\r\n","\r\n","The standard deviations of Ash and Alcalinity of ash are equal.\r\n","\r\n","<b>1 and 2 are true.</b>\r\n","\r\n","</pre>"],"metadata":{"id":"v4VABzEo6Alb"}},{"cell_type":"code","execution_count":15,"source":["wine[['Ash', 'Alcalinity of ash', 'Magnesium']].describe()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ash</th>\n","      <th>Alcalinity of ash</th>\n","      <th>Magnesium</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>178.000000</td>\n","      <td>178.000000</td>\n","      <td>178.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2.366517</td>\n","      <td>19.494944</td>\n","      <td>99.741573</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.274344</td>\n","      <td>3.339564</td>\n","      <td>14.282484</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.360000</td>\n","      <td>10.600000</td>\n","      <td>70.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>2.210000</td>\n","      <td>17.200000</td>\n","      <td>88.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>2.360000</td>\n","      <td>19.500000</td>\n","      <td>98.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2.557500</td>\n","      <td>21.500000</td>\n","      <td>107.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>3.230000</td>\n","      <td>30.000000</td>\n","      <td>162.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              Ash  Alcalinity of ash   Magnesium\n","count  178.000000         178.000000  178.000000\n","mean     2.366517          19.494944   99.741573\n","std      0.274344           3.339564   14.282484\n","min      1.360000          10.600000   70.000000\n","25%      2.210000          17.200000   88.000000\n","50%      2.360000          19.500000   98.000000\n","75%      2.557500          21.500000  107.000000\n","max      3.230000          30.000000  162.000000"]},"metadata":{"tags":[]},"execution_count":15}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"sauuB4Du7BXL","executionInfo":{"status":"ok","timestamp":1611500625516,"user_tz":180,"elapsed":769,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c51995a9-a94b-44f5-ead1-692d61cd7350"}},{"cell_type":"markdown","source":["**Both of these statements are true according to the statistics returned by describe()**"],"metadata":{"id":"1JyPc3jC63jl"}},{"cell_type":"markdown","source":["### Scaling data - standardizing columns\r\n"],"metadata":{"id":"IyIzw0Qv7HZL"}},{"cell_type":"markdown","source":["<p>Since we know that the <code>Ash</code>, <code>Alcalinity of ash</code>, and <code>Magnesium</code> columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.</p>"],"metadata":{"id":"w1x8rx8c7J3j"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import <code>StandardScaler</code> from <code>sklearn.preprocessing</code>.</li>\r\n","<li>Create the <code>StandardScaler()</code> method and store in a variable named <code>ss</code>.</li>\r\n","<li>Create a subset of the <code>wine</code> DataFrame of the <code>Ash</code>, <code>Alcalinity of ash</code>, and <code>Magnesium</code> columns, store in a variable named <code>wine_subset</code>.</li>\r\n","<li>Apply the <code>ss.fit_transform</code> method to the <code>wine_subset</code> DataFrame.</li>\r\n","</ul>"],"metadata":{"id":"e_pe0pPm7L3q"}},{"cell_type":"code","execution_count":27,"source":["# Import StandardScaler from scikit-learn\r\n","from sklearn.preprocessing import StandardScaler\r\n","\r\n","# Create the scaler\r\n","ss = StandardScaler()\r\n","\r\n","# Take a subset of the DataFrame you want to scale \r\n","wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\r\n","\r\n","# Apply the scaler to the DataFrame subset\r\n","wine_subset_scaled = ss.fit_transform(wine_subset)"],"outputs":[],"metadata":{"id":"4LVFE6EQ7hn4","executionInfo":{"status":"ok","timestamp":1611500969014,"user_tz":180,"elapsed":729,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"code","execution_count":38,"source":["wine_subset.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ash</th>\n","      <th>Alcalinity of ash</th>\n","      <th>Magnesium</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.43</td>\n","      <td>15.6</td>\n","      <td>127</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.14</td>\n","      <td>11.2</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.67</td>\n","      <td>18.6</td>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.50</td>\n","      <td>16.8</td>\n","      <td>113</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.87</td>\n","      <td>21.0</td>\n","      <td>118</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Ash  Alcalinity of ash  Magnesium\n","0  2.43               15.6        127\n","1  2.14               11.2        100\n","2  2.67               18.6        101\n","3  2.50               16.8        113\n","4  2.87               21.0        118"]},"metadata":{"tags":[]},"execution_count":38}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"p6BbFOIj8tjf","executionInfo":{"status":"ok","timestamp":1611501324676,"user_tz":180,"elapsed":985,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"917026df-6d41-4cd9-cabd-b39a60d7a18c"}},{"cell_type":"code","execution_count":41,"source":["wine_subset_scaled[:5]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.23205254, -1.16959318,  1.91390522],\n","       [-0.82799632, -2.49084714,  0.01814502],\n","       [ 1.10933436, -0.2687382 ,  0.08835836],\n","       [ 0.4879264 , -0.80925118,  0.93091845],\n","       [ 1.84040254,  0.45194578,  1.28198515]])"]},"metadata":{"tags":[]},"execution_count":41}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NK0CkSrF9s_w","executionInfo":{"status":"ok","timestamp":1611501350483,"user_tz":180,"elapsed":729,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"2a215c79-ba29-49e9-9231-68b2e25b6dfc"}},{"cell_type":"markdown","source":["**In scikit-learn, running fit_transform during preprocessing will both fit the method to the data as well as transform the data in a single step.**"],"metadata":{"id":"XQ3ZZSaC8bRx"}},{"cell_type":"markdown","source":["## Standardized data and modeling"],"metadata":{"id":"u-jFSyJ38eWY"}},{"cell_type":"markdown","source":["### KNN on non-scaled data\r\n"],"metadata":{"id":"U-ZxqtkR-VfD"}},{"cell_type":"markdown","source":["<p>Let's first take a look at the accuracy of a K-nearest neighbors model on the <code>wine</code> dataset without standardizing the data. The <code>knn</code> model as well as the <code>X</code> and <code>y</code> data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you.</p>"],"metadata":{"id":"Xv0Nyqkk-YUp"}},{"cell_type":"code","execution_count":47,"source":["wine = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/wine.csv')"],"outputs":[],"metadata":{"id":"r4Ou-EqJ_Wta","executionInfo":{"status":"ok","timestamp":1611501764199,"user_tz":180,"elapsed":899,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"code","execution_count":63,"source":["X = wine.drop('Type', 1)\r\n","y = wine['Type'] \r\n","\r\n","knn = KNeighborsClassifier()"],"outputs":[],"metadata":{"id":"MpTWA-K-_aLS","executionInfo":{"status":"ok","timestamp":1611501987353,"user_tz":180,"elapsed":738,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Split the dataset into training and test sets using <code>train_test_split()</code>.</li>\r\n","<li>Use the <code>knn</code> model's <code>fit()</code> method on the <code>X_train</code> data and <code>y_train</code> labels, to fit the model to the data.</li>\r\n","<li>Print out the <code>knn</code> model's <code>score()</code> on the <code>X_test</code> data and <code>y_test</code> labels to evaluate the model.</li>\r\n","</ul>"],"metadata":{"id":"zKAxo8a5-aeR"}},{"cell_type":"code","execution_count":66,"source":["# Split the dataset and labels into training and test sets\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\r\n","\r\n","# Fit the k-nearest neighbors model to the training data\r\n","knn.fit(X_train, y_train)\r\n","\r\n","# Score the model on the test data\r\n","print(knn.score(X_test, y_test))"],"outputs":[{"output_type":"stream","name":"stdout","text":["0.7333333333333333\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vwwq8xg-pLg","executionInfo":{"status":"ok","timestamp":1611501996640,"user_tz":180,"elapsed":670,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"75609b20-3f8a-418a-f765-652b15547621"}},{"cell_type":"markdown","source":["**This scikit-learn workflow should be very familiar to you at this point.**"],"metadata":{"id":"Bltdtjgy-sBA"}},{"cell_type":"markdown","source":["### KNN on scaled data\r\n"],"metadata":{"id":"NcSKUcV_AVbj"}},{"cell_type":"markdown","source":["<p>The accuracy score on the unscaled <code>wine</code> dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. Once again, the <code>knn</code> model as well as the <code>X</code> and <code>y</code> data and labels set have already been created for you.</p>"],"metadata":{"id":"e5sd-uCYAXsb"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create the <code>StandardScaler()</code> method, stored in a variable named <code>ss</code>.</li>\r\n","<li>Apply the <code>ss.fit_transform</code> method to the <code>X</code> dataset.</li>\r\n","<li>Use the <code>knn</code> model's <code>fit()</code> method on the <code>X_train</code> data and <code>y_train</code> labels, to fit the model to the data.</li>\r\n","<li>Print out the <code>knn</code> model's <code>score()</code> on the <code>X_test</code> data and <code>y_test</code> labels to evaluate the model.</li>\r\n","</ul>"],"metadata":{"id":"Vyif9LcSAZwi"}},{"cell_type":"code","execution_count":70,"source":["# Create the scaling method.\r\n","ss = StandardScaler()\r\n","\r\n","# Apply the scaling method to the dataset used for modeling.\r\n","X_scaled = ss.fit_transform(X)\r\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\r\n","\r\n","# Fit the k-nearest neighbors model to the training data.\r\n","knn.fit(X_train, y_train)\r\n","\r\n","# Score the model on the test data.\r\n","print(knn.score(X_test, y_test))"],"outputs":[{"output_type":"stream","name":"stdout","text":["0.9555555555555556\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W4jR-DumAXjS","executionInfo":{"status":"ok","timestamp":1611502116182,"user_tz":180,"elapsed":768,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"8a05cf96-82c8-4414-f134-a0403ab6aba5"}},{"cell_type":"markdown","source":["**The increase in accuracy is worth the extra step of scaling the dataset.**"],"metadata":{"id":"QMSHenagAy4R"}}]}