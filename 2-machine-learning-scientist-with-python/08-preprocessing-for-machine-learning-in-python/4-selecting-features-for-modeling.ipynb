{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"4-selecting-features-for-modeling.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPoBBN4RFIUh8M43NSKj6lS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Selecting features for modeling\r\n","> This chapter goes over a few different techniques for selecting the most important features from your dataset. You'll learn how to drop redundant features, work with text vectors, and reduce the number of features in your dataset using principal component analysis (PCA).\r\n","> \r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Python, Datacamp, Machine Learning]\r\n","- image: images/datacamp/1_supervised_learning_with_scikit_learn/2_regression.png"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 4 exercises \"Preprocessing for Machine Learning in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1611667966716,"user_tz":180,"elapsed":1721,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Feature selection"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### When to use feature selection\r\n","<p>Let's say you had finished standardizing your data and creating new features. Which of the following scenarios is NOT a good candidate for feature selection?</p>"],"metadata":{"id":"0hxCD104D0ae"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Several columns of running times that have been averaged into a new column.\r\n","\r\n","<b>A text field that hasn't been turned into a tf/idf vector yet.</b>\r\n","\r\n","A column of text that has already had a float extracted out of it.\r\n","\r\n","A categorical field that has been one-hot encoded.\r\n","\r\n","Your dataset contains columns related to whether something is a fruit or vegetable, the name of the fruit or vegetable, and the scientific name of the plant.\r\n","</pre>"],"metadata":{"id":"GzyW2NejD0P_"}},{"cell_type":"markdown","source":["**The text field needs to be vectorized before we can eliminate it, otherwise we might miss out on important data.**"],"metadata":{"id":"DvGZ_5-FD0Jn"}},{"cell_type":"markdown","source":["### Identifying areas for feature selection\r\n"],"metadata":{"id":"5JhUwCIcFqIQ"}},{"cell_type":"markdown","source":["<p>Take an exploratory look at the post-feature engineering <code>hiking</code> dataset. Which of the following columns is a good candidate for feature selection?</p>"],"metadata":{"id":"ILV0It0vFtpf"}},{"cell_type":"code","execution_count":null,"source":["hiking = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/hiking_29x20.csv')"],"outputs":[],"metadata":{"id":"3AxSzF8TC0ZC"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Length\r\n","\r\n","Difficulty\r\n","\r\n","Accessible\r\n","\r\n","<b>All of the above</b>\r\n","\r\n","None of the above\r\n","\r\n","</pre>"],"metadata":{"id":"TkeFRdZeFwCh"}},{"cell_type":"code","execution_count":null,"source":["hiking[['Length', 'Difficulty', 'Accessible']].head(15)"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Length</th>\n","      <th>Difficulty</th>\n","      <th>Accessible</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.8 miles</td>\n","      <td>NaN</td>\n","      <td>Y</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0 mile</td>\n","      <td>Easy</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.75 miles</td>\n","      <td>Easy</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.5 miles</td>\n","      <td>Easy</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.5 miles</td>\n","      <td>Easy</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Various</td>\n","      <td>Various</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.7 miles</td>\n","      <td>NaN</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2.4 miles</td>\n","      <td>NaN</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.0 mile</td>\n","      <td>NaN</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3.0 miles</td>\n","      <td>NaN</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>12.3 miles</td>\n","      <td>Easy/Moderate</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.85 miles</td>\n","      <td>Easy</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>4.0 miles</td>\n","      <td>Easy/Moderate</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>7.6 miles</td>\n","      <td>Easy/Moderate</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>8.0 miles</td>\n","      <td>Moderate/Difficult</td>\n","      <td>N</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Length          Difficulty Accessible\n","0    0.8 miles                 NaN          Y\n","1     1.0 mile                Easy          N\n","2   0.75 miles                Easy          N\n","3    0.5 miles                Easy          N\n","4    0.5 miles                Easy          N\n","5      Various             Various          N\n","6    1.7 miles                 NaN          N\n","7    2.4 miles                 NaN          N\n","8     1.0 mile                 NaN          N\n","9    3.0 miles                 NaN          N\n","10  12.3 miles       Easy/Moderate          N\n","11  0.85 miles                Easy          N\n","12   4.0 miles       Easy/Moderate          N\n","13   7.6 miles       Easy/Moderate          N\n","14   8.0 miles  Moderate/Difficult          N"]},"metadata":{"tags":[]},"execution_count":8}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"id":"4lQbqGrXGsVo","executionInfo":{"status":"ok","timestamp":1611587669774,"user_tz":180,"elapsed":1278,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"48cd6cdc-fc57-48a8-eab1-3b731a40b9d6"}},{"cell_type":"markdown","source":["**All three of these columns are good candidates for feature selection.**"],"metadata":{"id":"kdmaZNjGHOes"}},{"cell_type":"markdown","source":["## Removing redundant features"],"metadata":{"id":"HD4cpielHaGZ"}},{"cell_type":"markdown","source":["### Selecting relevant features\r\n"],"metadata":{"id":"rYl-UZpFIsrJ"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now let's identify the redundant columns in the <code>volunteer</code> dataset and  perform feature selection on the dataset to return a DataFrame of the relevant features.</p>\r\n","<p>For example, if you explore the <code>volunteer</code> dataset in the console, you'll see three features which are related to location: <code>locality</code>, <code>region</code>, and <code>postalcode</code>. They contain repeated information, so it would make sense to keep only one of the features. </p>\r\n","<p>There are also features that have gone through the feature engineering process: columns like <code>Education</code> and <code>Emergency Preparedness</code> are a product of encoding the categorical variable <code>category_desc</code>, so <code>category_desc</code> itself is redundant now.</p>\r\n","<p>Take a moment to examine the features of <code>volunteer</code> in the console, and try to identify the redundant features.</p></div>"],"metadata":{"id":"HIQcvMZjIwov"}},{"cell_type":"code","execution_count":null,"source":["volunteer = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/volunteer-617x16.csv')"],"outputs":[],"metadata":{"id":"dxmKKMq4I_w9"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a list of redundant column names and store it in the <code>to_drop</code> variable: <ul>\r\n","<li>Out of all the location-related features, keep only <code>postcode</code>.</li>\r\n","<li>Features that have gone through the feature engineering process are redundant as well.</li></ul></li>\r\n","<li>Drop the columns from the dataset using <code>.drop()</code>. </li>\r\n","<li>Print out the <code>.head()</code> of the DataFrame to see the selected columns.</li>\r\n","</ul>"],"metadata":{"id":"UGj6umMSIy0O"}},{"cell_type":"code","execution_count":null,"source":["volunteer.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>vol_requests</th>\n","      <th>title</th>\n","      <th>hits</th>\n","      <th>category_desc</th>\n","      <th>locality</th>\n","      <th>region</th>\n","      <th>postalcode</th>\n","      <th>created_date</th>\n","      <th>vol_requests_lognorm</th>\n","      <th>created_month</th>\n","      <th>Education</th>\n","      <th>Emergency Preparedness</th>\n","      <th>Environment</th>\n","      <th>Health</th>\n","      <th>Helping Neighbors in Need</th>\n","      <th>Strengthening Communities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>Web designer</td>\n","      <td>22</td>\n","      <td>Strengthening Communities</td>\n","      <td>5 22nd St\\nNew York, NY 10010\\n(40.74053152272...</td>\n","      <td>NY</td>\n","      <td>10010.0</td>\n","      <td>2011-01-14</td>\n","      <td>0.693147</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20</td>\n","      <td>Urban Adventures - Ice Skating at Lasker Rink</td>\n","      <td>62</td>\n","      <td>Strengthening Communities</td>\n","      <td>NaN</td>\n","      <td>NY</td>\n","      <td>10026.0</td>\n","      <td>2011-01-19</td>\n","      <td>2.995732</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500</td>\n","      <td>Fight global hunger and support women farmers ...</td>\n","      <td>14</td>\n","      <td>Strengthening Communities</td>\n","      <td>NaN</td>\n","      <td>NY</td>\n","      <td>2114.0</td>\n","      <td>2011-01-21</td>\n","      <td>6.214608</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15</td>\n","      <td>Stop 'N' Swap</td>\n","      <td>31</td>\n","      <td>Environment</td>\n","      <td>NaN</td>\n","      <td>NY</td>\n","      <td>10455.0</td>\n","      <td>2011-01-28</td>\n","      <td>2.708050</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>15</td>\n","      <td>Queens Stop 'N' Swap</td>\n","      <td>135</td>\n","      <td>Environment</td>\n","      <td>NaN</td>\n","      <td>NY</td>\n","      <td>11372.0</td>\n","      <td>2011-01-28</td>\n","      <td>2.708050</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   vol_requests  ... Strengthening Communities\n","0             2  ...                         1\n","1            20  ...                         1\n","2           500  ...                         1\n","3            15  ...                         0\n","4            15  ...                         0\n","\n","[5 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":13}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"id":"T_eo_5FKJ4XX","executionInfo":{"status":"ok","timestamp":1611588410132,"user_tz":180,"elapsed":1021,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"30269c5f-7f0d-4a02-befc-074fa1359541"}},{"cell_type":"code","execution_count":null,"source":["# Create a list of redundant column names to drop\r\n","to_drop = [\"locality\", \"region\", \"category_desc\", \"created_date\", \"vol_requests\"]\r\n","\r\n","# Drop those columns from the dataset\r\n","volunteer_subset = volunteer.drop(to_drop, 1)\r\n","\r\n","# Print out the head of the new dataset\r\n","volunteer_subset.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>hits</th>\n","      <th>postalcode</th>\n","      <th>vol_requests_lognorm</th>\n","      <th>created_month</th>\n","      <th>Education</th>\n","      <th>Emergency Preparedness</th>\n","      <th>Environment</th>\n","      <th>Health</th>\n","      <th>Helping Neighbors in Need</th>\n","      <th>Strengthening Communities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Web designer</td>\n","      <td>22</td>\n","      <td>10010.0</td>\n","      <td>0.693147</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Urban Adventures - Ice Skating at Lasker Rink</td>\n","      <td>62</td>\n","      <td>10026.0</td>\n","      <td>2.995732</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Fight global hunger and support women farmers ...</td>\n","      <td>14</td>\n","      <td>2114.0</td>\n","      <td>6.214608</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Stop 'N' Swap</td>\n","      <td>31</td>\n","      <td>10455.0</td>\n","      <td>2.708050</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Queens Stop 'N' Swap</td>\n","      <td>135</td>\n","      <td>11372.0</td>\n","      <td>2.708050</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  ...  Strengthening Communities\n","0                                       Web designer  ...                          1\n","1      Urban Adventures - Ice Skating at Lasker Rink  ...                          1\n","2  Fight global hunger and support women farmers ...  ...                          1\n","3                                      Stop 'N' Swap  ...                          0\n","4                               Queens Stop 'N' Swap  ...                          0\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":15}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"tMn9kLrRK6XO","executionInfo":{"status":"ok","timestamp":1611588699153,"user_tz":180,"elapsed":876,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"9e08d754-d2fb-479b-91cd-8a7810b888b6"}},{"cell_type":"markdown","source":["**It's often easier to collect a list of columns to drop, rather than dropping them individually.**"],"metadata":{"id":"1PmjXlnFLGC1"}},{"cell_type":"markdown","source":["### Checking for correlated features"],"metadata":{"id":"x6vCMnATLKiM"}},{"cell_type":"markdown","source":["<p>Let's take a look at the <code>wine</code> dataset again, which is made up of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.</p>"],"metadata":{"id":"buSEoGWVLNfM"}},{"cell_type":"code","execution_count":null,"source":["wine = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/wine-178x5.csv')"],"outputs":[],"metadata":{"id":"N2aZGeFRLlI5"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Print out the column correlations of the <code>wine</code> dataset using <code>corr()</code>.</li>\r\n","<li>Take a minute to look at the correlations. Identify a column where the correlation value is greater than 0.75 at least twice and store it in the <code>to_drop</code> variable.</li>\r\n","<li>Drop that column from the DataFrame using <code>drop()</code>.</li>\r\n","</ul>"],"metadata":{"id":"q1_BoJlhLPgs"}},{"cell_type":"code","execution_count":null,"source":["# Print out the column correlations of the wine dataset\r\n","wine.corr()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Flavanoids</th>\n","      <th>Total phenols</th>\n","      <th>Malic acid</th>\n","      <th>OD280/OD315 of diluted wines</th>\n","      <th>Hue</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Flavanoids</th>\n","      <td>1.000000</td>\n","      <td>0.864564</td>\n","      <td>-0.411007</td>\n","      <td>0.787194</td>\n","      <td>0.543479</td>\n","    </tr>\n","    <tr>\n","      <th>Total phenols</th>\n","      <td>0.864564</td>\n","      <td>1.000000</td>\n","      <td>-0.335167</td>\n","      <td>0.699949</td>\n","      <td>0.433681</td>\n","    </tr>\n","    <tr>\n","      <th>Malic acid</th>\n","      <td>-0.411007</td>\n","      <td>-0.335167</td>\n","      <td>1.000000</td>\n","      <td>-0.368710</td>\n","      <td>-0.561296</td>\n","    </tr>\n","    <tr>\n","      <th>OD280/OD315 of diluted wines</th>\n","      <td>0.787194</td>\n","      <td>0.699949</td>\n","      <td>-0.368710</td>\n","      <td>1.000000</td>\n","      <td>0.565468</td>\n","    </tr>\n","    <tr>\n","      <th>Hue</th>\n","      <td>0.543479</td>\n","      <td>0.433681</td>\n","      <td>-0.561296</td>\n","      <td>0.565468</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              Flavanoids  ...       Hue\n","Flavanoids                      1.000000  ...  0.543479\n","Total phenols                   0.864564  ...  0.433681\n","Malic acid                     -0.411007  ... -0.561296\n","OD280/OD315 of diluted wines    0.787194  ...  0.565468\n","Hue                             0.543479  ...  1.000000\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":20}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"I_APoXRRMRTz","executionInfo":{"status":"ok","timestamp":1611589059652,"user_tz":180,"elapsed":907,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"24053cc0-429a-4141-f5d9-ea74546afbab"}},{"cell_type":"code","execution_count":null,"source":["# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\r\n","to_drop = \"Flavanoids\"\r\n","\r\n","# Drop that column from the DataFrame\r\n","wine = wine.drop(to_drop, 1)"],"outputs":[],"metadata":{"id":"pV6dZDxvLPHs"}},{"cell_type":"markdown","source":["**Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.**"],"metadata":{"id":"-A0hjjHCMina"}},{"cell_type":"markdown","source":["## Selecting features using text vectors\r\n"],"metadata":{"id":"12XOjKpIMkHr"}},{"cell_type":"markdown","source":["## Exploring text vectors, part 1\r\n"],"metadata":{"id":"Iy7pjNcM34J-"}},{"cell_type":"markdown","source":["<p>Let's expand on the text vector exploration method we just learned about, using the <code>volunteer</code> dataset's <code>title</code> tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our <code>text_tfidf</code> vector.</p>"],"metadata":{"id":"G3qWH8_b36vF"}},{"cell_type":"code","execution_count":99,"source":["volunteer = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/volunteer-617x16.csv')\r\n","volunteer = volunteer[['category_desc', 'title']]\r\n","volunteer = volunteer.dropna(subset=['category_desc'], axis=0)"],"outputs":[],"metadata":{"id":"JoH9RSyo5I40","executionInfo":{"status":"ok","timestamp":1611670894556,"user_tz":180,"elapsed":831,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"code","execution_count":100,"source":["vocab = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/vocab.csv', index_col=0).to_dict()\r\n","vocab = vocab['0']\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","title_text = volunteer['title']\r\n","tfidf_vec = TfidfVectorizer()\r\n","text_tfidf = tfidf_vec.fit_transform(title_text)"],"outputs":[],"metadata":{"id":"vl_B-G_p_ZI6","executionInfo":{"status":"ok","timestamp":1611670896634,"user_tz":180,"elapsed":1037,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Add parameters called <code>original_vocab</code>, for the <code>tfidf_vec.vocabulary_</code>, and <code>top_n</code>.</li>\r\n","<li>Call <code>pd.Series</code> on the zipped dictionary. This will make it easier to operate on.</li>\r\n","<li>Use the <code>sort_values</code> function to sort the series and slice the index up to <code>top_n</code> words.</li>\r\n","<li>Call the function, setting <code>original_vocab=tfidf_vec.vocabulary_</code>, setting <code>vector_index=8</code> to grab the 9th row, and setting <code>top_n=3</code>, to grab the top 3 weighted words.</li>\r\n","</ul>"],"metadata":{"id":"KHrIJc6Q38gd"}},{"cell_type":"code","execution_count":101,"source":["# Add in the rest of the parameters\r\n","def return_weights(vocab, original_vocab, vector, vector_index, top_n):\r\n","    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\r\n","    \r\n","    # Let's transform that zipped dict into a series\r\n","    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\r\n","    \r\n","    # Let's sort the series to pull out the top n weighted words\r\n","    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\r\n","    return [original_vocab[i] for i in zipped_index]\r\n","\r\n","# Print out the weighted words\r\n","print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))"],"outputs":[{"output_type":"stream","name":"stdout","text":["[189, 942, 466]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epWdd81D3fzC","executionInfo":{"status":"ok","timestamp":1611670899528,"user_tz":180,"elapsed":628,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"59150871-3f39-475e-9440-8773c2ff99c9"}},{"cell_type":"markdown","source":["**This is a little complicated, but you'll see how it comes together in the next exercise.**"],"metadata":{"id":"9fhJFsAP41Cv"}},{"cell_type":"markdown","source":["### Exploring text vectors, part 2\r\n"],"metadata":{"id":"OPj6OVUKD7k3"}},{"cell_type":"markdown","source":["<p>Using the function we wrote in the previous exercise, we're going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.</p>"],"metadata":{"id":"_uDWHBqPD-He"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Call <code>return_weights</code> to return the top weighted words for that document.</li>\r\n","<li>Call <code>set</code> on the returned <code>filter_list</code> so we don't get duplicated numbers.</li>\r\n","<li>Call <code>words_to_filter</code>, passing in the following parameters: <code>vocab</code> for the <code>vocab</code> parameter, <code>tfidf_vec.vocabulary_</code> for the <code>original_vocab</code> parameter, <code>text_tfidf</code> for the <code>vector</code> parameter, and <code>3</code> to grab the <code>top_n</code> 3 weighted words from each document.</li>\r\n","<li>Finally, pass that <code>filtered_words</code> set into a list to use as a filter for the text vector.</li>\r\n","</ul>"],"metadata":{"id":"VmAoMtucD_p1"}},{"cell_type":"code","execution_count":102,"source":["def words_to_filter(vocab, original_vocab, vector, top_n):\r\n","    filter_list = []\r\n","    for i in range(0, vector.shape[0]):\r\n","    \r\n","        # Here we'll call the function from the previous exercise, and extend the list we're creating\r\n","        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\r\n","        filter_list.extend(filtered)\r\n","    # Return the list in a set, so we don't get duplicate word indices\r\n","    return set(filter_list)\r\n","\r\n","# Call the function to get the list of word indices\r\n","filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\r\n","\r\n","# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\r\n","filtered_text = text_tfidf[:, list(filtered_words)]"],"outputs":[],"metadata":{"id":"xEz0z6QwEdhK","executionInfo":{"status":"ok","timestamp":1611670902284,"user_tz":180,"elapsed":1206,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["**In the next section, you'll train a model using the filtered vector.**"],"metadata":{"id":"gJuJcjDsEnm-"}},{"cell_type":"markdown","source":["### Training Naive Bayes with feature selection\r\n"],"metadata":{"id":"DSOmLexAEqxY"}},{"cell_type":"markdown","source":["<p>Let's re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the <code>volunteer</code> dataset's <code>title</code> and <code>category_desc</code> columns.</p>"],"metadata":{"id":"UfWtW0pyEtIC"}},{"cell_type":"code","execution_count":104,"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.naive_bayes import GaussianNB\r\n","\r\n","nb = GaussianNB()\r\n","y = volunteer['category_desc']"],"outputs":[],"metadata":{"id":"pI6y6l_yFJgF","executionInfo":{"status":"ok","timestamp":1611671052255,"user_tz":180,"elapsed":609,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Use <code>train_test_split</code> on the <code>filtered_text</code> text vector, the <code>y</code> labels (which is the <code>category_desc</code> labels), and pass the <code>y</code> set to the <code>stratify</code> parameter, since we have an uneven class distribution.</li>\r\n","<li>Fit the <code>nb</code> Naive Bayes model to <code>train_X</code> and <code>train_y</code>.</li>\r\n","<li>Score the <code>nb</code> model on the <code>test_X</code> and <code>test_y</code> test sets.</li>\r\n","</ul>"],"metadata":{"id":"PBGJLtsFEu0_"}},{"cell_type":"code","execution_count":106,"source":["# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\r\n","train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\r\n","\r\n","# Fit the model to the training data\r\n","nb.fit(train_X, train_y)\r\n","\r\n","# Print out the model's accuracy\r\n","print(nb.score(test_X, test_y))"],"outputs":[{"output_type":"stream","name":"stdout","text":["0.567741935483871\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51spPzVoFEtN","executionInfo":{"status":"ok","timestamp":1611671062911,"user_tz":180,"elapsed":741,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"0e838672-2c15-4e81-9612-778f9d2746e8"}},{"cell_type":"markdown","source":["**You can see that our accuracy score wasn't that different from the score at the end of chapter 3. That's okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works.**"],"metadata":{"id":"Oi5APcG8FVUj"}},{"cell_type":"markdown","source":["## Dimensionality reduction\r\n"],"metadata":{"id":"vdsdhWypFZf6"}},{"cell_type":"markdown","source":["### Using PCA\r\n"],"metadata":{"id":"I1vczn5wGun4"}},{"cell_type":"markdown","source":["<p>Let's apply PCA to the <code>wine</code> dataset, to see if we can get an increase in our model's accuracy.</p>"],"metadata":{"id":"0zyRHymQGtUZ"}},{"cell_type":"code","execution_count":107,"source":["wine = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/wine.csv')"],"outputs":[],"metadata":{"id":"UIR6cOmRHIlt","executionInfo":{"status":"ok","timestamp":1611671607977,"user_tz":180,"elapsed":1141,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"code","execution_count":109,"source":["wine_X = wine.drop('Type', 1)"],"outputs":[],"metadata":{"id":"MomzUbdSHkkq","executionInfo":{"status":"ok","timestamp":1611671707605,"user_tz":180,"elapsed":603,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Set up the <code>PCA</code> object. You'll use PCA on the wine dataset minus its label for <code>Type</code>, stored in the variable <code>wine_X</code>.</li>\r\n","<li>Apply PCA to <code>wine_X</code> using <code>pca</code>'s <code>fit_transform</code> method and store the transformed vector in <code>transformed_X</code>.</li>\r\n","<li>Print out the <code>explained_variance_ratio_</code> attribute of <code>pca</code> to check how much variance is explained by each component.</li>\r\n","</ul>"],"metadata":{"id":"10SR3twDGyB3"}},{"cell_type":"code","execution_count":111,"source":["from sklearn.decomposition import PCA\r\n","\r\n","# Set up PCA and the X vector for diminsionality reduction\r\n","pca = PCA()\r\n","wine_X = wine.drop(\"Type\", axis=1)\r\n","\r\n","# Apply PCA to the wine dataset X vector\r\n","transformed_X = pca.fit_transform(wine_X)\r\n","\r\n","# Look at the percentage of variance explained by the different components\r\n","print(pca.explained_variance_ratio_)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n"," 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n"," 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n"," 8.25392788e-08]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3SgMRpTGoeZ","executionInfo":{"status":"ok","timestamp":1611671740680,"user_tz":180,"elapsed":711,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"f2944a15-a41e-469d-b4e7-0b05bc82ec3d"}},{"cell_type":"markdown","source":["**In the next section you'll train a model using the PCA-transformed vector.**"],"metadata":{"id":"dr_R7QfkGqwA"}},{"cell_type":"markdown","source":["### Training a model with PCA\r\n"],"metadata":{"id":"wgQmi9Q0H_Es"}},{"cell_type":"markdown","source":["<p>Now that we have run PCA on the <code>wine</code> dataset, let's try training a model with it.</p>"],"metadata":{"id":"lKHID5gRIBQe"}},{"cell_type":"code","execution_count":113,"source":["from sklearn.neighbors import KNeighborsClassifier\r\n","y = wine['Type']\r\n","knn = KNeighborsClassifier()"],"outputs":[],"metadata":{"id":"xdwwjqdwIioa","executionInfo":{"status":"ok","timestamp":1611671947702,"user_tz":180,"elapsed":684,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Split the <code>transformed_X</code> vector and the <code>y</code> labels set into training and test sets using <code>train_test_split</code>.</li>\r\n","<li>Fit the <code>knn</code> model using the <code>fit()</code> function on the <code>X_wine_train</code> and <code>y_wine_train</code> sets.</li>\r\n","<li>Print out the score using <code>knn</code>'s <code>score()</code> function on <code>X_wine_test</code> and <code>y_wine_test</code>.</li>\r\n","</ul>"],"metadata":{"id":"RG5gdMTrIDIe"}},{"cell_type":"code","execution_count":128,"source":["# Split the transformed X and the y labels into training and test sets\r\n","X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\r\n","\r\n","# Fit knn to the training data\r\n","knn.fit(X_wine_train, y_wine_train)\r\n","\r\n","# Score knn on the test data and print it out\r\n","knn.score(X_wine_test, y_wine_test)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7555555555555555"]},"metadata":{"tags":[]},"execution_count":128}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvPJ1eRsIb4j","executionInfo":{"status":"ok","timestamp":1611671976442,"user_tz":180,"elapsed":756,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d3854e4b-0d3d-488f-8d65-7f31bded5605"}},{"cell_type":"markdown","source":["**PCA is a decent choice for the wine dataset.**"],"metadata":{"id":"ORUxnerQIu0I"}}]}