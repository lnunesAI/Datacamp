{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"5-putting-it-all-together.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMMpP9ku4tEc+Lp0CyvhCJh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Putting it all together\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Python, Datacamp, Machine Learning]\r\n","- image: images/datacamp/1_supervised_learning_with_scikit_learn/2_regression.png"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 5 exercises \"Preprocessing for Machine Learning in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1611678647496,"user_tz":180,"elapsed":3010,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## UFOs and preprocessing"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### Checking column types"],"metadata":{"id":"6zsQLMbTMIwO"}},{"cell_type":"markdown","source":["<p>Take a look at the UFO dataset's column types using the <code>dtypes</code> attribute. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as <code>object</code>, and the <code>date</code> column, which can be transformed into the <code>datetime</code> type. That will make our feature engineering efforts easier later on.</p>"],"metadata":{"id":"hBN5igR4MMYT"}},{"cell_type":"code","execution_count":39,"source":["ufo = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/ufo.csv')"],"outputs":[],"metadata":{"id":"3AxSzF8TC0ZC","executionInfo":{"status":"ok","timestamp":1611676335369,"user_tz":180,"elapsed":780,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Print out the <code>dtypes</code> of the <code>ufo</code> dataset.</li>\r\n","<li>Change the type of the <code>seconds</code> column by passing the <code>float</code> type into the <code>astype()</code> method.</li>\r\n","<li>Change the type of the <code>date</code> column by passing <code>ufo[\"date\"]</code> into the <code>pd.to_datetime()</code> function.</li>\r\n","<li>Print out the <code>dtypes</code> of the <code>seconds</code> and <code>date</code> columns, to make sure it worked.</li>\r\n","</ul>"],"metadata":{"id":"0R27E4B-MOAM"}},{"cell_type":"code","execution_count":5,"source":["# Check the column types\r\n","print(ufo.dtypes)\r\n","\r\n","# Change the type of seconds to float\r\n","ufo['seconds'] = ufo['seconds'].astype(float)\r\n","\r\n","# Change the date column to type datetime\r\n","ufo['date'] = pd.to_datetime(ufo['date'])\r\n","\r\n","# Check the column types\r\n","print(ufo[['seconds', 'date']].dtypes)"],"outputs":[{"output_type":"stream","name":"stdout","text":["date               object\n","city               object\n","state              object\n","country            object\n","type               object\n","seconds           float64\n","length_of_time     object\n","desc               object\n","recorded           object\n","lat                object\n","long              float64\n","dtype: object\n","seconds           float64\n","date       datetime64[ns]\n","dtype: object\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6qsrEpoPVDM","executionInfo":{"status":"ok","timestamp":1611673721955,"user_tz":180,"elapsed":1074,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"86bfaebd-fe19-4fae-e845-b2c7e21ed0c9"}},{"cell_type":"markdown","source":["**Nice job on transforming the column types! This will make feature engineering and standardization easier.**"],"metadata":{"id":"IJ1JGGxbPe2i"}},{"cell_type":"markdown","source":["### Dropping missing data\r\n"],"metadata":{"id":"r_gaBhyjPnf5"}},{"cell_type":"markdown","source":["<p>Let's remove some of the rows where certain columns have missing values. We're going to look at the <code>length_of_time</code> column, the <code>state</code> column, and the <code>type</code> column. If any of the values in these columns are missing, we're going to drop the rows.</p>"],"metadata":{"id":"Lz_3MWLbPrb5"}},{"cell_type":"markdown","source":["<ul>\r\n","<li>Check how many values are missing in the <code>length_of_time</code>, <code>state</code>, and <code>type</code> columns, using <code>isnull()</code> to check for nulls and <code>sum()</code> to calculate how many exist.</li>\r\n","<li>Use boolean indexing to filter out the rows with those missing values, using <code>notnull()</code> to check the column. Here, we can chain together each column we want to check.</li>\r\n","<li>Print out the <code>shape</code> of the new <code>ufo_no_missing</code> dataset.</li>\r\n","</ul>"],"metadata":{"id":"Ih3one3lPuNI"}},{"cell_type":"code","execution_count":6,"source":["# Check how many values are missing in the length_of_time, state, and type columns\r\n","print(ufo[['length_of_time', 'state', 'type']].isnull().sum())\r\n","\r\n","# Keep only rows where length_of_time, state, and type are not null\r\n","ufo_no_missing = ufo[ufo['length_of_time'].notnull() & \r\n","          ufo['state'].notnull() & \r\n","          ufo['type'].notnull()\r\n","          ]\r\n","\r\n","# Print out the shape of the new dataset\r\n","print(ufo_no_missing.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["length_of_time    143\n","state             419\n","type              159\n","dtype: int64\n","(4283, 11)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjiF-lxjQqn5","executionInfo":{"status":"ok","timestamp":1611674091017,"user_tz":180,"elapsed":765,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"403ee040-ceec-4d9c-d8cc-763a46e9eca7"}},{"cell_type":"markdown","source":["**We'll work with this set going forward.**"],"metadata":{"id":"_bMlnblHQ5F_"}},{"cell_type":"markdown","source":["## Categorical variables and standardization\r\n"],"metadata":{"id":"fC7HfbLbRI-k"}},{"cell_type":"markdown","source":["### Extracting numbers from strings\r\n"],"metadata":{"id":"xYx36DjGRbkj"}},{"cell_type":"markdown","source":["<p>The <code>length_of_time</code> field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions.</p>"],"metadata":{"id":"Iz5xHiCMReZ7"}},{"cell_type":"code","execution_count":3,"source":["import re\r\n","ufo = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/8-preprocessing-for-machine-learning-in-python/datasets/ufo_1866x11.csv')"],"outputs":[],"metadata":{"id":"qnFFAP1cSS_D","executionInfo":{"status":"ok","timestamp":1611678656322,"user_tz":180,"elapsed":3029,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Pass <code>\\d+</code> into <code>re.compile()</code> in the <code>pattern</code> variable to designate that we want to grab as many digits as possible from the string.</li>\r\n","<li>Into <code>re.match()</code>, pass the <code>pattern</code> we just created, as well as the <code>time_string</code> we want to extract from.</li>\r\n","<li>Use <code>lambda</code> within the <code>apply()</code> method to perform the extraction.</li>\r\n","<li>Print out the <code>head()</code> of both the <code>length_of_time</code> and <code>minutes</code> columns to compare.</li>\r\n","</ul>"],"metadata":{"id":"n0lbO0t4RgFS"}},{"cell_type":"code","execution_count":4,"source":["def return_minutes(time_string):\r\n","\r\n","    # Use \\d+ to grab digits\r\n","    pattern = re.compile(r\"\\d+\")\r\n","\r\n","    # Use match on the pattern and column\r\n","    num = re.match(pattern, time_string)\r\n","    if num is not None:\r\n","        return int(num.group(0))\r\n","        \r\n","# Apply the extraction to the length_of_time column\r\n","ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(lambda row: return_minutes(row))\r\n","\r\n","# Take a look at the head of both of the columns\r\n","print(ufo[['length_of_time', 'minutes']].head(5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["    length_of_time  minutes\n","0  about 5 minutes      NaN\n","1       10 minutes     10.0\n","2        2 minutes      2.0\n","3        2 minutes      2.0\n","4        5 minutes      5.0\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mU8BjQnySPoj","executionInfo":{"status":"ok","timestamp":1611678664397,"user_tz":180,"elapsed":1173,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"77bd57c4-ebf1-48c5-9624-048611614e87"}},{"cell_type":"markdown","source":["**As you can see, we end up with some NaNs in the DataFrame. That's okay for now; we'll take care of those before modeling.**"],"metadata":{"id":"pILRfoDISXNi"}},{"cell_type":"markdown","source":["### Identifying features for standardization\r\n"],"metadata":{"id":"foTSiXxqUUmk"}},{"cell_type":"markdown","source":["<p>In this section, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the <code>seconds</code> and <code>minutes</code> column, you'll see that the variance of the <code>seconds</code> column is extremely high. Because <code>seconds</code> and <code>minutes</code> are related to each other (an issue we'll deal with when we select features for modeling), let's log normlize the <code>seconds</code> column.</p>"],"metadata":{"id":"-xP6fntKUYFL"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Use the <code>var()</code> method on the <code>seconds</code> and <code>minutes</code> columns to check the variance. Notice how high the variance is on the <code>seconds</code> column.</li>\r\n","<li>Using <code>np.log()</code> perform log normalization on the <code>seconds</code> column, transforming it into a new column named <code>seconds_log</code>.</li>\r\n","<li>Print out the variance of the <code>seconds_log</code> column.</li>\r\n","</ul>"],"metadata":{"id":"ybO8PVNMUZxr"}},{"cell_type":"code","execution_count":5,"source":["# Check the variance of the seconds and minutes columns\r\n","print(ufo[['seconds', 'minutes']].var())\r\n","\r\n","# Log normalize the seconds column\r\n","ufo[\"seconds_log\"] = np.log(ufo['seconds'])\r\n","\r\n","# Print out the variance of just the seconds_log column\r\n","print(ufo[\"seconds_log\"].var())"],"outputs":[{"output_type":"stream","name":"stdout","text":["seconds    424087.417474\n","minutes       117.546372\n","dtype: float64\n","1.122392388118297\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4zKvCs4VkcK","executionInfo":{"status":"ok","timestamp":1611678667706,"user_tz":180,"elapsed":1174,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"beca6c19-3a6f-4aa2-ea54-27f3c4bac9cf"}},{"cell_type":"markdown","source":["**In the next section, we'll focus on engineering new features.**"],"metadata":{"id":"e7OIBPHzVnsx"}},{"cell_type":"markdown","source":["### Engineering new features"],"metadata":{"id":"RiOdf0paV2UA"}},{"cell_type":"markdown","source":["### Encoding categorical variables\r\n"],"metadata":{"id":"cnMADGfSXiry"}},{"cell_type":"markdown","source":["<p>There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods.</p>"],"metadata":{"id":"x7B4Ck2MXlL5"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Using <code>apply()</code>, write a <code>lambda</code> that returns a 1 if the value is <code>us</code>, else return 0. This is something we learned in Chapter 3 if you need a refresher.</li>\r\n","<li>Next, print out the number of <code>unique()</code> values of the <code>type</code> column.</li>\r\n","<li>Using <code>pd.get_dummies()</code>, create a one-hot encoded set of the <code>type</code> column.</li>\r\n","<li>Finally, use <code>pd.concat()</code> to concatenate the <code>ufo</code> dataset to the <code>type_set</code> encoded variables.</li>\r\n","</ul>"],"metadata":{"id":"Y_lu-s1KXnRB"}},{"cell_type":"code","execution_count":6,"source":["# Use Pandas to encode us values as 1 and others as 0\r\n","ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda value: 1 if value == 'us' else 0)\r\n","\r\n","# Print the number of unique type values\r\n","print(len(ufo[\"type\"].unique()))\r\n","\r\n","# Create a one-hot encoded set of the type values\r\n","type_set = pd.get_dummies(ufo['type'])\r\n","\r\n","# Concatenate this set back to the ufo DataFrame\r\n","ufo = pd.concat([ufo, type_set], axis=1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["21\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voDqKKRZXdVr","executionInfo":{"status":"ok","timestamp":1611678670774,"user_tz":180,"elapsed":1184,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d1a0f4c6-c50c-4715-b72f-73d36c9d9aed"}},{"cell_type":"markdown","source":["**Let's continue on by extracting some date parts.**"],"metadata":{"id":"0KMdzJRlXf1q"}},{"cell_type":"markdown","source":["### Features from dates\r\n"],"metadata":{"id":"b7ATBli3XvAY"}},{"cell_type":"markdown","source":["<p>Another feature engineering task to perform is month and year extraction. Perform this task on the <code>date</code> column of the <code>ufo</code> dataset.</p>"],"metadata":{"id":"pms9VZL2XyEw"}},{"cell_type":"code","execution_count":7,"source":["ufo['date'] = pd.to_datetime(ufo['date'])"],"outputs":[],"metadata":{"id":"AuhA3zY8afaj","executionInfo":{"status":"ok","timestamp":1611678676288,"user_tz":180,"elapsed":1325,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Print out the <code>head()</code> of the <code>date</code> column.</li>\r\n","<li>Using <code>apply()</code>, <code>lambda</code>, and the <code>.month</code> attribute, extract the month from the <code>date</code> column.</li>\r\n","<li>Using <code>apply()</code>, <code>lambda</code>, and the <code>.year</code> attribute, extract the year from the <code>date</code> column.</li>\r\n","<li>Take a look at the <code>head()</code> of the <code>date</code>, <code>month</code>, and <code>year</code> columns.</li>\r\n","</ul>"],"metadata":{"id":"_r34pnUnXzk3"}},{"cell_type":"code","execution_count":8,"source":["# Look at the first 5 rows of the date column\r\n","print(ufo['date'].head())\r\n","\r\n","# Extract the month from the date column\r\n","ufo[\"month\"] = ufo[\"date\"].apply(lambda date: date.month)\r\n","\r\n","# Extract the year from the date column\r\n","ufo[\"year\"] = ufo[\"date\"].apply(lambda date: date.year)\r\n","\r\n","# Take a look at the head of all three columns\r\n","print(ufo[[\"date\", \"month\", \"year\"]].head())"],"outputs":[{"output_type":"stream","name":"stdout","text":["0   2002-11-21 05:45:00\n","1   2012-06-16 23:00:00\n","2   2013-06-09 00:00:00\n","3   2013-04-26 23:27:00\n","4   2013-09-13 20:30:00\n","Name: date, dtype: datetime64[ns]\n","                 date  month  year\n","0 2002-11-21 05:45:00     11  2002\n","1 2012-06-16 23:00:00      6  2012\n","2 2013-06-09 00:00:00      6  2013\n","3 2013-04-26 23:27:00      4  2013\n","4 2013-09-13 20:30:00      9  2013\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exZYvpUCYkyS","executionInfo":{"status":"ok","timestamp":1611678678097,"user_tz":180,"elapsed":1140,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e83c6756-9245-4d9a-a00b-f35f2adc01b1"}},{"cell_type":"markdown","source":["**Nice job on extracting dates! 'apply' and 'lambda' are extremely useful for extraction tasks.**"],"metadata":{"id":"Tezr9uyxZDku"}},{"cell_type":"markdown","source":["### Text vectorization\r\n"],"metadata":{"id":"V0wpqu7ziUiB"}},{"cell_type":"markdown","source":["<p>Let's transform the <code>desc</code> column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field.</p>"],"metadata":{"id":"SSPCIVISiW3A"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Print out the <code>head()</code> of the <code>ufo[\"desc\"]</code> column.</li>\r\n","<li>Set <code>vec</code> equal to the <code>TfidfVectorizer()</code> object.</li>\r\n","<li>Use <code>vec</code>'s <code>fit_transform()</code> method on the <code>ufo[\"desc\"]</code> column.</li>\r\n","<li>Print out the <code>shape</code> of the <code>desc_tfidf</code> vector, to take a look at the number of columns this created. The output is in the shape (rows, columns).</li>\r\n","</ul>"],"metadata":{"id":"S7TfTIjMiYeQ"}},{"cell_type":"code","execution_count":10,"source":["from sklearn.feature_extraction.text import TfidfVectorizer\r\n","\r\n","# Take a look at the head of the desc field\r\n","print(ufo[\"desc\"].head())\r\n","\r\n","# Create the tfidf vectorizer object\r\n","vec = TfidfVectorizer()\r\n","\r\n","# Use vec's fit_transform method on the desc field\r\n","desc_tfidf = vec.fit_transform(ufo[\"desc\"])\r\n","\r\n","# Look at the number of columns this creates\r\n","print(desc_tfidf.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["0    It was a large&#44 triangular shaped flying ob...\n","1    Dancing lights that would fly around and then ...\n","2    Brilliant orange light or chinese lantern at o...\n","3    Bright red light moving north to north west fr...\n","4    North-east moving south-west. First 7 or so li...\n","Name: desc, dtype: object\n","(1866, 3422)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_OdxVt2i2aN","executionInfo":{"status":"ok","timestamp":1611678853150,"user_tz":180,"elapsed":875,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"4478af5d-3540-458f-b060-6f9d8dffb043"}},{"cell_type":"markdown","source":["**You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section.**"],"metadata":{"id":"puFO8DpBjC-T"}},{"cell_type":"markdown","source":["## Feature selection and modeling\r\n"],"metadata":{"id":"U59UZPl6jFUG"}},{"cell_type":"markdown","source":["### Selecting the ideal dataset\r\n"],"metadata":{"id":"joOiOjcnjdLg"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Let's get rid of some of the unnecessary features. Because we have an encoded country column, <code>country_enc</code>, keep it and drop other columns related to location: <code>city</code>, <code>country</code>, <code>lat</code>, <code>long</code>, <code>state</code>. </p>\r\n","<p>We have columns related to <code>month</code> and <code>year</code>, so we don't need the <code>date</code> or <code>recorded</code> columns. </p>\r\n","<p>We vectorized <code>desc</code>, so we don't need it anymore. For now we'll keep <code>type</code>. </p>\r\n","<p>We'll keep <code>seconds_log</code> and drop <code>seconds</code> and <code>minutes</code>. </p>\r\n","<p>Let's also get rid of the <code>length_of_time</code> column, which is unnecessary after extracting <code>minutes</code>.</p></div>"],"metadata":{"id":"3YQ_lfmMjf2o"}},{"cell_type":"code","execution_count":17,"source":["def return_weights(vocab, original_vocab, vector, vector_index, top_n):\r\n","    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\r\n","    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\r\n","    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\r\n","    return [original_vocab[i] for i in zipped_index]\r\n","\r\n","def words_to_filter(vocab, original_vocab, vector, top_n):\r\n","    filter_list = []\r\n","    \r\n","    for i in range(0, vector.shape[0]):\r\n","        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\r\n","        filter_list.extend(filtered)\r\n","        \r\n","    return set(filter_list)\r\n","    \r\n","vocab = {v:k for k,v in vec.vocabulary_.items()}"],"outputs":[],"metadata":{"id":"kBoW3bYalszv","executionInfo":{"status":"ok","timestamp":1611679621948,"user_tz":180,"elapsed":610,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Use <code>.corr()</code> to run the correlation on <code>seconds</code>, <code>seconds_log</code>, and <code>minutes</code> in the <code>ufo</code> DataFrame.</li>\r\n","<li>Make a list of columns to drop, in alphabetical order.</li>\r\n","<li>Use <code>drop()</code> to drop the columns.</li>\r\n","<li>Use the <code>words_to_filter()</code> function we created previously. Pass in <code>vocab</code>, <code>vec.vocabulary_</code>, <code>desc_tfidf</code>, and let's keep the top <code>4</code> words as the last parameter.</li>\r\n","</ul>"],"metadata":{"id":"ULyLfipujhnn"}},{"cell_type":"code","execution_count":26,"source":["# Check the correlation between the seconds, seconds_log, and minutes columns\r\n","print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\r\n","\r\n","# Make a list of features to drop\r\n","to_drop = ['city', 'country', 'lat', 'long', 'state',\r\n","'date', 'recorded', 'desc', 'seconds', 'minutes', 'length_of_time']\r\n","\r\n","# Drop those features\r\n","ufo_dropped = ufo.drop(to_drop, 1)\r\n","\r\n","# Let's also filter some words out of the text vector we created\r\n","filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"],"outputs":[{"output_type":"stream","name":"stdout","text":["              seconds  seconds_log   minutes\n","seconds      1.000000     0.853371  0.980341\n","seconds_log  0.853371     1.000000  0.824493\n","minutes      0.980341     0.824493  1.000000\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vig9sOhJkw4u","executionInfo":{"status":"ok","timestamp":1611680076414,"user_tz":180,"elapsed":2666,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"690fa4a0-adf9-4e34-9778-9b52d28ea227"}},{"cell_type":"markdown","source":["**You're almost done. In the next exercises, we'll try modeling the UFO data in a couple of different ways.**"],"metadata":{"id":"jpbaXZJ7nvqW"}},{"cell_type":"markdown","source":["### Modeling the UFO dataset, part 1"],"metadata":{"id":"e1JlkZZhnyE-"}},{"cell_type":"markdown","source":["<p>In this exercise, we're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our <code>X</code> dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The <code>y</code> labels are the encoded country column, where 1 is <code>us</code> and 0 is <code>ca</code>.</p>"],"metadata":{"id":"ch8qDg-Pn2W1"}},{"cell_type":"code","execution_count":33,"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.neighbors import KNeighborsClassifier\r\n","\r\n","X = ufo_dropped.drop(['type', 'country_enc'], 1)\r\n","y = ufo_dropped['country_enc']\r\n","\r\n","knn = KNeighborsClassifier()"],"outputs":[],"metadata":{"id":"kIsIOqz2pCIE","executionInfo":{"status":"ok","timestamp":1611680516795,"user_tz":180,"elapsed":628,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Print out the <code>.columns</code> of the <code>X</code> set.</li>\r\n","<li>Split up the <code>X</code> and <code>y</code> sets using <code>train_test_split()</code>. Pass the <code>y</code> set to the <code>stratify=</code> parameter, since we have imbalanced classes here.</li>\r\n","<li>Use <code>fit()</code> to fit <code>train_X</code> and <code>train_y</code>.</li>\r\n","<li>Print out the <code>.score()</code> of the <code>knn</code> model on the <code>test_X</code> and <code>test_y</code> sets.</li>\r\n","</ul>"],"metadata":{"id":"H_qWmGiMoKDj"}},{"cell_type":"code","execution_count":37,"source":["# Take a look at the features in the X set of data\r\n","print(X.columns)\r\n","\r\n","# Split the X and y sets using train_test_split, setting stratify=y\r\n","train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\r\n","\r\n","# Fit knn to the training sets\r\n","knn.fit(train_X, train_y)\r\n","\r\n","# Print the score of knn on the test sets\r\n","print(knn.score(test_X, test_y))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n","       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n","       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n","       'teardrop', 'triangle', 'unknown', 'month', 'year'],\n","      dtype='object')\n","0.8779443254817987\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C43QE52Foc_J","executionInfo":{"status":"ok","timestamp":1611680537639,"user_tz":180,"elapsed":655,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"3a45ef50-126b-4cf0-bec7-dff67372e360"}},{"cell_type":"markdown","source":["**This model performs pretty well. It seems like we've made pretty good feature selection choices here.**"],"metadata":{"id":"lTIZ24fxoflA"}},{"cell_type":"markdown","source":["### Modeling the UFO dataset, part 2\r\n"],"metadata":{"id":"onftdSzxpdO_"}},{"cell_type":"markdown","source":["<p>Finally, let's build a model using the text vector we created, <code>desc_tfidf</code>, using the <code>filtered_words</code> list to create a filtered text vector. Let's see if we can predict the <code>type</code> of the sighting based on the text. We'll use a Naive Bayes model for this.</p>"],"metadata":{"id":"JaL1yTJRpfOe"}},{"cell_type":"code","execution_count":42,"source":["from sklearn.naive_bayes import GaussianNB\r\n","y = ufo_dropped['type']\r\n","nb = GaussianNB()"],"outputs":[],"metadata":{"id":"-GF3O8YrqUra","executionInfo":{"status":"ok","timestamp":1611680821920,"user_tz":180,"elapsed":582,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>On the <code>desc_tfidf</code> vector, filter by passing a list of <code>filtered_words</code> into the index.</li>\r\n","<li>Split up the <code>X</code> and <code>y</code> sets using <code>train_test_split()</code>. Remember to convert <code>filtered_text</code> using <code>toarray()</code>. Pass the <code>y</code> set to the <code>stratify=</code> parameter, since we have imbalanced classes here.</li>\r\n","<li>Use the <code>nb</code> model's <code>fit()</code> to fit <code>train_X</code> and <code>train_y</code>.</li>\r\n","<li>Print out the <code>.score()</code> of the <code>nb</code> model on the <code>test_X</code> and <code>test_y</code> sets.</li>\r\n","</ul>"],"metadata":{"id":"A9JCg_Mjpg9g"}},{"cell_type":"code","execution_count":43,"source":["# Use the list of filtered words we created to filter the text vector\r\n","filtered_text = desc_tfidf[:, list(filtered_words)]\r\n","\r\n","# Split the X and y sets using train_test_split, setting stratify=y \r\n","train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\r\n","\r\n","# Fit nb to the training sets\r\n","nb.fit(train_X, train_y)\r\n","\r\n","# Print the score of nb on the test sets\r\n","nb.score(test_X, test_y)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.15845824411134904"]},"metadata":{"tags":[]},"execution_count":43}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjUrkIyvqOUa","executionInfo":{"status":"ok","timestamp":1611680824506,"user_tz":180,"elapsed":956,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ce9db533-df86-4ea7-b0c7-9eed0db3e3b4"}},{"cell_type":"markdown","source":["**As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type.**"],"metadata":{"id":"4ttE8Ko0qaGh"}}]}