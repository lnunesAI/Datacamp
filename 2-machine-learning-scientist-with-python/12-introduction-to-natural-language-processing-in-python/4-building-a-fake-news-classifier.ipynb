{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"4-building-a-fake-news-classifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPiJF9HGmqFovEcE4Tq6TGs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Building a \"fake news\" classifier\r\n",">  You'll apply the basics of what you've learned along with some supervised machine learning to build a \"fake news\" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify fake news articles.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Python, Datacamp, Machine Learning]\r\n","- image: images/datacamp/1_supervised_learning_with_scikit_learn/2_regression.png"],"metadata":{"id":"f5FTJNcbC9gb"}},{"cell_type":"markdown","source":["> Note: This is a summary of the course's chapter 4 exercises \"Introduction to Natural Language Processing in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import pandas as pd\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","plt.rcParams['figure.figsize'] = (8, 8)"],"outputs":[],"metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1612721755881,"user_tz":180,"elapsed":1623,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["## Classifying fake news using supervised learning with NLP"],"metadata":{"id":"UvrKEMkeEF2g"}},{"cell_type":"markdown","source":["### Which possible features?"],"metadata":{"id":"IfcyGuehlp8Z"}},{"cell_type":"markdown","source":["<p>Which of the following are possible features for a text classification problem?</p>"],"metadata":{"id":"d83Pzoeolt3h"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Number of words in a document.\r\n","\r\n","Specific named entities.\r\n","\r\n","Language.\r\n","\r\n","<b>All of the above.</b>\r\n","\r\n","</pre>"],"metadata":{"id":"U-kWg04nlyGj"}},{"cell_type":"markdown","source":["### Training and testing"],"metadata":{"id":"0az2u3b9mJ7a"}},{"cell_type":"markdown","source":["<p>What datasets are needed for supervised learning?</p>"],"metadata":{"id":"FiCwP-p2mNBa"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Training data.\r\n","\r\n","Testing data.\r\n","\r\n","<b>Both training and testing data.</b>\r\n","\r\n","A label or outcome.\r\n","\r\n","</pre>"],"metadata":{"id":"z0CFzUarmQBj"}},{"cell_type":"markdown","source":["### Building word count vectors with scikit-learn"],"metadata":{"id":"FU-gtFFDmhEZ"}},{"cell_type":"markdown","source":["### CountVectorizer for text classification"],"metadata":{"id":"Xf_X0uE_pj5T"}},{"cell_type":"markdown","source":["<div class=\"\"><p>It's time to begin building your text classifier! The <a href=\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3629/fake_or_real_news.csv\" target=\"_blank\" rel=\"noopener noreferrer\">data</a> has been loaded into a DataFrame called <code>df</code>. Explore it in the IPython Shell to investigate what columns you can use. The <code>.head()</code> method is particularly informative.</p>\r\n","<p>In this exercise, you'll use <code>pandas</code> alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a <code>CountVectorizer</code> and investigate some of its features.</p></div>"],"metadata":{"id":"Viz1QcALpfC5"}},{"cell_type":"code","execution_count":25,"source":["df = pd.read_csv('https://raw.githubusercontent.com/lnunesAI/Datacamp/main/2-machine-learning-scientist-with-python/12-introduction-to-natural-language-processing-in-python/datasets/fake_or_real_news.csv')"],"outputs":[],"metadata":{"id":"3AxSzF8TC0ZC","executionInfo":{"status":"ok","timestamp":1612722991685,"user_tz":180,"elapsed":1495,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import <code>CountVectorizer</code> from <code>sklearn.feature_extraction.text</code> and <code>train_test_split</code> from <code>sklearn.model_selection</code>.</li>\r\n","<li>Create a Series <code>y</code> to use for the labels by assigning the <code>.label</code> attribute of <code>df</code> to <code>y</code>.</li>\r\n","<li>Using <code>df[\"text\"]</code> (features) and <code>y</code> (labels), create training and test sets using <code>train_test_split()</code>. Use a <code>test_size</code> of <code>0.33</code> and a <code>random_state</code> of <code>53</code>.</li>\r\n","<li>Create a <code>CountVectorizer</code> object called <code>count_vectorizer</code>. Ensure you specify the keyword argument <code>stop_words=\"english\"</code> so that stop words are removed.</li>\r\n","<li>Fit and transform the training data <code>X_train</code> using the <code>.fit_transform()</code> method of your <code>CountVectorizer</code> object. Do the same with the test data <code>X_test</code>, except using the <code>.transform()</code> method.</li>\r\n","<li>Print the first 10 features of the <code>count_vectorizer</code> using its <code>.get_feature_names()</code> method.</li>\r\n","</ul>"],"metadata":{"id":"KL5U_hS5pqzV"}},{"cell_type":"code","execution_count":26,"source":["# Import the necessary modules\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","# Print the head of df\r\n","print(df.head())\r\n","\r\n","# Create a series to store the labels: y\r\n","y = df.label\r\n","\r\n","# Create training and test sets\r\n","X_train, X_test, y_train, y_test = train_test_split(df.text, y, test_size=0.33, random_state=53)\r\n","\r\n","# Initialize a CountVectorizer object: count_vectorizer\r\n","count_vectorizer = CountVectorizer(stop_words=\"english\")\r\n","\r\n","# Transform the training data using only the 'text' column values: count_train \r\n","count_train = count_vectorizer.fit_transform(X_train)\r\n","\r\n","# Transform the test data using only the 'text' column values: count_test \r\n","count_test = count_vectorizer.transform(X_test)\r\n","\r\n","# Print the first 10 features of the count_vectorizer\r\n","print(count_vectorizer.get_feature_names()[:10])"],"outputs":[{"output_type":"stream","name":"stdout","text":["   Unnamed: 0  ... label\n","0        8476  ...  FAKE\n","1       10294  ...  FAKE\n","2        3608  ...  REAL\n","3       10142  ...  FAKE\n","4         875  ...  REAL\n","\n","[5 rows x 4 columns]\n","['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-WP3q5opcih","executionInfo":{"status":"ok","timestamp":1612722995578,"user_tz":180,"elapsed":3801,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"7c5ed5a6-2cfc-4164-de51-d95ecb403dc1"}},{"cell_type":"markdown","source":["### TfidfVectorizer for text classification"],"metadata":{"id":"QQ8K5W9tuIY7"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Similar to the sparse <code>CountVectorizer</code> created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a <code>TfidfVectorizer</code> and investigate some of its features.</p>\r\n","<p>In this exercise, you'll use <code>pandas</code> and <code>sklearn</code> along with the same <code>X_train</code>, <code>y_train</code> and <code>X_test</code>, <code>y_test</code> DataFrames and Series you created in the last exercise.</p></div>"],"metadata":{"id":"1ChdrGkFuK_z"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import <code>TfidfVectorizer</code> from <code>sklearn.feature_extraction.text</code>.</li>\r\n","<li>Create a <code>TfidfVectorizer</code> object called <code>tfidf_vectorizer</code>. When doing so, specify the keyword arguments <code>stop_words=\"english\"</code> and <code>max_df=0.7</code>.</li>\r\n","<li>Fit and transform the training data. </li>\r\n","<li>Transform the test data.</li>\r\n","<li>Print the first 10 features of <code>tfidf_vectorizer</code>.</li>\r\n","<li>Print the first 5 vectors of the tfidf training data using slicing on the <code>.A</code> (or array) <strong><em>attribute</em></strong> of <code>tfidf_train</code>.</li>\r\n","</ul>"],"metadata":{"id":"hAeo0bGXuNs8"}},{"cell_type":"code","execution_count":27,"source":["# Import TfidfVectorizer\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","\r\n","# Initialize a TfidfVectorizer object: tfidf_vectorizer\r\n","tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\r\n","\r\n","# Transform the training data: tfidf_train \r\n","tfidf_train = tfidf_vectorizer.fit_transform(X_train)\r\n","\r\n","# Transform the test data: tfidf_test \r\n","tfidf_test = tfidf_vectorizer.transform(X_test)\r\n","\r\n","# Print the first 10 features\r\n","print(tfidf_vectorizer.get_feature_names()[:10])\r\n","\r\n","# Print the first 5 vectors of the tfidf training data\r\n","print(tfidf_train.A[:5])"],"outputs":[{"output_type":"stream","name":"stdout","text":["['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6OOi6mQvF-K","executionInfo":{"status":"ok","timestamp":1612723000499,"user_tz":180,"elapsed":4994,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"cc88d809-9d07-4cd0-e29f-21e0e647a0a7"}},{"cell_type":"markdown","source":["### Inspecting the vectors"],"metadata":{"id":"sA_rm1ZvwhWF"}},{"cell_type":"markdown","source":["<div class=\"\"><p>To get a better idea of how the vectors work, you'll investigate them by converting them into <code>pandas</code> DataFrames.</p>\r\n","<p>Here, you'll use the same data structures you created in the previous two exercises (<code>count_train</code>, <code>count_vectorizer</code>, <code>tfidf_train</code>, <code>tfidf_vectorizer</code>) as well as <code>pandas</code>, which is imported as <code>pd</code>.</p></div>"],"metadata":{"id":"0rhARvI8wjoA"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create the DataFrames <code>count_df</code> and <code>tfidf_df</code> by using <code>pd.DataFrame()</code> and specifying the values as the first argument and the columns (or features) as the second argument.<ul>\r\n","<li>The values can be accessed by using the <code>.A</code> attribute of, respectively, <code>count_train</code> and <code>tfidf_train</code>.</li>\r\n","<li>The columns can be accessed using the <code>.get_feature_names()</code> methods of <code>count_vectorizer</code> and <code>tfidf_vectorizer</code>.</li></ul></li>\r\n","<li>Print the head of each DataFrame to investigate their structure. <em>This has been done for you.</em></li>\r\n","<li>Test if the column names are the same for each DataFrame by creating a new object called <code>difference</code> to see the difference between the columns that <code>count_df</code> has from <code>tfidf_df</code>. Columns can be accessed using the <code>.columns</code> attribute of a DataFrame. Subtract the set of <code>tfidf_df.columns</code> from the set of <code>count_df.columns</code>.</li>\r\n","<li>Test if the two DataFrames are equivalent by using the <code>.equals()</code> method on <code>count_df</code> with <code>tfidf_df</code> as the argument.</li>\r\n","</ul>"],"metadata":{"id":"FB2NwkvpwlTh"}},{"cell_type":"code","execution_count":28,"source":["# Create the CountVectorizer DataFrame: count_df\r\n","count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\r\n","\r\n","# Create the TfidfVectorizer DataFrame: tfidf_df\r\n","tfidf_df = pd.DataFrame(tfidf_train.A, columns=count_vectorizer.get_feature_names())\r\n","\r\n","# Print the head of count_df\r\n","print(count_df.head())\r\n","\r\n","# Print the head of tfidf_df\r\n","print(tfidf_df.head())\r\n","\r\n","# Calculate the difference in columns: difference\r\n","difference = set(count_df.columns) - set(tfidf_df.columns)\r\n","print(difference)\r\n","\r\n","# Check whether the DataFrames are equal\r\n","print(count_df.equals(tfidf_df))"],"outputs":[{"output_type":"stream","name":"stdout","text":["   00  000  0000  00000031  000035  00006  ...  ما  محاولات  من  هذا  والمرضى  ยงade\n","0   0    0     0         0       0      0  ...   0        0   0    0        0      0\n","1   0    0     0         0       0      0  ...   0        0   0    0        0      0\n","2   0    0     0         0       0      0  ...   0        0   0    0        0      0\n","3   0    0     0         0       0      0  ...   0        0   0    0        0      0\n","4   0    0     0         0       0      0  ...   0        0   0    0        0      0\n","\n","[5 rows x 56922 columns]\n","    00  000  0000  00000031  000035  ...  محاولات   من  هذا  والمرضى  ยงade\n","0  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n","1  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n","2  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n","3  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n","4  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n","\n","[5 rows x 56922 columns]\n","set()\n","False\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5zd9bwnxdAK","executionInfo":{"status":"ok","timestamp":1612723004930,"user_tz":180,"elapsed":2352,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"0985321d-e077-404d-a31b-7ef5ee42254b"}},{"cell_type":"markdown","source":["## Training and testing a classification model with scikit-learn"],"metadata":{"id":"9B0lS5RZHXJM"}},{"cell_type":"markdown","source":["### Text classification models"],"metadata":{"id":"eLv7c5nQHdz2"}},{"cell_type":"markdown","source":["<p>Which of the below is the most reasonable model to use when training a new supervised model using text vector data?</p>"],"metadata":{"id":"wQpDhadAHvBz"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Random Forests\r\n","\r\n","<b>Naive Bayes</b>\r\n","\r\n","Linear Regression\r\n","\r\n","Deep Learning\r\n","\r\n","</pre>"],"metadata":{"id":"cjQtTKNiHhI_"}},{"cell_type":"markdown","source":["### Training and testing the \"fake news\" model with CountVectorizer"],"metadata":{"id":"39M4xozKH0I0"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the <code>CountVectorizer</code> data.</p>\r\n","<p>The training and test sets have been created, and <code>count_vectorizer</code>, <code>count_train</code>, and <code>count_test</code> have been computed.</p></div>"],"metadata":{"id":"RTLGp2YnKZDm"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Import the <code>metrics</code> module from <code>sklearn</code> and <code>MultinomialNB</code> from <code>sklearn.naive_bayes</code>.</li>\r\n","<li>Instantiate a <code>MultinomialNB</code> classifier called <code>nb_classifier</code>.</li>\r\n","<li>Fit the classifier to the training data.</li>\r\n","<li>Compute the predicted tags for the test data.</li>\r\n","<li>Calculate and print the accuracy score of the classifier.</li>\r\n","<li>Compute the confusion matrix. To make it easier to read, specify the keyword argument <code>labels=['FAKE', 'REAL']</code>.</li>\r\n","</ul>"],"metadata":{"id":"fDKmG5G2Kap2"}},{"cell_type":"code","execution_count":30,"source":["# Import the necessary modules\r\n","from sklearn import metrics\r\n","from sklearn.naive_bayes import MultinomialNB\r\n","\r\n","# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\r\n","nb_classifier = MultinomialNB()\r\n","\r\n","# Fit the classifier to the training data\r\n","nb_classifier.fit(count_train, y_train)\r\n","\r\n","# Create the predicted tags: pred\r\n","pred = nb_classifier.predict(count_test)\r\n","\r\n","# Calculate the accuracy score: score\r\n","score = metrics.accuracy_score(y_test, pred)\r\n","print(score)\r\n","\r\n","# Calculate the confusion matrix: cm\r\n","cm = metrics.confusion_matrix(y_test, pred, ['FAKE', 'REAL'])\r\n","print(cm)"],"outputs":[{"output_type":"stream","name":"stdout","text":["0.893352462936394\n","[[ 865  143]\n"," [  80 1003]]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"biL2O3cjNv1L","executionInfo":{"status":"ok","timestamp":1612730511484,"user_tz":180,"elapsed":1184,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"47281404-4104-4304-e3b7-9af63aa16833"}},{"cell_type":"markdown","source":["### Training and testing the \"fake news\" model with TfidfVectorizer"],"metadata":{"id":"5q28lCYmPAcd"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now that you have evaluated the model using the <code>CountVectorizer</code>, you'll do the same using the <code>TfidfVectorizer</code> with a Naive Bayes model.</p>\r\n","<p>The training and test sets have been created, and <code>tfidf_vectorizer</code>, <code>tfidf_train</code>, and <code>tfidf_test</code> have been computed. Additionally, <code>MultinomialNB</code> and <code>metrics</code> have been imported from, respectively, <code>sklearn.naive_bayes</code> and <code>sklearn</code>.</p></div>"],"metadata":{"id":"Up57-MHSPEVm"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Instantiate a <code>MultinomialNB</code> classifier called <code>nb_classifier</code>.</li>\r\n","<li>Fit the classifier to the training data.</li>\r\n","<li>Compute the predicted tags for the test data.</li>\r\n","<li>Calculate and print the accuracy score of the classifier.</li>\r\n","<li>Compute the confusion matrix. As in the previous exercise, specify the keyword argument <code>labels=['FAKE', 'REAL']</code> so that the resulting confusion matrix is easier to read.</li>\r\n","</ul>"],"metadata":{"id":"7TLH4b7HPGEe"}},{"cell_type":"code","execution_count":31,"source":["# Create a Multinomial Naive Bayes classifier: nb_classifier\r\n","nb_classifier = MultinomialNB()\r\n","\r\n","# Fit the classifier to the training data\r\n","nb_classifier.fit(tfidf_train, y_train)\r\n","\r\n","# Create the predicted tags: pred\r\n","pred = nb_classifier.predict(tfidf_test)\r\n","\r\n","# Calculate the accuracy score: score\r\n","score = metrics.accuracy_score(y_test, pred)\r\n","print(score)\r\n","\r\n","# Calculate the confusion matrix: cm\r\n","cm = metrics.confusion_matrix(y_test, pred, ['FAKE', 'REAL'])\r\n","print(cm)"],"outputs":[{"output_type":"stream","name":"stdout","text":["0.8565279770444764\n","[[ 739  269]\n"," [  31 1052]]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKMFvRmfPf1-","executionInfo":{"status":"ok","timestamp":1612730731593,"user_tz":180,"elapsed":617,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"95d6d426-91c9-4e3e-d8ff-47204302c948"}},{"cell_type":"markdown","source":["### Simple NLP, complex problems"],"metadata":{"id":"6-T-RyU-PwLV"}},{"cell_type":"markdown","source":["### Improving the model"],"metadata":{"id":"IvC6WbFqQtX2"}},{"cell_type":"markdown","source":["<pre>\r\n","Possible Answers\r\n","\r\n","Tweaking alpha levels.\r\n","\r\n","Trying a new classification model.\r\n","\r\n","Training on a larger dataset.\r\n","\r\n","Improving text preprocessing.\r\n","\r\n","<b>All of the above.</b>\r\n","\r\n","</pre>"],"metadata":{"id":"OExgyBNbQvhG"}},{"cell_type":"markdown","source":["### Improving your model"],"metadata":{"id":"cCEv0WV4RENQ"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Your job in this exercise is to test a few different alpha levels using the <code>Tfidf</code> vectors to determine if there is a better performing combination.</p>\r\n","<p>The training and test sets have been created, and <code>tfidf_vectorizer</code>, <code>tfidf_train</code>, and <code>tfidf_test</code> have been computed.</p></div>"],"metadata":{"id":"YMbtnDwwREFG"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Create a list of alphas to try using <code>np.arange()</code>. Values should range from <code>0</code> to <code>1</code> with steps of <code>0.1</code>.</li>\r\n","<li>Create a function <code>train_and_predict()</code> that takes in one argument: <code>alpha</code>. The function should:<ul>\r\n","<li>Instantiate a <code>MultinomialNB</code> classifier with <code>alpha=alpha</code>.</li>\r\n","<li>Fit it to the training data.</li>\r\n","<li>Compute predictions on the test data.</li>\r\n","<li>Compute and return the accuracy score.</li></ul></li>\r\n","<li>Using a <code>for</code> loop, print the <code>alpha</code>, <code>score</code> and a newline in between. Use your <code>train_and_predict()</code> function to compute the <code>score</code>. Does the score change along with the alpha? What is the best alpha?</li>\r\n","</ul>"],"metadata":{"id":"pIrWCAjCRKVn"}},{"cell_type":"code","execution_count":32,"source":["# Create the list of alphas: alphas\r\n","alphas = np.arange(0, 1, 0.1)\r\n","\r\n","# Define train_and_predict()\r\n","def train_and_predict(alpha):\r\n","    # Instantiate the classifier: nb_classifier\r\n","    nb_classifier = MultinomialNB(alpha=alpha)\r\n","    # Fit to the training data\r\n","    nb_classifier.fit(tfidf_train, y_train)\r\n","    # Predict the labels: pred\r\n","    pred = nb_classifier.predict(tfidf_test)\r\n","    # Compute accuracy: score\r\n","    score = metrics.accuracy_score(y_test, pred)\r\n","    return score\r\n","\r\n","# Iterate over the alphas and print the corresponding score\r\n","for alpha in alphas:\r\n","    print('Alpha: ', alpha)\r\n","    print('Score: ', train_and_predict(alpha))\r\n","    print()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha:  0.0\n","Score:  0.8813964610234337\n","\n","Alpha:  0.1\n","Score:  0.8976566236250598\n","\n","Alpha:  0.2\n","Score:  0.8938307030129125\n","\n","Alpha:  0.30000000000000004\n","Score:  0.8900047824007652\n","\n","Alpha:  0.4\n","Score:  0.8857006217120995\n","\n","Alpha:  0.5\n","Score:  0.8842659014825442\n","\n","Alpha:  0.6000000000000001\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n","  'setting alpha = %.1e' % _ALPHA_MIN)\n"]},{"output_type":"stream","name":"stdout","text":["Score:  0.874701099952176\n","\n","Alpha:  0.7000000000000001\n","Score:  0.8703969392635102\n","\n","Alpha:  0.8\n","Score:  0.8660927785748446\n","\n","Alpha:  0.9\n","Score:  0.8589191774270684\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"civfAE_mR2gq","executionInfo":{"status":"ok","timestamp":1612731347906,"user_tz":180,"elapsed":896,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"9f1f441a-696f-4c11-e5ce-49dab1d7b12f"}},{"cell_type":"markdown","source":["### Inspecting your model"],"metadata":{"id":"rL_E70nUSJdJ"}},{"cell_type":"markdown","source":["<div class=\"\"><p>Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.</p>\r\n","<p>You have your well performing tfidf Naive Bayes classifier available as <code>nb_classifier</code>, and the vectors as <code>tfidf_vectorizer</code>.</p></div>"],"metadata":{"id":"Ub59jTtoSL9C"}},{"cell_type":"markdown","source":["Instructions\r\n","<ul>\r\n","<li>Save the class labels as <code>class_labels</code> by accessing the <code>.classes_</code> attribute of <code>nb_classifier</code>.</li>\r\n","<li>Extract the features using the <code>.get_feature_names()</code> method of <code>tfidf_vectorizer</code>.</li>\r\n","<li>Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use <code>zip()</code> with the arguments <code>nb_classifier.coef_[0]</code> and <code>feature_names</code>. Then, use <code>sorted()</code> on this.</li>\r\n","<li>Print the <em>top</em> 20 weighted features for the first label of <code>class_labels</code> and print the bottom 20 weighted features for the second label of <code>class_labels</code>. <em>This has been done for you.</em></li>\r\n","</ul>"],"metadata":{"id":"AfvowIoCSNW7"}},{"cell_type":"code","execution_count":33,"source":["# Get the class labels: class_labels\r\n","class_labels = nb_classifier.classes_\r\n","\r\n","# Extract the features: feature_names\r\n","feature_names = tfidf_vectorizer.get_feature_names()\r\n","\r\n","# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\r\n","feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\r\n","\r\n","# Print the first class label and the top 20 feat_with_weights entries\r\n","print(class_labels[0], feat_with_weights[:20])\r\n","\r\n","# Print the second class label and the bottom 20 feat_with_weights entries\r\n","print(class_labels[1], feat_with_weights[-20:])"],"outputs":[{"output_type":"stream","name":"stdout","text":["FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n","REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfNSr6LlSm4y","executionInfo":{"status":"ok","timestamp":1612731546623,"user_tz":180,"elapsed":723,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ef0769fe-071b-4fa6-8237-a032f863f75b"}}]}