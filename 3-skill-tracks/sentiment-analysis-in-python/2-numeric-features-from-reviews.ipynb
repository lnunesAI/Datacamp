{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-numeric-features-from-reviews.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOx2EkJD1qtwJPzc+JWTz1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9743e28d9bd04ea6bb42c0205c7feb89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_498b1b72c5a041ff8619ff175a76381d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4262cb44844d4762b4337a5d8eeb2270","IPY_MODEL_f08c4af80ffa42fdaef035c2f66116b3"]}},"498b1b72c5a041ff8619ff175a76381d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4262cb44844d4762b4337a5d8eeb2270":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_09a0ca59f32f4b22bf4915ebf4e72b95","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ab4179c49f848b99fdfc26c885cc6db"}},"f08c4af80ffa42fdaef035c2f66116b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_866d8106ee884bc68faeea2127125b03","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/10000 [00:58&lt;00:00, 170.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5aa09fe7ddd54bbb85a3aeb12ca653f1"}},"09a0ca59f32f4b22bf4915ebf4e72b95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ab4179c49f848b99fdfc26c885cc6db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"866d8106ee884bc68faeea2127125b03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5aa09fe7ddd54bbb85a3aeb12ca653f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"f5FTJNcbC9gb"},"source":["# Numeric Features from Reviews\r\n",">  Imagine you are in the shoes of a company offering a variety of products. You want to know which of your products are bestsellers and most of all - why. We embark on step 1 of understanding the reviews of products, using a dataset with Amazon product reviews. To that end, we transform the text into a numeric form and consider a few complexities in the process.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Datacamp]\r\n","- image: images/datacamp/___"]},{"cell_type":"markdown","metadata":{"id":"f9J9naPqLbDt"},"source":["> Note: This is a summary of the course's chapter 2 exercises \"Sentiment Analysis in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/natural-language-processing-in-python)"]},{"cell_type":"code","metadata":{"id":"7SbXqsjxFOUG"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvrKEMkeEF2g"},"source":["## Bag-of-words"]},{"cell_type":"markdown","metadata":{"id":"Ho1eZTss8AvH"},"source":["### Which statement about BOW is true?"]},{"cell_type":"markdown","metadata":{"id":"VTIJ2QziRSKE"},"source":["<p>You were introduced to a bag-of-words(BOW) and some of its characteristics in the video. Which of the following statements about BOW <strong>is</strong> true?</p>"]},{"cell_type":"markdown","metadata":{"id":"SbY5dssjRPXj"},"source":["<pre>\r\n","Possible Answers\r\n","Bag-of-words preserves the word order and grammar rules.\r\n","Bag-of-words describes the order and frequency of words or tokens within a corpus of documents.\r\n","<b>Bag-of-words is a simple but effective method to build a vocabulary of all the words occurring in a document.</b>\r\n","Bag-of-words can only be applied to a large document, not to shorter documents or single sentences.\r\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"WULSMItcRk6p"},"source":["### Your first BOW"]},{"cell_type":"markdown","metadata":{"id":"3wIE86tkRnvT"},"source":["<div class=\"\"><p>A bag-of-words is an approach to transform text to numeric form. </p>\r\n","<p>In this exercise, you will apply a BOW to the <code>annak</code> list before moving on to a larger dataset in the next exercise.  </p>\r\n","<p>Your task will be to work with this list and apply a BOW using the <code>CountVectorizer()</code>. This transformation is your first step in being able to understand the sentiment of a text. Pay attention to words which might carry a strong sentiment. </p>\r\n","<p>Remember that the output of a <code>CountVectorizer()</code> is a sparse matrix, which stores only entries which are non-zero. To look at the actual content of this matrix, we convert it to a dense array using the <code>.toarray()</code> method.</p>\r\n","<p>Note that in this case you don't need to specify the <code>max_features</code> argument because the text is short.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"1ZNci2R5RpOT"},"source":["Instructions\r\n","<ul>\r\n","<li>Import the count vectorizer function from <code>sklearn.feature_extraction.text</code>.</li>\r\n","<li>Build and fit the vectorizer on the small dataset.</li>\r\n","<li>Create the BOW representation with name <code>anna_bow</code> by calling the <code>transform()</code> method.</li>\r\n","<li>Print the BOW result as a dense array.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"w48_1NTN8ASP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615382781190,"user_tz":180,"elapsed":1067,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"8c67bfc1-2462-4101-9036-fefc2cf29ee8"},"source":["# Import the required function\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\r\n","\r\n","# Build the vectorizer and fit it\r\n","anna_vect = CountVectorizer(annak)\r\n","anna_vect.fit(annak)\r\n","\r\n","# Create the bow representation\r\n","anna_bow = anna_vect.transform(annak)\r\n","\r\n","# Print the bag-of-words result \r\n","print(anna_bow.toarray())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1 1 1 0 1 0 1 0 0 0 0 0 0]\n"," [0 0 0 1 0 1 0 1 1 1 1 2 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZnSHjY4mdtGI"},"source":["**You have transformed the first sentence of Anna Karenina to an array counting the frequencies of each word. However, the output is not very readable, is it? We are still missing the names of the features. And does the approach change when we apply it to a larger dataset?**"]},{"cell_type":"markdown","metadata":{"id":"f57vEwkceUdd"},"source":["### BOW using product reviews"]},{"cell_type":"markdown","metadata":{"id":"k2-CApaXeW-u"},"source":["<div class=\"\"><p>You practiced a BOW on a small dataset. Now you will apply it to a sample of Amazon product reviews. The data has been imported for you and is called <code>reviews</code>. It contains two columns. The first one is called <code>score</code> and it is <code>0</code> when the review is negative, and <code>1</code> when it is positive. The second column is called <code>review</code> and it contains the text of the review that a customer wrote. Feel free to explore the data in the IPython Shell.</p>\r\n","<p>Your task is to build a BOW vocabulary, using the <code>review</code> column.</p>\r\n","<p>Remember that we can call the <code>.get_feature_names()</code> method on the vectorizer to obtain a list of all the vocabulary elements.</p></div>"]},{"cell_type":"code","metadata":{"id":"FSEq2qWQS6Tt"},"source":["reviews_df = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/3-skill-tracks/sentiment-analysis-in-python/datasets/amazon_reviews_sample.csv', index_col=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"luokRCtyUgoK"},"source":["reviews = reviews_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwKTy4OHeYWh"},"source":["Instructions\r\n","<ul>\r\n","<li>Create a CountVectorizer object, specifying the maximum number of features. </li>\r\n","<li>Fit the vectorizer. </li>\r\n","<li>Transform the fitted vectorizer.</li>\r\n","<li>Create a DataFrame where you transform the sparse matrix to a dense array and make sure to correctly specify the names of columns.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"9nnHRYyifDaj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615382888034,"user_tz":180,"elapsed":1849,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"ccc57457-6377-47e8-9765-315955988c5e"},"source":["from sklearn.feature_extraction.text import CountVectorizer \r\n","\r\n","# Build the vectorizer, specify max features \r\n","vect = CountVectorizer(max_features=100)\r\n","# Fit the vectorizer\r\n","vect.fit(reviews.review)\r\n","\r\n","# Transform the review column\r\n","X_review = vect.transform(reviews.review)\r\n","\r\n","# Create the bow representation\r\n","X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\r\n","print(X_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   about  after  all  also  am  an  ...  will  with  work  would  you  your\n","0      0      0    1     0   0   0  ...     0     1     0      2    0     1\n","1      0      0    0     0   0   0  ...     0     0     0      1    1     0\n","2      0      0    3     0   0   1  ...     0     0     1      1    2     0\n","3      0      0    0     0   0   0  ...     0     0     0      0    0     0\n","4      0      1    0     0   0   0  ...     0     0     0      0    3     1\n","\n","[5 rows x 100 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sVhiIoHsfGyc"},"source":["**You have successfully built your first BOW generated vocabulary and transformed it to numeric features of the dataset!**"]},{"cell_type":"markdown","metadata":{"id":"2BUPNyB2gEjl"},"source":["### Getting granular with n-grams"]},{"cell_type":"markdown","metadata":{"id":"95yOa_mkqGtR"},"source":["### Specify token sequence length with BOW"]},{"cell_type":"markdown","metadata":{"id":"Lr4YKNCBqJXp"},"source":["<div class=\"\"><p>We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.</p>\r\n","<p>In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the <code>review</code> column and specify the sequence length of tokens.</p></div>"]},{"cell_type":"code","metadata":{"id":"PeKhxfVDUeWh"},"source":["reviews = reviews_df[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7gEE6BBqLAh"},"source":["Instructions\r\n","<ul>\r\n","<li>Build the vectorizer, specifying the token sequence length to be uni- and bigrams.</li>\r\n","<li>Fit the vectorizer.</li>\r\n","<li>Transform the fitted vectorizer.</li>\r\n","<li>In the DataFrame, make sure to correctly specify the column names.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"c39DdUIsqdrc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615382901306,"user_tz":180,"elapsed":503,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"cdbc3219-d043-414e-a09d-4fc2d3fd363e"},"source":["from sklearn.feature_extraction.text import CountVectorizer \r\n","\r\n","# Build the vectorizer, specify token sequence and fit\r\n","vect = CountVectorizer(ngram_range=(1,2))\r\n","vect.fit(reviews.review)\r\n","\r\n","# Transform the review column\r\n","X_review = vect.transform(reviews.review)\r\n","\r\n","# Create the bow representation\r\n","X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\r\n","print(X_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   10  10 95  10 cups  ...  zen  zen baseball  zen motorcycle\n","0   0      0        0  ...    0             0               0\n","1   0      0        0  ...    0             0               0\n","2   0      0        0  ...    0             0               0\n","3   0      0        0  ...    0             0               0\n","4   0      0        0  ...    0             0               0\n","\n","[5 rows x 8436 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fbhKqW0Nqsoz"},"source":["### Size of vocabulary of movies reviews"]},{"cell_type":"markdown","metadata":{"id":"wBa-_h75qvSL"},"source":["<div class=\"\"><p>In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the <code>movies</code> reviews dataset. The first column is the <code>review</code>, which is of type <code>object</code> and the second column is the <code>label</code>, which is <code>0</code> for a negative review and <code>1</code> for a positive one. </p>\r\n","<p>The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features.</p></div>"]},{"cell_type":"code","metadata":{"id":"fbDP4VKSU6mJ"},"source":["movies_df = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/3-skill-tracks/sentiment-analysis-in-python/datasets/IMDB_sample.csv', index_col=0)\r\n","movies = movies_df[:1000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blF14qhbqxIh"},"source":["Instructions 1/3\r\n","<p>Using the <code>movies</code> dataset, limit the size of the vocabulary to  <code>100</code>.</p>"]},{"cell_type":"code","metadata":{"id":"YkKUr22HtCxX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383032013,"user_tz":180,"elapsed":948,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"16f1cac4-27f1-465b-b099-0592eeecc301"},"source":["from sklearn.feature_extraction.text import CountVectorizer \r\n","\r\n","# Build the vectorizer, specify size of vocabulary and fit\r\n","vect = CountVectorizer(max_features=100)\r\n","vect.fit(movies.review)\r\n","\r\n","# Transform the review column\r\n","X_review = vect.transform(movies.review)\r\n","# Create the bow representation\r\n","X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\r\n","print(X_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   about  all  also  an  and  any  ...  which  who  will  with  would  you\n","0      0    0     0   0    1    0  ...      0    0     0     1      1    0\n","1      0    3     1   1   11    0  ...      2    0     2     7      2    3\n","2      0    0     0   1    7    0  ...      0    0     0     2      0    0\n","3      0    0     0   2    1    0  ...      0    1     0     0      0    1\n","4      0    3     0   0    8    0  ...      1    0     0     2      0    0\n","\n","[5 rows x 100 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IKcFtpT4qxIt"},"source":["Instructions 2/3\r\n","<p>Using the <code>movies</code> dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents.</p>"]},{"cell_type":"code","metadata":{"id":"A4_e7q_dtlsB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383041451,"user_tz":180,"elapsed":934,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"768d5ce3-085b-4abf-8f23-1905e05b70c3"},"source":["# Build and fit the vectorizer\r\n","vect = CountVectorizer(max_df=200)\r\n","vect.fit(movies.review)\r\n","\r\n","# Transform the review column\r\n","X_review = vect.transform(movies.review)\r\n","# Create the bow representation\r\n","X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\r\n","print(X_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   00  000  000s  007  00s  ...  zooms  zsigmond  zulu  zuniga  zvyagvatsev\n","0   0    0     0    0    0  ...      0         0     0       0            0\n","1   0    0     0    0    0  ...      0         0     0       0            0\n","2   0    0     0    0    0  ...      0         0     0       0            0\n","3   0    0     0    0    0  ...      0         0     0       0            0\n","4   0    0     0    0    0  ...      0         0     0       0            0\n","\n","[5 rows x 17669 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uyHlesIcqxIu"},"source":["Instructions 3/3\r\n","<p>Using the <code>movies</code> dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents.</p>"]},{"cell_type":"code","metadata":{"id":"H4uTeusZt2TW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383046793,"user_tz":180,"elapsed":944,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"7b2618e6-9854-4a4e-8013-a40be38bc820"},"source":["# Build and fit the vectorizer\r\n","vect = CountVectorizer(min_df=50)\r\n","vect.fit(movies.review)\r\n","\r\n","# Transform the review column\r\n","X_review = vect.transform(movies.review)\r\n","# Create the bow representation\r\n","X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\r\n","print(X_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   10  about  absolutely  acting  action  ...  yes  yet  you  young  your\n","0   0      0           0       0       0  ...    0    0    0      0     0\n","1   1      0           0       1       0  ...    0    1    3      0     2\n","2   0      0           0       0       0  ...    0    0    0      1     0\n","3   0      0           0       0       1  ...    0    0    1      1     0\n","4   1      0           0       0       1  ...    0    0    0      0     0\n","\n","[5 rows x 434 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"90z_O3Ckt9b1"},"source":["**Any of the three methods you applied here can be used to limit the size of the vocabulary. Which of the three methods you used resulted in the lowest number of constructed features?**"]},{"cell_type":"markdown","metadata":{"id":"vMiyj1ypuAVQ"},"source":["### BOW with n-grams and vocabulary size"]},{"cell_type":"markdown","metadata":{"id":"FEQQgv4JvYeT"},"source":["<p>In this exercise, you will practice building a bag-of-words once more, using the <code>reviews</code> dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence.</p>"]},{"cell_type":"markdown","metadata":{"id":"wmtWBPB2vZ2y"},"source":["Instructions\r\n","<ul>\r\n","<li>Import the vectorizer from <code>sklearn</code>.</li>\r\n","<li>Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.</li>\r\n","<li>Fit the vectorizer to the <code>review</code> column.</li>\r\n","<li>Create a DataFrame from the BOW representation.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"Sh9ULa0Kv7OM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383361887,"user_tz":180,"elapsed":497,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d0ee09c4-b4ab-4c08-ffd8-a504a3c985e6"},"source":["#Import the vectorizer\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","\r\n","# Build the vectorizer, specify max features and fit\r\n","vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\r\n","vect.fit(reviews.review)\r\n","\r\n","# Transform the review\r\n","X_review = vect.transform(reviews.review)\r\n","\r\n","# Create a DataFrame from the bow representation\r\n","X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\r\n","print(X_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   1980 style  aa batteries  ...  your money  yr old\n","0           0             0  ...           0       0\n","1           0             0  ...           0       0\n","2           0             0  ...           0       0\n","3           0             0  ...           0       0\n","4           0             0  ...           0       0\n","\n","[5 rows x 1000 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jIrTksm2wAtt"},"source":["**You have successfully created a bag-of-words representation of the product reviews dataset, including more sophisticated sequence of tokens, while limiting the size of the vocabulary**"]},{"cell_type":"markdown","metadata":{"id":"rKspiLPGwF5_"},"source":["## Build new features from text"]},{"cell_type":"markdown","metadata":{"id":"fZq-mGDy7SKT"},"source":["### Tokenize a string from GoT"]},{"cell_type":"markdown","metadata":{"id":"aG29Ymjm7VK7"},"source":["<div class=\"\"><p>A first standard step when working with text is to tokenize it, in other words, split a bigger string into individual strings, which are usually single words (tokens). </p>\r\n","<p>A string <code>GoT</code> has been created for you and it contains a quote from George R.R. Martin's <em>Game of Thrones</em>. Your task is to split it into individual tokens.</p></div>"]},{"cell_type":"code","metadata":{"id":"vYPjhFC27jUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383392372,"user_tz":180,"elapsed":1783,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"de2672b2-bac1-46f7-9c62-5221509756e0"},"source":["import nltk\r\n","nltk.download('punkt')\r\n","GoT = 'Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armour yourself in it, and it will never be used to hurt you.'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zUsfUVe_7WoM"},"source":["Instructions\r\n","<ul>\r\n","<li>Import the word tokenizing function from <code>nltk</code>.</li>\r\n","<li>Transform the <code>GoT</code> string to word tokens.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"nGzUjmq17iaw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383395159,"user_tz":180,"elapsed":496,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"0a696cad-5a11-46af-f497-96a319a29faa"},"source":["# Import the required function\r\n","from nltk import word_tokenize\r\n","\r\n","# Transform the GoT string to word tokens\r\n","print(word_tokenize(GoT))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Never', 'forget', 'what', 'you', 'are', ',', 'for', 'surely', 'the', 'world', 'will', 'not', '.', 'Make', 'it', 'your', 'strength', '.', 'Then', 'it', 'can', 'never', 'be', 'your', 'weakness', '.', 'Armour', 'yourself', 'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'used', 'to', 'hurt', 'you', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0p4f6jWv7o9R"},"source":["### Word tokens from the Avengers"]},{"cell_type":"markdown","metadata":{"id":"g2aSJ1GL7rnK"},"source":["<div class=\"\"><p>Now that you have tokenized your first string, it is time to iterate over items of a list and tokenize them as well. An easy way to do that with one line of code is with a list comprehension.</p>\r\n","<p>A list <code>avengers</code> has been created for you. It contains a few quotes from the <em>Avengers</em> movies. You can explore it in the IPython Shell.</p></div>"]},{"cell_type":"code","metadata":{"id":"uyZPWbF473Gu"},"source":["avengers = [\"Cause if we can't protect the Earth, you can be d*** sure we'll avenge it\",\r\n"," 'There was an idea to bring together a group of remarkable people, to see if we could become something more',\r\n"," \"These guys come from legend, Captain. They're basically Gods.\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ozm-zlSV7u6_"},"source":["Instructions\r\n","<ul>\r\n","<li>Import the required function and package.</li>\r\n","<li>Apply the word tokenizing function on each item of our list.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"eGq6eHS872A1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383401649,"user_tz":180,"elapsed":654,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d0bdbf38-67da-4ec3-eef8-8b3e5f10645b"},"source":["# Import the word tokenizing function\r\n","from nltk import word_tokenize\r\n","\r\n","# Tokenize each item in the avengers \r\n","tokens_avengers = [word_tokenize(item) for item in avengers]\r\n","\r\n","print(tokens_avengers)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['Cause', 'if', 'we', 'ca', \"n't\", 'protect', 'the', 'Earth', ',', 'you', 'can', 'be', 'd***', 'sure', 'we', \"'ll\", 'avenge', 'it'], ['There', 'was', 'an', 'idea', 'to', 'bring', 'together', 'a', 'group', 'of', 'remarkable', 'people', ',', 'to', 'see', 'if', 'we', 'could', 'become', 'something', 'more'], ['These', 'guys', 'come', 'from', 'legend', ',', 'Captain', '.', 'They', \"'re\", 'basically', 'Gods', '.']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y1mdvDiA7_mQ"},"source":["### A feature for the length of a review"]},{"cell_type":"markdown","metadata":{"id":"wFJL4DwV8DBw"},"source":["<div class=\"\"><p>You have now worked with a string and a list with string items, it is time to use a larger sample of data.</p>\r\n","<p>You task in this exercise is to create a new feature for the length of a review, using the familiar <code>reviews</code> dataset.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"xD5NEjyf8JT_"},"source":["Instructions 1/2\r\n","<ul>\r\n","<li>Import the word tokenizing function from the required package.</li>\r\n","<li>Apply the function to the <code>review</code> column of the <code>reviews</code> dataset.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"S8Q83HCP9Fb8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383405198,"user_tz":180,"elapsed":499,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b59f1da9-757b-42a6-a476-b487bdbf284a"},"source":["# Import the needed packages\r\n","from nltk import word_tokenize\r\n","\r\n","# Tokenize each item in the review column \r\n","word_tokens = [word_tokenize(review) for review in reviews.review]\r\n","\r\n","# Print out the first item of the word_tokens list\r\n","print(word_tokens[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Stuning', 'even', 'for', 'the', 'non-gamer', ':', 'This', 'sound', 'track', 'was', 'beautiful', '!', 'It', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'I', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'I', 'have', 'played', 'the', 'game', 'Chrono', 'Cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'I', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'It', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'It', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZTBhD1i98JUF"},"source":["Instructions 2/2\r\n","<ul>\r\n","<li>Iterate over the created <code>word_tokens</code> list. </li>\r\n","<li>As you iterate, find the length of each item in the list and append it to the empty <code>len_tokens</code> list. </li>\r\n","<li>Create a new feature <code>n_words</code> in the <code>reviews</code> for the length of the reviews.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"_ytuHEHp9pJP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383414583,"user_tz":180,"elapsed":499,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"91d29de0-968f-42ed-849c-25a3a2d6e2ee"},"source":["# Create an empty list to store the length of reviews\r\n","len_tokens = []\r\n","\r\n","# Iterate over the word_tokens list and determine the length of each item\r\n","for i in range(len(word_tokens)):\r\n","     len_tokens.append(len(word_tokens[i]))\r\n","#len_tokens = [len(x) for x in word_tokens]\r\n","\r\n","# Create a new feature for the lengh of each review\r\n","reviews['n_words'] = len_tokens "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"tOyR5yJHW0T1","executionInfo":{"status":"ok","timestamp":1615383449292,"user_tz":180,"elapsed":505,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e6e56616-dab6-4831-f181-145a0f6575d2"},"source":["reviews.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","      <th>review</th>\n","      <th>n_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Stuning even for the non-gamer: This sound tr...</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>The best soundtrack ever to anything.: I'm re...</td>\n","      <td>109</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Amazing!: This soundtrack is my favorite musi...</td>\n","      <td>165</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>Excellent Soundtrack: I truly like this sound...</td>\n","      <td>145</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>Remember, Pull Your Jaw Off The Floor After H...</td>\n","      <td>109</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   score                                             review  n_words\n","0      1   Stuning even for the non-gamer: This sound tr...       87\n","1      1   The best soundtrack ever to anything.: I'm re...      109\n","2      1   Amazing!: This soundtrack is my favorite musi...      165\n","3      1   Excellent Soundtrack: I truly like this sound...      145\n","4      1   Remember, Pull Your Jaw Off The Floor After H...      109"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"k7ycuyLX-0mK"},"source":["## Can you guess the language?"]},{"cell_type":"markdown","metadata":{"id":"Mt9NN0gdidE2"},"source":["### Identify the language of a string"]},{"cell_type":"markdown","metadata":{"id":"lLNxR7A1ifbH"},"source":["<div class=\"\"><p>Sometimes you might need to analyze the sentiment of non-English text. Your first task in such a case will be to identify the foreign language. </p>\r\n","<p>In this exercise you will identify the language of a single string. A string called <code>foreign</code> has been created for you. Feel free to explore it in the IPython Shell.</p></div>"]},{"cell_type":"code","metadata":{"id":"fEUxARhpivEJ"},"source":["foreign = 'La histoire rendu Ã©tai fidÃ¨le, excellent, et grand.'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0vsnEh_W42l"},"source":["%%capture\r\n","!pip install langdetect"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzAKgg-PihYO"},"source":["Instructions\r\n","<ul>\r\n","<li>Import the required function from the language detection package.</li>\r\n","<li>Detect the language of the <code>foreign</code> string.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"Ub_XvfC3ithJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383510810,"user_tz":180,"elapsed":1028,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"6c7fcea8-ca7a-4512-819c-b198f2ccabf8"},"source":["# Import the language detection function and package\r\n","from langdetect import detect_langs\r\n","\r\n","# Detect the language of the foreign string\r\n","print(detect_langs(foreign))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[fr:0.999997159109143]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GQtX8PQbiyjL"},"source":["### Detect language of a list of strings"]},{"cell_type":"markdown","metadata":{"id":"fA8Bvt69i0z0"},"source":["<p>Now you will detect the language of each item in a list. A list called <code>sentences</code> has been created for you and it contains 3 sentences, each in a different language. They have been randomly extracted from the product reviews dataset.</p>"]},{"cell_type":"code","metadata":{"id":"NEfdLgyyjCLw"},"source":["sentences = ['La histoire rendu Ã©tai fidÃ¨le, excellent, et grand.',\r\n"," 'Excelente muy recomendable.',\r\n"," 'It had a leak from day one but the return and exchange process was very quick.']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KT8fhE4i2hc"},"source":["Instructions\r\n","<ul>\r\n","<li>Iterate over the sentences in the list.</li>\r\n","<li>Detect the language of each sentence and append the detected language to the empty list <code>languages</code>.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"FYsGf0uEjAi_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615383529027,"user_tz":180,"elapsed":708,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c575b802-2570-43df-c90c-f252ba2566b6"},"source":["from langdetect import detect_langs\r\n","\r\n","languages = []\r\n","\r\n","# Loop over the sentences in the list and detect their language\r\n","for sentence in range(len(sentences)):\r\n","    languages.append(detect_langs(sentences[sentence]))\r\n","    \r\n","print('The detected languages are: ', languages)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The detected languages are:  [[fr:0.9999993247241308], [es:0.9999944158600821], [en:0.9999961920509105]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vh3dSDt0jM67"},"source":["### Language detection of product reviews"]},{"cell_type":"markdown","metadata":{"id":"VMagvt7SjPtE"},"source":["<div class=\"\"><p>You will practice language detection on a small dataset called <code>non_english_reviews</code>. It is a sample of non-English reviews from the Amazon product reviews. </p>\r\n","<p>You will iterate over the rows of the dataset, detecting the language of each row and appending it to an empty list. The list needs to be cleaned so that it only contains the language of the review such as <code>'en'</code> for English instead of the regular output <code>en:0.9987654</code>. Remember that the language detection function might detect more than one language and the first item in the returned list is the most likely candidate. Finally, you will assign the list to a new column. </p>\r\n","<p>The logic is the same as used in the slides and the exercise before but instead of applying the function to a list, you work with a dataset.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"22j0LfDKjRH0"},"source":["Instructions\r\n","<ul>\r\n","<li>Iterate over the rows of the <code>non_english_reviews</code> dataset.   </li>\r\n","<li>Inside the loop, detect the language of the second column of the dataset.</li>\r\n","<li>Clean the string by splitting on a <code>:</code> inside the list comprehension expression.</li>\r\n","<li>Finally, assign the cleaned list to a new column.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254,"referenced_widgets":["9743e28d9bd04ea6bb42c0205c7feb89","498b1b72c5a041ff8619ff175a76381d","4262cb44844d4762b4337a5d8eeb2270","f08c4af80ffa42fdaef035c2f66116b3","09a0ca59f32f4b22bf4915ebf4e72b95","5ab4179c49f848b99fdfc26c885cc6db","866d8106ee884bc68faeea2127125b03","5aa09fe7ddd54bbb85a3aeb12ca653f1"]},"id":"-WJXg0EJXqeF","executionInfo":{"status":"ok","timestamp":1615384085749,"user_tz":180,"elapsed":59230,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d41f1057-5347-4fa0-fd1d-1fc3caeace79"},"source":["from langdetect import detect_langs\r\n","languages = [] \r\n","non_english_reviews = reviews_df\r\n","# Loop over the rows of the dataset and append  \r\n","for row in tqdm(range(len(non_english_reviews))):\r\n","    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\r\n","\r\n","# Clean the list by splitting     \r\n","languages = [str(lang).split(':')[0][1:] for lang in languages]\r\n","\r\n","# Assign the list to a new feature \r\n","non_english_reviews['language'] = languages\r\n","\r\n","non_english_reviews.head()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9743e28d9bd04ea6bb42c0205c7feb89","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","      <th>review</th>\n","      <th>language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Stuning even for the non-gamer: This sound tr...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>The best soundtrack ever to anything.: I'm re...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Amazing!: This soundtrack is my favorite musi...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>Excellent Soundtrack: I truly like this sound...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>Remember, Pull Your Jaw Off The Floor After H...</td>\n","      <td>en</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   score                                             review language\n","0      1   Stuning even for the non-gamer: This sound tr...       en\n","1      1   The best soundtrack ever to anything.: I'm re...       en\n","2      1   Amazing!: This soundtrack is my favorite musi...       en\n","3      1   Excellent Soundtrack: I truly like this sound...       en\n","4      1   Remember, Pull Your Jaw Off The Floor After H...       en"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"9fRzWlS5YLLm","executionInfo":{"status":"ok","timestamp":1615384162570,"user_tz":180,"elapsed":498,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"0b1e0bab-6aed-4c8a-f46d-859f17c66bcb"},"source":["non_english_reviews[non_english_reviews.language != 'en'].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","      <th>review</th>\n","      <th>language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1249</th>\n","      <td>1</td>\n","      <td>Il grande ritorno!: E' dai tempi del tour di ...</td>\n","      <td>it</td>\n","    </tr>\n","    <tr>\n","      <th>1259</th>\n","      <td>1</td>\n","      <td>La reencarnaciÃ³n vista por un cientÃ­fico: El ...</td>\n","      <td>es</td>\n","    </tr>\n","    <tr>\n","      <th>1260</th>\n","      <td>1</td>\n","      <td>Excelente Libro / Amazing book!!: Este libro ...</td>\n","      <td>es</td>\n","    </tr>\n","    <tr>\n","      <th>1261</th>\n","      <td>1</td>\n","      <td>Magnifico libro: Brian Weiss ha dejado una ma...</td>\n","      <td>es</td>\n","    </tr>\n","    <tr>\n","      <th>1639</th>\n","      <td>1</td>\n","      <td>El libro mas completo que existe para nosotra...</td>\n","      <td>es</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      score                                             review language\n","1249      1   Il grande ritorno!: E' dai tempi del tour di ...       it\n","1259      1   La reencarnaciÃ³n vista por un cientÃ­fico: El ...       es\n","1260      1   Excelente Libro / Amazing book!!: Este libro ...       es\n","1261      1   Magnifico libro: Brian Weiss ha dejado una ma...       es\n","1639      1   El libro mas completo que existe para nosotra...       es"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kLAdej6Zr66","executionInfo":{"status":"ok","timestamp":1615384207592,"user_tz":180,"elapsed":483,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"007ceb42-7158-4f96-ca82-6e7528b7e25b"},"source":["sum(non_english_reviews.language[non_english_reviews.language != 'en'].value_counts())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYzvhuItYK3e","executionInfo":{"status":"ok","timestamp":1615384203807,"user_tz":180,"elapsed":490,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"0f6b417e-bd35-4886-8fea-321d98edc68c"},"source":["non_english_reviews.language[non_english_reviews.language != 'en'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["es    16\n","fr     8\n","de     3\n","it     1\n","id     1\n","Name: language, dtype: int64"]},"metadata":{"tags":[]},"execution_count":100}]}]}