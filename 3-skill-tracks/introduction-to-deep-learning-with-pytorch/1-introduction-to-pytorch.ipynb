{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1-introduction-to-pytorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO18W/awbe9reQc/LR6ELdB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f5FTJNcbC9gb"},"source":["# Introduction to PyTorch\r\n",">  In this first chapter, we introduce basic concepts of neural networks and deep learning using PyTorch library.\r\n","\r\n","- toc: true \r\n","- badges: true\r\n","- comments: true\r\n","- author: Lucas Nunes\r\n","- categories: [Datacamp]\r\n","- image: images/datacamp/___"]},{"cell_type":"markdown","metadata":{"id":"f9J9naPqLbDt"},"source":["> Note: This is a summary of the course's chapter 1 exercises \"Introduction to Deep Learning with PyTorch\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/deep-learning-in-python)"]},{"cell_type":"markdown","metadata":{"id":"UvrKEMkeEF2g"},"source":["## Introduction to PyTorch"]},{"cell_type":"markdown","metadata":{"id":"Ho1eZTss8AvH"},"source":["### Creating tensors in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"oC-VKBOIibGi"},"source":["<div class=\"\"><p>Random tensors are very important in neural networks. Parameters of the neural networks typically are initialized with random weights (random tensors).</p>\r\n","<p>Let us start practicing building tensors in PyTorch library. As you know, tensors are arrays with an arbitrary number of dimensions, corresponding to NumPy's ndarrays. You are going to create a random tensor of sizes 3 by 3 and set it to variable <code>your_first_tensor</code>. Then, you will need to print it. Finally, calculate its size in variable <code>tensor_size</code> and print its value.</p>\r\n","<p><em>NB: In case you have trouble solving the problems, you can always refer to slides in the bottom right of the screen.</em></p></div>"]},{"cell_type":"markdown","metadata":{"id":"nr16KezAidR1"},"source":["Instructions\r\n","<ul>\r\n","<li>Import PyTorch main library.</li>\r\n","<li>Create the variable <code>your_first_tensor</code> and set it to a random torch tensor of size 3 by 3.</li>\r\n","<li>Calculate its shape (dimension sizes) and set it to variable <code>tensor_size</code>.</li>\r\n","<li>Print the values of <code>your_first_tensor</code> and <code>tensor_size</code>.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"jlbQbWIRiWj8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615038387430,"user_tz":180,"elapsed":1096,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"7f61c508-2e0b-4e19-d4d7-9598b45ed6f1"},"source":["# Import torch\r\n","import torch\r\n","\r\n","# Create random tensor of size 3 by 3\r\n","your_first_tensor = torch.rand(3, 3)\r\n","\r\n","# Calculate the shape of the tensor\r\n","tensor_size = your_first_tensor.shape\r\n","\r\n","# Print the values of the tensor and its shape\r\n","print(your_first_tensor)\r\n","print(tensor_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.0141, 0.0859, 0.3999],\n","        [0.6517, 0.0857, 0.3699],\n","        [0.0851, 0.9952, 0.5838]])\n","torch.Size([3, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ouD_mOnBi-T5"},"source":["### Matrix multiplication"]},{"cell_type":"markdown","metadata":{"id":"SKe2y0VMjA3g"},"source":["<div class=\"\"><p>There are many important types of matrices which have their uses in neural networks. Some important matrices are matrices of ones (where each entry is set to 1) and the identity matrix (where the diagonal is set to 1 while all other values are 0). The identity matrix is very important in linear algebra: any matrix multiplied with identity matrix is simply the original matrix.</p>\r\n","<p>Let us experiment with these two types of matrices. You are going to build a matrix of ones with shape 3 by 3 called <code>tensor_of_ones</code> and an identity matrix of the same shape, called <code>identity_tensor</code>. We are going to see what happens when we multiply these two matrices, and what happens if we do an element-wise multiplication of them.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"hMurrAhajCbP"},"source":["Instructions\r\n","<ul>\r\n","<li>Create a matrix of ones with shape 3 by 3, and put it on variable <code>tensor_of_ones</code>.</li>\r\n","<li>Create an identity matrix with shape 3 by 3, and put it on variable <code>identity_tensor</code>.</li>\r\n","<li>Do a matrix multiplication of <code>tensor_of_ones</code> with <code>identity_tensor</code> and print its value.</li>\r\n","<li>Do an element-wise multiplication of <code>tensor_of_ones</code> with <code>identity_tensor</code> and print its value.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"H2RKyWq4jkYv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615038465351,"user_tz":180,"elapsed":562,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"44beb449-9dad-49b8-90bd-c6fd69e8cc0c"},"source":["# Create a matrix of ones with shape 3 by 3\r\n","tensor_of_ones = torch.ones(3, 3)\r\n","\r\n","# Create an identity matrix with shape 3 by 3\r\n","identity_tensor = torch.eye(3)\r\n","\r\n","# Do a matrix multiplication of tensor_of_ones with identity_tensor\r\n","matrices_multiplied = torch.matmul(tensor_of_ones, identity_tensor)\r\n","print(matrices_multiplied)\r\n","\r\n","# Do an element-wise multiplication of tensor_of_ones with identity_tensor\r\n","element_multiplication = tensor_of_ones * identity_tensor\r\n","print(element_multiplication)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EM7k3Gk5jww0"},"source":["**matrices_multiplied is same as tensor_of_ones (because identity matrix is the neutral element in matrix multiplication, the product of any matrix multiplied with it gives the original matrix), while element_multiplication is same as identity_tensor.**"]},{"cell_type":"markdown","metadata":{"id":"hVUXRZkpjz01"},"source":["## Forward propagation"]},{"cell_type":"markdown","metadata":{"id":"M8tqK1U8mGKW"},"source":["### Forward pass"]},{"cell_type":"markdown","metadata":{"id":"T6TzPwnvmInH"},"source":["<div class=\"\"><p>Let's have something resembling more a neural network. The computational graph has been given below. You are going to initialize 3 large random tensors, and then do the operations as given in the computational graph. The final operation is the mean of the tensor, given by <code>torch.mean(your_tensor)</code>.</p>\r\n","<p><img src=\"https://assets.datacamp.com/production/repositories/4094/datasets/ab707279d7be2835c17787a38c6e2e54f6d89409/graph_exercise.jpg\" alt=\"\"></p></div>"]},{"cell_type":"markdown","metadata":{"id":"mhA3qvbfmKap"},"source":["Instructions\r\n","<ul>\r\n","<li>Initialize random tensors <code>x</code>, <code>y</code> and <code>z</code>, each having shape <code>(1000, 1000)</code>.</li>\r\n","<li>Multiply <code>x</code> with <code>y</code>, putting the result in tensor <code>q</code>.</li>\r\n","<li>Do an elementwise multiplication of tensor <code>z</code> with tensor <code>q</code>, putting the results in <code>f</code></li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"_5WBjotZmadi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615038475105,"user_tz":180,"elapsed":566,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"2264810f-1370-41f2-d092-b62ba8bdaca8"},"source":["# Initialize tensors x, y and z\r\n","x = torch.rand(1000, 1000)\r\n","y = torch.rand(1000, 1000)\r\n","z = torch.rand(1000, 1000)\r\n","\r\n","# Multiply x with y\r\n","q = x * y\r\n","\r\n","# Multiply elementwise z with q\r\n","f = q * z\r\n","\r\n","mean_f = torch.mean(f)\r\n","print(mean_f)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.1249)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"95i8syWnmf5X"},"source":["**You just built a nice computational graph containing 5'000'001 values.**"]},{"cell_type":"markdown","metadata":{"id":"mwpj7Omymk82"},"source":["### Backpropagation by auto-differentiation"]},{"cell_type":"markdown","metadata":{"id":"pfzs98I-sxz_"},"source":["### Backpropagation by hand"]},{"cell_type":"markdown","metadata":{"id":"QIM6xC00s0tv"},"source":["<div class=\"\"><p><img src=\"https://assets.datacamp.com/production/repositories/4094/datasets/b483da1f7b9a03a3669973dc6faa0e8899e399fa/der_example.jpg\" alt=\"\"></p>\r\n","<p>Given the computational graph above, we want to calculate the derivatives for the leaf nodes (x, y and z). To get you started we already calculated the results of the forward pass (in red) in addition to calculating the derivatives of f and q.</p>\r\n","<p>The rules for derivative computations have been given in the table below:</p>\r\n","<table>\r\n","<thead>\r\n","<tr>\r\n","<th>Interaction</th>\r\n","<th>Overall Change</th>\r\n","</tr>\r\n","</thead>\r\n","<tbody>\r\n","<tr>\r\n","<td>Addition</td>\r\n","<td><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"0\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"3\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-msup space=\"4\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em; margin-left: 0.053em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msup space=\"3\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>f</mi><mo>+</mo><mi>g</mi><msup><mo stretchy=\"false\">)</mo><mo>′</mo></msup><mo>=</mo><msup><mi>f</mi><mo>′</mo></msup><mo>+</mo><msup><mi>g</mi><mo>′</mo></msup></math></mjx-assistive-mml></mjx-container></td>\r\n","</tr>\r\n","<tr>\r\n","<td>Multiplication</td>\r\n","<td><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"1\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"3\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"3\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"3\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"3\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>f</mi><mo>⋅</mo><mi>g</mi><msup><mo stretchy=\"false\">)</mo><mo>′</mo></msup><mo>=</mo><mi>f</mi><mo>⋅</mo><mi>d</mi><mi>g</mi><mo>+</mo><mi>g</mi><mo>⋅</mo><mi>d</mi><mi>f</mi></math></mjx-assistive-mml></mjx-container></td>\r\n","</tr>\r\n","<tr>\r\n","<td>Powers</td>\r\n","<td><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"2\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D465 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mfrac space=\"4\"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size=\"s\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D465 TEX-I\"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D465 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D465 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-texatom size=\"s\" texclass=\"ORD\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2212\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi>n</mi></msup><msup><mo stretchy=\"false\">)</mo><mo>′</mo></msup><mo>=</mo><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><msup><mi>x</mi><mi>n</mi></msup><mo>=</mo><mi>n</mi><msup><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></td>\r\n","</tr>\r\n","<tr>\r\n","<td>Inverse</td>\r\n","<td><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"3\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D465 TEX-I\"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c2212\"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msup size=\"s\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D465 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.289em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mfrac><mn>1</mn><mi>x</mi></mfrac><msup><mo stretchy=\"false\">)</mo><mo>′</mo></msup><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><msup><mi>x</mi><mn>2</mn></msup></mfrac></math></mjx-assistive-mml></mjx-container></td>\r\n","</tr>\r\n","<tr>\r\n","<td>Division</td>\r\n","<td><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"4\" style=\"font-size: 116.7%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mo class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c2032\"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mfrac space=\"3\"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size=\"s\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2212\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msup size=\"s\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.289em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"3\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mfrac><mi>f</mi><mi>g</mi></mfrac><msup><mo stretchy=\"false\">)</mo><mo>′</mo></msup><mo>=</mo><mo stretchy=\"false\">(</mo><mi>d</mi><mi>f</mi><mo>⋅</mo><mfrac><mn>1</mn><mi>g</mi></mfrac><mo stretchy=\"false\">)</mo><mo>+</mo><mo stretchy=\"false\">(</mo><mfrac><mrow><mo>−</mo><mn>1</mn></mrow><msup><mi>g</mi><mn>2</mn></msup></mfrac><mi>d</mi><mi>g</mi><mo>⋅</mo><mi>f</mi><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container></td>\r\n","</tr>\r\n","</tbody>\r\n","</table></div>"]},{"cell_type":"markdown","metadata":{"id":"Gzlg8iPatsPf"},"source":["<pre>\r\n","Possible Answers\r\n","<b>The Derivative of x is 5, the derivative of y is 5, the derivative of z is 1.</b>\r\n","The Derivative of x is 5, the derivative of y is 5, the derivative of z is 5.\r\n","The Derivative of x is 8, the derivative of y is -3, the derivative of z is 0.\r\n","Derivatives are lame, integrals are cool.\r\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"Sit8H8Hgt4Y6"},"source":["### Backpropagation using PyTorch"]},{"cell_type":"markdown","metadata":{"id":"v1j0UGnTt6_s"},"source":["<p>Here, you are going to use automatic differentiation of PyTorch in order to compute the derivatives of <code>x</code>, <code>y</code> and <code>z</code> from the previous exercise.</p>"]},{"cell_type":"markdown","metadata":{"id":"vInuTz0Lt-_k"},"source":["Instructions\r\n","<ul>\r\n","<li>Initialize tensors <code>x</code>, <code>y</code> and <code>z</code> to values 4, -3 and 5.</li>\r\n","<li>Put the sum of tensors <code>x</code> and <code>y</code> in <code>q</code>, put the product of <code>q</code> and <code>z</code> in <code>f</code>.</li>\r\n","<li>Calculate the derivatives of the computational graph.</li>\r\n","<li>Print the gradients of the <code>x</code>, <code>y</code> and <code>z</code> tensors.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"fO3Bb-wIuxEh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615038484649,"user_tz":180,"elapsed":567,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d817fb27-eff6-4ba8-a484-9835a2821f3a"},"source":["# Initialize x, y and z to values 4, -3 and 5\r\n","x = torch.tensor(4., requires_grad=True)\r\n","y = torch.tensor(-3., requires_grad=True)\r\n","z = torch.tensor(5., requires_grad=True)\r\n","\r\n","# Set q to sum of x and y, set f to product of q with z\r\n","q = x + y\r\n","f = q * z\r\n","\r\n","# Compute the derivatives\r\n","f.backward()\r\n","\r\n","# Print the gradients\r\n","print(\"Gradient of x is: \" + str(x.grad))\r\n","print(\"Gradient of y is: \" + str(y.grad))\r\n","print(\"Gradient of z is: \" + str(z.grad))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Gradient of x is: tensor(5.)\n","Gradient of y is: tensor(5.)\n","Gradient of z is: tensor(1.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E-kuv1JJu2xk"},"source":["**No surprise here, the results are the same as when you calculated them by hand!**"]},{"cell_type":"markdown","metadata":{"id":"ktDPUNi-u5td"},"source":["### Calculating gradients in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"gHZFnQNYu74H"},"source":["<div class=\"\"><p>Remember the exercise in forward pass? Now that you know how to calculate derivatives, let's make a step forward and start calculating the gradients (derivatives of tensors) of the computational graph you built back then. We have already initialized for you three random tensors of shape <code>(1000, 1000)</code> called <code>x</code>, <code>y</code> and <code>z</code>. First, we multiply tensors <code>x</code> and <code>y</code>, then we do an elementwise multiplication of their product with tensor <code>z</code>, and then we compute its <code>mean</code>. In the end, we compute the derivatives.</p>\r\n","<p>The main difference from the previous exercise is the scale of the tensors. While before, tensors <code>x</code>, <code>y</code> and <code>z</code> had just 1 number, now they each have 1 million numbers.</p>\r\n","<p><img src=\"https://assets.datacamp.com/production/repositories/4094/datasets/ab707279d7be2835c17787a38c6e2e54f6d89409/graph_exercise.jpg\" alt=\"\"></p></div>"]},{"cell_type":"code","metadata":{"id":"vT47ptftzCod"},"source":["x = torch.rand(1000, 1000, requires_grad=True)\r\n","y = torch.rand(1000, 1000, requires_grad=True)\r\n","z = torch.rand(1000, 1000, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yH3l9LOtu9_W"},"source":["Instructions\r\n","<ul>\r\n","<li>Multiply tensors <code>x</code> and <code>y</code>, put the product in tensor <code>q</code>.</li>\r\n","<li>Do an elementwise multiplication of tensors <code>z</code> with <code>q</code>.</li>\r\n","<li>Calculate the gradients.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"yVHu9H_Vvxel"},"source":["# Multiply tensors x and y\r\n","q = torch.matmul(x, y)\r\n","\r\n","# Elementwise multiply tensors z with q\r\n","f = z * q\r\n","\r\n","mean_f = torch.mean(f)\r\n","\r\n","# Calculate the gradients\r\n","mean_f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Dfv73EFv2vW"},"source":["**In general, calculating gradients is as easy as calculating derivatives in PyTorch. Obviously, if the tensors are very large (billions of values) then the calculation might take some time.**"]},{"cell_type":"markdown","metadata":{"id":"ewCEVZ-Cv6n_"},"source":["## Introduction to Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"ZEE1HJ7yJvLb"},"source":["### Your first neural network"]},{"cell_type":"markdown","metadata":{"id":"qyIsmgSXJyEE"},"source":["<p>You are going to build a neural network in PyTorch, using the hard way. Your input will be images of size <code>(28, 28)</code>, so images containing <code>784</code> pixels. Your network will contain an <code>input_layer</code> (provided for you), a hidden layer with <code>200</code> units, and an output layer with <code>10</code> classes. The input layer has already been created for you. You are going to create the weights, and then do matrix multiplications, getting the results from the network.</p>"]},{"cell_type":"code","metadata":{"id":"G2XgzvDUzIHb"},"source":["input_layer = torch.rand(784)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HK0b2_lQJzl8"},"source":["Instructions\r\n","<ul>\r\n","<li>Initialize with random numbers two matrices of weights, called <code>weight_1</code> and <code>weight_2</code>.</li>\r\n","<li>Set the result of <code>input_layer</code> times <code>weight_1</code> to <code>hidden_1</code>. Set the result of <code>hidden_1</code> times <code>weight_2</code> to <code>output_layer</code>.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"4tuTvnECKzon","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615038549615,"user_tz":180,"elapsed":573,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"35ec7c19-a641-4e1b-9f8b-84b9fcf74d7d"},"source":["# Initialize the weights of the neural network\r\n","weight_1 = torch.rand(784, 200)\r\n","weight_2 = torch.rand(200, 10)\r\n","\r\n","# Multiply input_layer with weight_1\r\n","hidden_1 = torch.matmul(input_layer, weight_1)\r\n","\r\n","# Multiply hidden_1 with weight_2\r\n","output_layer = torch.matmul(hidden_1, weight_2)\r\n","print(output_layer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([20436.0547, 20528.6582, 20435.9531, 19162.4219, 20058.9766, 18554.9512,\n","        21562.4766, 20638.4668, 20152.7910, 19314.1621])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0OAqI3d1K4Eg"},"source":["**For the most part, neural networks are just matrix (tensor) multiplication. This is the reason why we have put so much emphasis on matrices and tensors!**"]},{"cell_type":"markdown","metadata":{"id":"RHyBpDsMK6Vo"},"source":["### Your first PyTorch neural network"]},{"cell_type":"markdown","metadata":{"id":"dwZIx2k1K9Mq"},"source":["<p>You are going to build the same neural network you built in the previous exercise, but now using the PyTorch way. As a reminder, you have 784 units in the input layer, 200 hidden units and 10 units for the output layer.</p>"]},{"cell_type":"code","metadata":{"id":"k4B_RqeRzZyk"},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4qOdjLcK-2q"},"source":["Instructions\r\n","<ul>\r\n","<li>Instantiate two linear layers calling them <code>self.fc1</code> and <code>self.fc2</code>. Determine their correct dimensions. </li>\r\n","<li>Implement the <code>.forward()</code> method, using the two layers you defined and returning <code>x</code>.</li>\r\n","</ul>"]},{"cell_type":"code","metadata":{"id":"PEU0ZN9SLUY4"},"source":["class Net(nn.Module):\r\n","    def __init__(self):\r\n","        super(Net, self).__init__()\r\n","        \r\n","        # Instantiate all 2 linear layers  \r\n","        self.fc1 = nn.Linear(784 , 200)\r\n","        self.fc2 = nn.Linear(200 , 10)\r\n","\r\n","    def forward(self, x):\r\n","      \r\n","        # Use the instantiated layers and return x\r\n","        x = self.fc1(x)\r\n","        x = self.fc2(x)\r\n","        return  x"],"execution_count":null,"outputs":[]}]}