{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-creating-a-simple-first-model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSM183p6jHLjEn55MH5yF3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f5FTJNcbC9gb"},"source":["# Creating a simple first model\n",">  In this chapter, you'll build a first-pass model. You'll use numeric data only to train the model. Spoiler alert - throwing out all of the text data is bad for performance! But you'll learn how to format your predictions. Then, you'll be introduced to natural language processing (NLP) in order to start working with the large amounts of text in the data.\n","\n","- toc: true \n","- badges: true\n","- comments: true\n","- author: Lucas Nunes\n","- categories: [Datacamp]\n","- image: images/datacamp/___"]},{"cell_type":"markdown","metadata":{"id":"f9J9naPqLbDt"},"source":["> Note: This is a summary of the course's chapter 2 exercises \"Case Study: School Budgeting with Machine Learning in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"]},{"cell_type":"code","metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1616699747925,"user_tz":180,"elapsed":2066,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.rcParams['figure.figsize'] = (8, 8)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvrKEMkeEF2g"},"source":["## It's time to build a model"]},{"cell_type":"markdown","metadata":{"id":"Ho1eZTss8AvH"},"source":["### Setting up a train-test split in scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"-S2Qs0HoIrmz"},"source":["<div class=\"\"><p>Alright, you've been patient and awesome. It's finally time to start training models! </p>\n","<p>The first step is to split the data into a training set and a test set. Some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets. We provide a function that will make sure at least <code>min_count</code> examples of each label appear in each split: <code>multilabel_train_test_split</code>.</p>\n","<p>Feel free to check out the full code for <code>multilabel_train_test_split</code> <a href=\"https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n","<p>You'll start with a simple model that uses <strong>just the numeric columns</strong> of your DataFrame when calling <code>multilabel_train_test_split</code>. The data has been read into a DataFrame <code>df</code> and a list consisting of just the numeric columns is available as <code>NUMERIC_COLUMNS</code>.</p></div>"]},{"cell_type":"code","metadata":{"id":"X_-AzbrDzMhJ","executionInfo":{"status":"ok","timestamp":1616699826219,"user_tz":180,"elapsed":756,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["NUMERIC_COLUMNS = ['FTE', 'Total']\n","LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"boHxO7pc0COT","executionInfo":{"status":"ok","timestamp":1616699772529,"user_tz":180,"elapsed":1017,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["df = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/3-skill-tracks/case-study-school-budgeting-with-machine-learning-in-python/data/TrainingData.csv', index_col=0)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqNycWMU0kYK","executionInfo":{"status":"ok","timestamp":1616700016715,"user_tz":180,"elapsed":709,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["#https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py\n","\n","from warnings import warn\n","\n","import numpy as np\n","import pandas as pd\n","\n","def multilabel_sample(y, size=1000, min_count=5, seed=None):\n","    \"\"\" Takes a matrix of binary labels `y` and returns\n","        the indices for a sample of size `size` if\n","        `size` > 1 or `size` * len(y) if size =< 1.\n","        The sample is guaranteed to have > `min_count` of\n","        each label.\n","    \"\"\"\n","    try:\n","        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n","            raise ValueError()\n","    except (TypeError, ValueError):\n","        raise ValueError('multilabel_sample only works with binary indicator matrices')\n","\n","    if (y.sum(axis=0) < min_count).any():\n","        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n","\n","    if size <= 1:\n","        size = np.floor(y.shape[0] * size)\n","\n","    if y.shape[1] * min_count > size:\n","        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n","        warn(msg.format(y.shape[1] * min_count, size))\n","        size = y.shape[1] * min_count\n","\n","    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n","\n","    if isinstance(y, pd.DataFrame):\n","        choices = y.index\n","        y = y.values\n","    else:\n","        choices = np.arange(y.shape[0])\n","\n","    sample_idxs = np.array([], dtype=choices.dtype)\n","\n","    # first, guarantee > min_count of each label\n","    for j in range(y.shape[1]):\n","        label_choices = choices[y[:, j] == 1]\n","        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n","        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n","\n","    sample_idxs = np.unique(sample_idxs)\n","\n","    # now that we have at least min_count of each, we can just random sample\n","    sample_count = int(size - sample_idxs.shape[0])\n","\n","    # get sample_count indices from remaining choices\n","    remaining_choices = np.setdiff1d(choices, sample_idxs)\n","    remaining_sampled = rng.choice(remaining_choices,\n","                                   size=sample_count,\n","                                   replace=False)\n","\n","    return np.concatenate([sample_idxs, remaining_sampled])\n","\n","\n","def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n","    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n","        classes in the binary matrix `labels` are represented at\n","        least `min_count` times.\n","    \"\"\"\n","    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n","    return df.loc[idxs]\n","\n","\n","def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n","    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n","        returns (X_train, X_test, Y_train, Y_test) where all\n","        classes in Y are represented at least `min_count` times.\n","    \"\"\"\n","    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n","\n","    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n","    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n","\n","    test_set_mask = index.isin(test_set_idxs)\n","    train_set_mask = ~test_set_mask\n","\n","    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0wj6nnSItyf"},"source":["Instructions\n","<ul>\n","<li>Create a new DataFrame named <code>numeric_data_only</code> by applying the <code>.fillna(-1000)</code> method to the numeric columns (available in the list <code>NUMERIC_COLUMNS</code>) of <code>df</code>.</li>\n","<li>Convert the labels (available in the list <code>LABELS</code>) to dummy variables. Save the result as <code>label_dummies</code>.</li>\n","<li>In the call to <code>multilabel_train_test_split()</code>, set the <code>size</code> of your test set to be <code>0.2</code>. Use a <code>seed</code> of <code>123</code>.</li>\n","<li>Fill in the <code>.info()</code> method calls for <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"w48_1NTN8ASP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616700021328,"user_tz":180,"elapsed":712,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"a55b1935-5c60-4da9-dd2c-25d8bcdbc382"},"source":["# Create the new DataFrame: numeric_data_only\n","numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n","\n","# Get labels and convert to dummy variables: label_dummies\n","label_dummies = pd.get_dummies(df[LABELS])\n","\n","# Create training and test sets\n","X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n","                                                               label_dummies,\n","                                                               size=0.2, \n","                                                               seed=123)\n","# Print the info\n","print(\"X_train info:\")\n","print(X_train.info())\n","print(\"\\nX_test info:\")  \n","print(X_test.info())\n","print(\"\\ny_train info:\")  \n","print(y_train.info())\n","print(\"\\ny_test info:\")  \n","print(y_test.info()) "],"execution_count":18,"outputs":[{"output_type":"stream","text":["X_train info:\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 1040 entries, 198 to 101861\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   FTE     1040 non-null   float64\n"," 1   Total   1040 non-null   float64\n","dtypes: float64(2)\n","memory usage: 24.4 KB\n","None\n","\n","X_test info:\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 520 entries, 209 to 448628\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   FTE     520 non-null    float64\n"," 1   Total   520 non-null    float64\n","dtypes: float64(2)\n","memory usage: 12.2 KB\n","None\n","\n","y_train info:\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 1040 entries, 198 to 101861\n","Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n","dtypes: uint8(104)\n","memory usage: 113.8 KB\n","None\n","\n","y_test info:\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 520 entries, 209 to 448628\n","Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n","dtypes: uint8(104)\n","memory usage: 56.9 KB\n","None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"iBLxrXTuKWoT"},"source":["### Training a model"]},{"cell_type":"markdown","metadata":{"id":"wLTpCbUAKaNL"},"source":["<div class=\"\"><p>With split data in hand, you're only a few lines away from training a model.</p>\n","<p>In this exercise, you will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the <code>NUMERIC_COLUMNS</code> of your feature data.</p>\n","<p>Then you'll test and print the accuracy with the <code>.score()</code> method to see the results of training. </p>\n","<p><strong>Before you train!</strong> Remember, we're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that you're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment.</p>\n","<p>All data necessary to call <code>multilabel_train_test_split()</code> has been loaded into the workspace.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"stTy_HF2KcK8"},"source":["Instructions\n","<ul>\n","<li>Import <code>LogisticRegression</code> from <code>sklearn.linear_model</code> and <code>OneVsRestClassifier</code> from <code>sklearn.multiclass</code>.</li>\n","<li>Instantiate the classifier <code>clf</code> by placing <code>LogisticRegression()</code> inside <code>OneVsRestClassifier()</code>.</li>\n","<li>Fit the classifier to the training data <code>X_train</code> and <code>y_train</code>.</li>\n","<li>Compute and print the accuracy of the classifier using its <code>.score()</code> method, which accepts two arguments: <code>X_test</code> and <code>y_test</code>.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"vGXBNtmPK0dE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616700145859,"user_tz":180,"elapsed":2749,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d176dcb8-2ddc-4e21-aafb-4bce06fc03d0"},"source":["# Import classifiers\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","# Create the DataFrame: numeric_data_only\n","numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n","\n","# Get labels and convert to dummy variables: label_dummies\n","label_dummies = pd.get_dummies(df[LABELS])\n","\n","# Create training and test sets\n","X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n","                                                               label_dummies,\n","                                                               size=0.2, \n","                                                               seed=123)\n","\n","# Instantiate the classifier: clf\n","clf = OneVsRestClassifier(LogisticRegression())\n","\n","# Fit the classifier to the training data\n","clf.fit(X_train, y_train)\n","\n","# Print the accuracy\n","print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8lcdoqofLBfH"},"source":["**The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss.**"]},{"cell_type":"markdown","metadata":{"id":"xIJ9_bG4LDav"},"source":["## Making predictions"]},{"cell_type":"markdown","metadata":{"id":"nx5uVNdz59dt"},"source":["### Use your model to predict values on holdout data"]},{"cell_type":"markdown","metadata":{"id":"WiGe652n6BIl"},"source":["<div class=\"\"><p>You're ready to make some predictions! Remember, the train-test-split you've carried out so far is for model development. The original competition provides an additional test set, for which you'll never actually <em>see</em> the correct labels. This is called the \"holdout data.\"</p>\n","<p>The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, you can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.</p>\n","<p>Remember that the original goal is to predict the <strong>probability of each label</strong>. In this exercise you'll do just that by using the <code>.predict_proba()</code> method on your trained model.</p>\n","<p>First, however, you'll need to load the holdout data, which is available in the workspace as the file <code>HoldoutData.csv</code>.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"mn3ZOwZy6DKw"},"source":["Instructions\n","<ul>\n","<li>Read <code>HoldoutData.csv</code> into a DataFrame called <code>holdout</code>. Specify the keyword argument <code>index_col=0</code> in your call to <code>read_csv()</code>.</li>\n","<li>Generate predictions using <code>.predict_proba()</code> on the numeric columns (available in the <code>NUMERIC_COLUMNS</code> list) of <code>holdout</code>. Make sure to fill in missing values with <code>-1000</code>!</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"AAJijtNw6wPE","executionInfo":{"status":"ok","timestamp":1616700246829,"user_tz":180,"elapsed":2623,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Instantiate the classifier: clf\n","clf = OneVsRestClassifier(LogisticRegression())\n","\n","# Fit it to the training data\n","clf.fit(X_train, y_train)\n","\n","# Load the holdout data: holdout\n","holdout = pd.read_csv(\"https://github.com/lnunesAI/Datacamp/raw/main/3-skill-tracks/case-study-school-budgeting-with-machine-learning-in-python/data/HoldoutData.csv\", index_col=0)\n","holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n","\n","# Generate predictions: predictions\n","predictions = clf.predict_proba(holdout)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gwlJeih-6yxl"},"source":["### Writing out your results to a csv for submission"]},{"cell_type":"markdown","metadata":{"id":"1M0uV6iw625G"},"source":["<div class=\"\"><p>At last, you're ready to submit some predictions for scoring. In this exercise, you'll write your predictions to a <code>.csv</code> using the <code>.to_csv()</code> method on a pandas DataFrame. Then you'll evaluate your performance according to the LogLoss metric discussed earlier!</p>\n","<p>You'll need to make sure your submission obeys the <a href=\"https://www.drivendata.org/competitions/4/page/15/#sub_values\" target=\"_blank\" rel=\"noopener noreferrer\">correct format</a>.</p>\n","<p>To do this, you'll use your <code>predictions</code> values to create a new DataFrame, <code>prediction_df</code>. </p>\n","<p><strong>Interpreting LogLoss &amp; Beating the Benchmark:</strong> </p>\n","<p>When interpreting your log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this <em>very basic</em> model performs, compare your score to the <strong>DrivenData benchmark model performance: 2.0455</strong>, which merely submitted uniform probabilities for each class.</p>\n","<p>Remember, the lower the log loss the better. Is your model's log loss lower than 2.0455?</p></div>"]},{"cell_type":"code","metadata":{"id":"Py_iCus12Tm5","executionInfo":{"status":"ok","timestamp":1616700866202,"user_tz":180,"elapsed":704,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["PATH_TO_PREDICTIONS = 'predictions.csv'\n","PATH_TO_HOLDOUT_LABELS = 'https://github.com/lnunesAI/Datacamp/raw/main/3-skill-tracks/case-study-school-budgeting-with-machine-learning-in-python/data/TestSetLabelsSample.csv'\n","BOX_PLOTS_COLUMN_INDICES = [range(0, 37), range(37, 48), range(48, 51), range(51, 76), range(76, 79), range(79, 82), range(82, 87), range(87, 96), range(96, 104)]"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vd3wTsfP22ab","executionInfo":{"status":"ok","timestamp":1616700870057,"user_tz":180,"elapsed":1150,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["test = pd.read_csv(PATH_TO_HOLDOUT_LABELS, index_col=0)\n","test2 = pd.read_csv(PATH_TO_PREDICTIONS, index_col=0)"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBb3Yxsp2MD_","executionInfo":{"status":"ok","timestamp":1616700871471,"user_tz":180,"elapsed":828,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["def score_submission(pred_path=PATH_TO_PREDICTIONS, holdout_path=PATH_TO_HOLDOUT_LABELS):\n","    # this happens on the backend to get the score\n","    holdout_labels = pd.get_dummies(\n","                        pd.read_csv(holdout_path, index_col=0)\n","                          .apply(lambda x: x.astype('category'), axis=0)\n","                      )\n","\n","    preds = pd.read_csv(pred_path, index_col=0)\n","    \n","    # make sure that format is correct\n","    assert (preds.columns == holdout_labels.columns).all()\n","    assert (preds.index == holdout_labels.index).all()\n","\n","    return _multi_multi_log_loss(preds.values, holdout_labels.values)"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"iraJCLR32kjm","executionInfo":{"status":"ok","timestamp":1616700873482,"user_tz":180,"elapsed":871,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["def _multi_multi_log_loss(predicted,\n","                          actual,\n","                          class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n","                          eps=1e-15):\n","    \"\"\" Multi class version of Logarithmic Loss metric as implemented on\n","        DrivenData.org\n","    \"\"\"\n","    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n","\n","    # calculate log loss for each set of columns that belong to a class:\n","    for k, this_class_indices in enumerate(class_column_indices):\n","        # get just the columns for this class\n","        preds_k = predicted[:, this_class_indices].astype(np.float64)\n","\n","        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n","        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n","\n","        actual_k = actual[:, this_class_indices]\n","\n","        # shrink predictions so\n","        y_hats = np.clip(preds_k, eps, 1 - eps)\n","        sum_logs = np.sum(actual_k * np.log(y_hats))\n","        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n","\n","    return np.average(class_scores)"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggEFVBpc7RVN"},"source":["Instructions\n","<ul>\n","<li>Create the <code>prediction_df</code> DataFrame by specifying the following arguments to the provided parameters <code>pd.DataFrame()</code>:<ul>\n","<li><code>pd.get_dummies(df[LABELS]).columns</code>.</li>\n","<li><code>holdout.index</code>.</li>\n","<li><code>predictions</code>.</li></ul></li>\n","<li>Save <code>prediction_df</code> to a csv file called <code>'predictions.csv'</code> using the <code>.to_csv()</code> method.</li>\n","<li>Submit the predictions for scoring by using the <code>score_submission()</code> function with <code>pred_path</code> set to <code>'predictions.csv'</code>.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"wYBhi6rx8kaP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616700877710,"user_tz":180,"elapsed":1115,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"e69dc8b3-5cc4-4bda-e334-a7a43bb48966"},"source":["# Generate predictions: predictions\n","predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n","\n","# Format predictions in DataFrame: prediction_df\n","prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n","                             index=holdout.index,\n","                             data=predictions)\n","\n","\n","# Save prediction_df to csv\n","prediction_df.to_csv(\"predictions.csv\")\n","\n","# Submit the predictions for scoring: score\n","score = score_submission(\"predictions.csv\")\n","\n","# Print score\n","print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Your model, trained with numeric data only, yields logloss score: 1.9922002790817794\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WckToDOe8qdQ"},"source":["**Even though your basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. You've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up your game and incorporate the text data.**"]},{"cell_type":"markdown","metadata":{"id":"iwb_1H_p8ssQ"},"source":["### A very brief introduction to NLP"]},{"cell_type":"markdown","metadata":{"id":"wao01YLf_xJs"},"source":["### Tokenizing text"]},{"cell_type":"markdown","metadata":{"id":"hqZEEdll_070"},"source":["<div class=\"\"><p>As we talked about in the video, <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\" target=\"_blank\" rel=\"noopener noreferrer\">tokenization</a> is the process of chopping up a character sequence into pieces called <em>tokens</em>. </p>\n","<p>How do we determine what constitutes a token? Often, tokens are separated by whitespace. But we can specify other delimiters as well. For example, if we decided to tokenize on punctuation, then any punctuation mark would be treated like a whitespace. How we tokenize text in our DataFrame can affect the statistics we use in our model.</p>\n","<p>A particular cell in our budget DataFrame may have the string content <code>Title I - Disadvantaged Children/Targeted Assistance</code>. The number of n-grams generated by this text data is sensitive to whether or not we tokenize on punctuation, as you'll show in the following exercise.</p>\n","<p>How many tokens (1-grams) are in the string</p>\n","<pre><code>Title I - Disadvantaged Children/Targeted Assistance\n","</code></pre>\n","<p>if we tokenize on whitespace and punctuation?</p></div>"]},{"cell_type":"markdown","metadata":{"id":"fYOrj72oAQuV"},"source":["<pre>\n","Possible Answers\n","4.\n","<b>6.</b>\n","8.\n","13.\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"uzaRj9jUAYu4"},"source":["**Yes! Tokenizing on whitespace and punctuation means that Children/Targeted becomes two tokens and - is dropped altogether.**"]},{"cell_type":"markdown","metadata":{"id":"IbvtT893AgVB"},"source":["### Testing your NLP credentials with n-grams"]},{"cell_type":"markdown","metadata":{"id":"ja7rFctQAjja"},"source":["<div class=\"\"><p>You're well on your way to NLP superiority. Let's test your mastery of n-grams!</p>\n","<p>In the workspace, we have the loaded a python list, <code>one_grams</code>, which contains all 1-grams of the string <code>petro-vend fuel and fluids</code>, tokenized on punctuation. Specifically,</p>\n","<pre><code>one_grams = ['petro', 'vend', 'fuel', 'and', 'fluids']\n","</code></pre>\n","<p>In this exercise, your job is to determine the <strong>sum</strong> of the sizes of 1-grams, 2-grams and 3-grams generated by the string <code>petro-vend fuel and fluids</code>, tokenized on punctuation.</p>\n","<p><em>Recall that the n-gram of a sequence consists of all ordered subsequences of length n.</em></p></div>"]},{"cell_type":"code","metadata":{"id":"-mWAj6uUBTVR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616700883952,"user_tz":180,"elapsed":951,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"d443528e-212d-4581-99c6-ce8875451440"},"source":["one_grams = ['petro', 'vend', 'fuel', 'and', 'fluids']\n","len(one_grams)+len(one_grams)-1+len(one_grams)-2"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"YbO8vFvpAnJj"},"source":["<pre>\n","Possible Answers\n","3.\n","4.\n","7.\n","<b>12.</b>\n","15.\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"jIpfRXm2BhYF"},"source":["## Representing text numerically"]},{"cell_type":"markdown","metadata":{"id":"ySHnKb5wFfKL"},"source":["### Creating a bag-of-words in scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"3Yr57SBoFiP5"},"source":["<div class=\"\"><p>In this exercise, you'll study the effects of tokenizing in different ways by comparing the bag-of-words representations resulting from different token patterns.</p>\n","<p>You will focus on one feature only, the <code>Position_Extra</code> column, which describes any additional information not captured by the <code>Position_Type</code> label.</p>\n","<p>For example, in the Shell you can check out the budget item in row 8960 of the data using <code>df.loc[8960]</code>. Looking at the output reveals that this <code>Object_Description</code> is overtime pay. For who? The Position Type is merely \"other\", but the Position Extra elaborates: \"BUS DRIVER\". Explore the column further to see more instances. It has a lot of NaN values.</p>\n","<p>Your task is to turn the raw text in this column into a bag-of-words representation by creating tokens that contain <em>only</em> alphanumeric characters.</p>\n","<p>For comparison purposes, the first 15 tokens of <code>vec_basic</code>, which splits <code>df.Position_Extra</code> into tokens when it encounters only <em>whitespace</em> characters, have been printed along with the length of the representation.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"C0QzAJXLFkfC"},"source":["Instructions\n","<ul>\n","<li>Import <code>CountVectorizer</code> from <code>sklearn.feature_extraction.text</code>.</li>\n","<li>Fill missing values in <code>df.Position_Extra</code> using <code>.fillna('')</code> to replace NaNs with empty strings. Specify the additional keyword argument <code>inplace=True</code> so that you don't have to assign the result back to <code>df</code>.</li>\n","<li>Instantiate the <code>CountVectorizer</code> as <code>vec_alphanumeric</code> by specifying the <code>token_pattern</code> to be <code>TOKENS_ALPHANUMERIC</code>.</li>\n","<li>Fit <code>vec_alphanumeric</code> to <code>df.Position_Extra</code>.</li>\n","<li>Hit 'Submit Answer' to see the <code>len</code> of the fitted representation as well as the first 15 elements, and compare to <code>vec_basic</code>.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"2yeBB_d5kwtd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616700890168,"user_tz":180,"elapsed":695,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"7caf7c63-a053-4918-a8f2-4b860d89e98a"},"source":["# Import CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create the token pattern: TOKENS_ALPHANUMERIC\n","TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n","\n","# Fill missing values in df.Position_Extra\n","df.Position_Extra.fillna('', inplace=True)\n","\n","# Instantiate the CountVectorizer: vec_alphanumeric\n","vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n","\n","# Fit to the data\n","vec_alphanumeric.fit(df.Position_Extra)\n","\n","# Print the number of tokens and first 15 tokens\n","msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n","print(msg.format(len(vec_alphanumeric.get_feature_names())))\n","print(vec_alphanumeric.get_feature_names()[:15])"],"execution_count":52,"outputs":[{"output_type":"stream","text":["There are 123 tokens in Position_Extra if we split on non-alpha numeric\n","['1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment', 'assistant', 'asst', 'athletic']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j0V9F13Ak3EE"},"source":["**Treating only alpha-numeric characters as tokens gives you a smaller number of more meaningful tokens.**"]},{"cell_type":"markdown","metadata":{"id":"IM6C4XFck663"},"source":["### Combining text columns for tokenization"]},{"cell_type":"markdown","metadata":{"id":"p9JD2gLJk-Yo"},"source":["<div class=\"\"><p>In order to get a bag-of-words representation for all of the text data in our DataFrame, you must first convert the text data in each row of the DataFrame into a single string.</p>\n","<p>In the previous exercise, this wasn't necessary because you only looked at one column of data, so each row was already just a single string. <code>CountVectorizer</code> expects each row to just be a single string, so in order to use all of the text columns, you'll need a method to turn a list of strings into a single string.</p>\n","<p>In this exercise, you'll complete the function definition <code>combine_text_columns()</code>. When completed, this function will convert all training text data in your DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the <code>.fit_transform()</code> method.</p>\n","<p>Note that the function uses <code>NUMERIC_COLUMNS</code> and <code>LABELS</code> to determine which columns to drop. These lists have been loaded into the workspace.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"zjLvAbwxlBaY"},"source":["Instructions\n","<ul>\n","<li>Use the <code>.drop()</code> method on <code>data_frame</code> with <code>to_drop</code> and <code>axis=</code> as arguments to drop the non-text data. Save the result as <code>text_data</code>.</li>\n","<li>Fill in missing values (inplace) in <code>text_data</code> with blanks (<code>\"\"</code>), using the <code>.fillna()</code> method.</li>\n","<li>Complete the <code>.apply()</code> method by writing a lambda function that uses the <code>.join()</code> method to join all the items in a row with a space in between.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"4pbhXRqUlrHt","executionInfo":{"status":"ok","timestamp":1616701050719,"user_tz":180,"elapsed":770,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Define combine_text_columns()\n","def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n","    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n","    \n","    # Drop non-text columns that are in the df\n","    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n","    text_data = data_frame.drop(to_drop, axis=1)\n","    \n","    # Replace nans with blanks\n","    text_data.fillna('', inplace=True)\n","    \n","    # Join all text items in a row that have a space in between\n","    return text_data.apply(lambda x: \" \".join(x), axis=1)"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szE2DNQPltjD"},"source":["### What's in a token?"]},{"cell_type":"markdown","metadata":{"id":"WCVhBOUglv-m"},"source":["<div class=\"\"><p>Now you will use <code>combine_text_columns</code> to convert all training text data in your DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the <code>.fit_transform()</code> method.</p>\n","<p>You'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"8CL5YNkNlxpX"},"source":["Intructions\n","<ul>\n","<li>Import <code>CountVectorizer</code> from <code>sklearn.feature_extraction.text</code>.</li>\n","<li>Instantiate <code>vec_basic</code> and <code>vec_alphanumeric</code> using, respectively, the <code>TOKENS_BASIC</code> and <code>TOKENS_ALPHANUMERIC</code> patterns.</li>\n","<li>Create the text vector by using the <code>combine_text_columns()</code> function on <code>df</code>.</li>\n","<li>Using the <code>.fit_transform()</code> method with <code>text_vector</code>, fit and transform first <code>vec_basic</code> and then <code>vec_alphanumeric</code>. Print the number of tokens they contain.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"ycTpfhWNm7Mh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616701066560,"user_tz":180,"elapsed":699,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"206789ed-94ad-4baa-8386-74673b9f5299"},"source":["# Import the CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create the basic token pattern\n","TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n","\n","# Create the alphanumeric token pattern\n","TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n","\n","# Instantiate basic CountVectorizer: vec_basic\n","vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n","\n","# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n","vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n","\n","# Create the text vector\n","text_vector = combine_text_columns(df)\n","\n","# Fit and transform vec_basic\n","vec_basic.fit_transform(text_vector)\n","\n","# Print number of tokens of vec_basic\n","print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n","\n","# Fit and transform vec_alphanumeric\n","vec_alphanumeric.fit_transform(text_vector)\n","\n","# Print number of tokens of vec_alphanumeric\n","print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"],"execution_count":57,"outputs":[{"output_type":"stream","text":["There are 1404 tokens in the dataset\n","There are 1117 alpha-numeric tokens in the dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XQdh6BNynFCW"},"source":["**Notice that tokenizing on alpha-numeric tokens reduced the number of tokens, just as in the last exercise. We'll keep this in mind when building a better model with the Pipeline object next.**"]}]}