{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-learning-from-the-experts.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPWCRRNn3+WPY0K0I1iqSgP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f5FTJNcbC9gb"},"source":["# Learning from the experts\n",">  In this chapter, you will learn the tricks used by the competition winner, and implement them yourself using scikit-learn. Enjoy!\n","\n","- toc: true \n","- badges: true\n","- comments: true\n","- author: Lucas Nunes\n","- categories: [Datacamp]\n","- image: images/datacamp/___"]},{"cell_type":"markdown","metadata":{"id":"f9J9naPqLbDt"},"source":["> Note: This is a summary of the course's chapter 4 exercises \"Case Study: School Budgeting with Machine Learning in Python\" at datacamp. <br>[Github repo](https://github.com/lnunesAI/Datacamp/) / [Course link](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)"]},{"cell_type":"code","metadata":{"id":"7SbXqsjxFOUG","executionInfo":{"status":"ok","timestamp":1616768901441,"user_tz":180,"elapsed":509,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvrKEMkeEF2g"},"source":["## Learning from the expert: processing"]},{"cell_type":"markdown","metadata":{"id":"Ho1eZTss8AvH"},"source":["### How many tokens?"]},{"cell_type":"markdown","metadata":{"id":"3NlNKG430hwc"},"source":["<div class=\"\"><p>Recall from previous chapters that how you tokenize text affects the n-gram statistics used in your model.</p>\n","<p>Going forward, you'll use alpha-numeric sequences, and <em>only</em> alpha-numeric sequences, as tokens. Alpha-numeric tokens contain only letters a-z and numbers 0-9 (no other characters). In other words, you'll tokenize on punctuation to generate n-gram statistics.</p>\n","<p>In this exercise, you'll make sure you remember how to tokenize on punctuation.</p>\n","<p>Assuming we tokenize on punctuation, accepting only alpha-numeric sequences as tokens, how many tokens are in the following string from the main dataset?</p>\n","<pre><code>'PLANNING,RES,DEV,&amp; EVAL      '\n","</code></pre>\n","<p>If you want, we've loaded this string into the workspace as <code>SAMPLE_STRING</code>, but you may not need it to answer the question.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"fxtWaD4W0-wr"},"source":["<pre>\n","Possible Answers\n","4, because RES and DEV are not tokens\n","<b>4, because , and & are not tokens</b>\n","7, because there are 4 different words, some commas, an & symbol, and whitespace\n","7, because there are 7 whitespaces\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"D11ahDGK06c6"},"source":["**Commas, \"&\", and whitespace are not alpha-numeric tokens. Keep it up!**"]},{"cell_type":"markdown","metadata":{"id":"XtgPwG6W1HyO"},"source":["### Deciding what's a word"]},{"cell_type":"markdown","metadata":{"id":"y9IRsOwm1KIf"},"source":["<div class=\"\"><p>Before you build up to the winning pipeline, it will be useful to look a little deeper into how the text features will be processed.</p>\n","<p>In this exercise, you will use <code>CountVectorizer</code> on the training data <code>X_train</code> (preloaded into the workspace) to see the effect of tokenization on punctuation.</p>\n","<p>Remember, since <code>CountVectorizer</code> expects a vector, you'll need to use the preloaded function, <code>combine_text_columns</code> before fitting to the training data.</p></div>"]},{"cell_type":"code","metadata":{"id":"Firi5AsE7pWH","executionInfo":{"status":"ok","timestamp":1616768924372,"user_tz":180,"elapsed":1021,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('https://github.com/lnunesAI/Datacamp/raw/main/3-skill-tracks/case-study-school-budgeting-with-machine-learning-in-python/data/TrainingData.csv', index_col=0)\n","LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n","NUMERIC_COLUMNS = ['FTE', 'Total']\n","\n","# Get the dummy encoding of the labels\n","dummy_labels = pd.get_dummies(df[LABELS])\n","# Get the columns that are features in the original df\n","NON_LABELS = [c for c in df.columns if c not in LABELS]\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[NON_LABELS], \n","                                                    dummy_labels, \n","                                                    test_size=0.2, \n","                                                    random_state=123)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DR2136x7QnI","executionInfo":{"status":"ok","timestamp":1616768779310,"user_tz":180,"elapsed":494,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n","    \"\"\" Takes the dataset as read in, drops the non-feature, non-text columns and\n","        then combines all of the text columns into a single vector that has all of\n","        the text for a row.\n","        \n","        :param data_frame: The data as read in with read_csv (no preprocessing necessary)\n","        :param to_drop (optional): Removes the numeric and label columns by default.\n","    \"\"\"\n","    # drop non-text columns that are in the df\n","    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n","    text_data = data_frame.drop(to_drop, axis=1)\n","    \n","    # replace nans with blanks\n","    text_data.fillna(\"\", inplace=True)\n","    \n","    # joins all of the text items in a row (axis=1)\n","    # with a space in between\n","    return text_data.apply(lambda x: \" \".join(x), axis=1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dv196Z5c1Lrv"},"source":["Instructions\n","<ul>\n","<li>Create <code>text_vector</code> by preprocessing <code>X_train</code> using <code>combine_text_columns</code>. This is important, or else you won't get any tokens!</li>\n","<li>Instantiate <code>CountVectorizer</code> as <code>text_features</code>. Specify the keyword argument <code>token_pattern=TOKENS_ALPHANUMERIC</code>.</li>\n","<li>Fit <code>text_features</code> to the <code>text_vector</code>.</li>\n","<li>Hit 'Submit Answer' to print the first 10 tokens.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"YNRwrhgS12EE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616769189862,"user_tz":180,"elapsed":542,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"c58b4380-4b6b-4020-e0b4-c2ad9e6fa43f"},"source":["# Import the CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create the text vector\n","text_vector = combine_text_columns(X_train)\n","\n","# Create the token pattern: TOKENS_ALPHANUMERIC\n","TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n","\n","# Instantiate the CountVectorizer: text_features\n","text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n","\n","# Fit text_features to the text vector\n","text_features.fit(text_vector)\n","\n","# Print the first 10 tokens\n","print(text_features.get_feature_names()[:10])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["['00a', '12', '2nd', '3rd', '4th', '5th', '70', '70h', '8', 'a']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vnruDC6q2GeQ"},"source":["### N-gram range in scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"RpfEhsHA2JE6"},"source":["<div class=\"\"><p>In this exercise you'll insert a <code>CountVectorizer</code> instance into your pipeline for the main dataset, and compute multiple n-gram features to be used in the model.</p>\n","<p>In order to look for ngram relationships at multiple scales, you will use the <code>ngram_range</code> parameter as Peter discussed in the video. </p>\n","<p><strong>Special functions:</strong> You'll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the <code>dim_red</code> step following the <code>vectorizer</code> step , and the <code>scale</code> step preceeding the <code>clf</code> (classification) step.</p>\n","<p>These have been added in order to account for the fact that you're using a reduced-size sample of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction\" target=\"_blank\" rel=\"noopener noreferrer\">dimensionality reduction</a> technique, which is what the <code>dim_red</code> step does, and we have to <a href=\"https://en.wikipedia.org/wiki/Feature_scaling\" target=\"_blank\" rel=\"noopener noreferrer\">scale the features</a> to lie between -1 and 1, which is what the <code>scale</code> step does.</p>\n","<p>The <code>dim_red</code> step uses a scikit-learn function called <code>SelectKBest()</code>, applying something called the <a href=\"https://en.wikipedia.org/wiki/Chi-squared_test\" target=\"_blank\" rel=\"noopener noreferrer\">chi-squared test</a> to select the K \"best\" features. The <code>scale</code> step uses a scikit-learn function called <code>MaxAbsScaler()</code> in order to squash the relevant features into the interval -1 to 1.</p>\n","<p>You won't need to do anything extra with these functions here, just complete the vectorizing pipeline steps below. However, notice how easy it was to add more processing steps to our pipeline!</p></div>"]},{"cell_type":"markdown","metadata":{"id":"0uBIgDV12LnJ"},"source":["Instructions\n","<ul>\n","<li>Import <code>CountVectorizer</code> from <code>sklearn.feature_extraction.text</code>. </li>\n","<li>Add a <code>CountVectorizer</code> step to the pipeline with the name <code>'vectorizer'</code>.<ul>\n","<li>Set the token pattern to be <code>TOKENS_ALPHANUMERIC</code>.</li>\n","<li>Set the <code>ngram_range</code> to be <code>(1, 2)</code>.</li></ul></li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"lbVjCgs82lVP","executionInfo":{"status":"ok","timestamp":1616771174385,"user_tz":180,"elapsed":590,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Import pipeline\n","from sklearn.pipeline import Pipeline\n","\n","# Import classifiers\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","# Import CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Import other preprocessing modules\n","from sklearn.impute import SimpleImputer\n","#from sklearn.preprocessing import Imputer\n","from sklearn.feature_selection import chi2, SelectKBest\n","\n","# Select 300 best features\n","chi_k = 300\n","\n","# Import functional utilities\n","from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n","from sklearn.pipeline import FeatureUnion\n","\n","# Perform preprocessing\n","get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n","get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n","\n","# Create the token pattern: TOKENS_ALPHANUMERIC\n","TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n","\n","# Instantiate pipeline: pl\n","pl = Pipeline([\n","        ('union', FeatureUnion(\n","            transformer_list = [\n","                ('numeric_features', Pipeline([\n","                    ('selector', get_numeric_data),\n","                    ('imputer', SimpleImputer())\n","                ])),\n","                ('text_features', Pipeline([\n","                    ('selector', get_text_data),\n","                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n","                                                   ngram_range=(1, 2))),\n","                    ('dim_red', SelectKBest(chi2, chi_k))\n","                ]))\n","             ]\n","        )),\n","        ('scale', MaxAbsScaler()),\n","        ('clf', OneVsRestClassifier(LogisticRegression()))\n","    ])"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gDpHSmca2os6"},"source":["**Log loss score: 1.2681. Great work! You'll now add some additional tricks to make the pipeline even better.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnO8B9EK-aC9","executionInfo":{"status":"ok","timestamp":1616771177844,"user_tz":180,"elapsed":4032,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"1995c92b-6f5e-4470-e8ff-5b8946b689b9"},"source":["%%time\n","pl.fit(X_train, y_train)\n","accuracy = pl.score(X_test, y_test)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["CPU times: user 3.03 s, sys: 125 ms, total: 3.15 s\n","Wall time: 3.21 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6q6fK8xUDB0e","executionInfo":{"status":"ok","timestamp":1616771177845,"user_tz":180,"elapsed":4023,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"119bd5bf-1691-4f76-eff4-534f3b58175a"},"source":["accuracy"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1987179487179487"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"KPVnjE0q3P8U"},"source":["## Learning from the expert: a stats trick"]},{"cell_type":"markdown","metadata":{"id":"lvZ2yC8lH6GG"},"source":["### Which models of the data include interaction terms?"]},{"cell_type":"markdown","metadata":{"id":"Me-6wJrMIhLp"},"source":["<div class=\"\"><p>Recall from the video that interaction terms involve products of features.</p>\n","<p>Suppose we have two features <code>x</code> and <code>y</code>, and we use models that process the features as follows:</p>\n","<ol>\n","<li>βx + βy + ββ</li>\n","<li>βxy + βx + βy</li>\n","<li>βx + βy + βx^2 + βy^2</li>\n","</ol>\n","<p>where β is a coefficient in your model (not a feature).</p>\n","<p>Which expression(s) include interaction terms?</p></div>"]},{"cell_type":"markdown","metadata":{"id":"Qvm8ll1JItje"},"source":["<pre>\n","Possible Answers\n","The first expression.\n","<b>The second expression.</b>\n","The third expression.\n","The first and third expressions.\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"AtODmOQiJOgX"},"source":["**An xy term is present, which represents interactions between features. Nice work, let''s implement this!**"]},{"cell_type":"markdown","metadata":{"id":"63g-YeEzJTef"},"source":["### Implement interaction modeling in scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"jIqC0G-lJXaq"},"source":["<div class=\"\"><p>It's time to add interaction features to your model. The <code>PolynomialFeatures</code> object in scikit-learn does just that, but here you're going to use a custom interaction object, <code>SparseInteractions</code>. Interaction terms are a statistical tool that lets your model express what happens if two features appear together in the same row.</p>\n","<p><code>SparseInteractions</code> does the same thing as <code>PolynomialFeatures</code>, but it uses sparse matrices to do so. You can get the code for <code>SparseInteractions</code> at <a href=\"https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py\" target=\"_blank\" rel=\"noopener noreferrer\">this GitHub Gist</a>.</p>\n","<p><code>PolynomialFeatures</code> and <code>SparseInteractions</code> both take the argument <code>degree</code>, which tells them what polynomial degree of interactions to compute.</p>\n","<p>You're going to consider interaction terms of <code>degree=2</code> in your pipeline. You will insert these steps <em>after</em> the preprocessing steps you've built out so far, but <em>before</em> the classifier steps.</p>\n","<p>Pipelines with interaction terms take a while to train (since you're making n features into n-squared features!), so as long as you set it up right, we'll do the heavy lifting and tell you what your score is!</p></div>"]},{"cell_type":"code","metadata":{"id":"si-MLrp_Jn_N","executionInfo":{"status":"ok","timestamp":1616769324524,"user_tz":180,"elapsed":928,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["#https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py\n","from itertools import combinations\n","\n","import numpy as np\n","from scipy import sparse\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","\n","class SparseInteractions(BaseEstimator, TransformerMixin):\n","    def __init__(self, degree=2, feature_name_separator=\"_\"):\n","        self.degree = degree\n","        self.feature_name_separator = feature_name_separator\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        if not sparse.isspmatrix_csc(X):\n","            X = sparse.csc_matrix(X)\n","\n","        if hasattr(X, \"columns\"):\n","            self.orig_col_names = X.columns\n","        else:\n","            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n","\n","        spi = self._create_sparse_interactions(X)\n","        return spi\n","\n","    def get_feature_names(self):\n","        return self.feature_names\n","\n","    def _create_sparse_interactions(self, X):\n","        out_mat = []\n","        self.feature_names = self.orig_col_names.tolist()\n","\n","        for sub_degree in range(2, self.degree + 1):\n","            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n","                # add name for new column\n","                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n","                self.feature_names.append(name)\n","\n","                # get column multiplications value\n","                out = X[:, col_ixs[0]]\n","                for j in col_ixs[1:]:\n","                    out = out.multiply(X[:, j])\n","\n","                out_mat.append(out)\n","\n","        return sparse.hstack([X] + out_mat)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hpqz7iJMJZI5"},"source":["Instructions\n","<li>Add the interaction terms step using <code>SparseInteractions()</code> with <code>degree=2</code>. Give it a name of <code>'int'</code>, and make sure it is after the preprocessing step but before scaling.</li>"]},{"cell_type":"code","metadata":{"id":"u48zIpHtJ6aD","executionInfo":{"status":"ok","timestamp":1616771051036,"user_tz":180,"elapsed":501,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Instantiate pipeline: pl\n","pl = Pipeline([\n","        ('union', FeatureUnion(\n","            transformer_list = [\n","                ('numeric_features', Pipeline([\n","                    ('selector', get_numeric_data),\n","                    ('imputer', SimpleImputer())\n","                ])),\n","                ('text_features', Pipeline([\n","                    ('selector', get_text_data),\n","                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n","                                                   ngram_range=(1, 2))),  \n","                    ('dim_red', SelectKBest(chi2, chi_k))\n","                ]))\n","             ]\n","        )),\n","        ('int', SparseInteractions(degree=2)),\n","        ('scale', MaxAbsScaler()),\n","        ('clf', OneVsRestClassifier(LogisticRegression()))\n","    ])"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88Vg2-pfJ-_c"},"source":["**Log loss score: 1.2256. Nice improvement from 1.2681!**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"os5IpcSL9vc4","executionInfo":{"status":"ok","timestamp":1616771173788,"user_tz":180,"elapsed":120725,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"3eef7b95-8c7e-497f-e26d-561b158758e1"},"source":["%%time\n","pl.fit(X_train, y_train)\n","accuracy = pl.score(X_test, y_test)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 46s, sys: 1min 21s, total: 3min 8s\n","Wall time: 2min\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRkv0LNHC9zu","executionInfo":{"status":"ok","timestamp":1616771173788,"user_tz":180,"elapsed":119837,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"2abcd7f7-ccd2-48be-a8d7-e782d605236f"},"source":["accuracy"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2948717948717949"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"XDEm2Go7KESO"},"source":["## Learning from the expert: the winning model"]},{"cell_type":"markdown","metadata":{"id":"glqkeffcw2lR"},"source":["### Why is hashing a useful trick?"]},{"cell_type":"markdown","metadata":{"id":"uoFmMveGw5Nq"},"source":["<div class=\"\"><p>In the video, Peter explained that a <a href=\"https://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick\" target=\"_blank\" rel=\"noopener noreferrer\">hash</a> function takes an input, in your case a token, and outputs a hash value. For example, the input may  be a string and the hash value may be an integer.</p>\n","<p>We've loaded a familiar python datatype, a dictionary called <code>hash_dict</code>, that makes this mapping concept a bit more explicit. In fact, <a href=\"http://stackoverflow.com/questions/114830/is-a-python-dictionary-an-example-of-a-hash-table\" target=\"_blank\" rel=\"noopener noreferrer\">python dictionaries ARE hash tables</a>!</p>\n","<p>Print <code>hash_dict</code> in the IPython Shell to get a sense of how strings can be mapped to integers.</p>\n","<p>By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets.</p>\n","<p>Using the above information, answer the following:</p>\n","<p>Why is hashing a useful trick?</p></div>"]},{"cell_type":"code","metadata":{"id":"mVHvvO_FxF7-"},"source":["hash_dict = {'and': 780, 'fluids': 354, 'fuel': 895, 'petro': 354, 'vend': 785}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0UTP_tXw9Tk"},"source":["<pre>\n","Possible Answers\n","Hashing isn't useful unless you're working with numbers.\n","Some problems are memory-bound and not easily parallelizable, but hashing parallelizes them.\n","<b>Some problems are memory-bound and not easily parallelizable, and hashing enforces a fixed length computation instead of using a mutable datatype (like a dictionary).</b>\n","Hashing enforces a mutable length computation instead of using a fixed length datatype, like a dictionary.\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"WHVTJIuIxvgr"},"source":["**Enforcing a fixed length can speed up calculations drastically, especially on large datasets!**"]},{"cell_type":"markdown","metadata":{"id":"aeqxuvMYxxTj"},"source":["### Implementing the hashing trick in scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"aBX-m_RWxz7l"},"source":["<div class=\"\"><p>In this exercise you will check out the scikit-learn implementation of <code>HashingVectorizer</code> before adding it to your pipeline later.</p>\n","<p>As you saw in the video, <code>HashingVectorizer</code> acts just like <code>CountVectorizer</code> in that it can accept <code>token_pattern</code> and <code>ngram_range</code> parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing!</p></div>"]},{"cell_type":"markdown","metadata":{"id":"_JhX79jLx1kc"},"source":["Instructions\n","<ul>\n","<li>Import <code>HashingVectorizer</code> from <code>sklearn.feature_extraction.text</code>.</li>\n","<li>Instantiate the <code>HashingVectorizer</code> as <code>hashing_vec</code> using the <code>TOKENS_ALPHANUMERIC</code> pattern.</li>\n","<li>Fit and transform <code>hashing_vec</code> using <code>text_data</code>. Save the result as <code>hashed_text</code>.</li>\n","<li>Hit 'Submit Answer' to see some of the resulting hash values.</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"y48bykQYygxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616769951678,"user_tz":180,"elapsed":540,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"76108929-7f27-4ae4-d1dd-4b98f01cf77b"},"source":["# Import HashingVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","\n","# Get text data: text_data\n","text_data = combine_text_columns(X_train)\n","\n","# Create the token pattern: TOKENS_ALPHANUMERIC\n","TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n","\n","# Instantiate the HashingVectorizer: hashing_vec\n","hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n","\n","# Fit and transform the Hashing Vectorizer\n","hashed_text = hashing_vec.fit_transform(text_data)\n","\n","# Create DataFrame and print the head\n","hashed_df = pd.DataFrame(hashed_text.data)\n","print(hashed_df.head())"],"execution_count":32,"outputs":[{"output_type":"stream","text":["          0\n","0  0.213201\n","1 -0.213201\n","2 -0.213201\n","3 -0.213201\n","4  0.213201\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xqPBK8FSydSR"},"source":["**As you can see, some text is hashed to the same value, but as Peter mentioned in the video, this doesn't neccessarily hurt performance.**"]},{"cell_type":"markdown","metadata":{"id":"X7klt9xRyk9D"},"source":["### Build the winning model"]},{"cell_type":"markdown","metadata":{"id":"mUB8yZNSyogs"},"source":["<div class=\"\"><p>You have arrived! This is where all of your hard work pays off. It's time to build the model that won DrivenData's competition.</p>\n","<p>You've constructed a robust, powerful pipeline capable of processing training <em>and</em> testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow!</p>\n","<p>All you need to do is add the <code>HashingVectorizer</code> step to the pipeline to replace the <code>CountVectorizer</code> step.</p>\n","<p>The parameters <code>non_negative=True</code>, <code>norm=None</code>, and  <code>binary=False</code> make the <code>HashingVectorizer</code> perform similarly to the default settings on the <code>CountVectorizer</code> so you can just replace one with the other.</p></div>"]},{"cell_type":"markdown","metadata":{"id":"6CWTD7kKyxqG"},"source":["Instructions\n","<ul>\n","<li>Import <code>HashingVectorizer</code> from <code>sklearn.feature_extraction.text</code>.</li>\n","<li>Add a <code>HashingVectorizer</code> step to the pipeline.<ul>\n","<li>Name the step <code>'vectorizer'</code>.</li>\n","<li>Use the <code>TOKENS_ALPHANUMERIC</code> token pattern.</li>\n","<li>Specify the <code>ngram_range</code> to be <code>(1, 2)</code></li></ul></li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"5BgX6UZQzJXO","executionInfo":{"status":"ok","timestamp":1616770476311,"user_tz":180,"elapsed":523,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}}},"source":["# Import the hashing vectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","\n","# Instantiate the winning model pipeline: pl\n","pl = Pipeline([\n","        ('union', FeatureUnion(\n","            transformer_list = [\n","                ('numeric_features', Pipeline([\n","                    ('selector', get_numeric_data),\n","                    ('imputer', SimpleImputer())\n","                ])),\n","                ('text_features', Pipeline([\n","                    ('selector', get_text_data),\n","                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC, norm=None, binary=False,\n","                                                     ngram_range=(1, 2), alternate_sign=False)), #non_negative=True\n","                    ('dim_red', SelectKBest(chi2, chi_k))\n","                ]))\n","             ]\n","        )),\n","        ('int', SparseInteractions(degree=2)),\n","        ('scale', MaxAbsScaler()),\n","        ('clf', OneVsRestClassifier(LogisticRegression()))\n","    ])"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqVzT4wuzQdo"},"source":["**Log loss: 1.2258. Looks like the performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer. Try this pipeline out on the whole dataset on your local machine to see its full power!**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3AnOUeLAGQV","executionInfo":{"status":"ok","timestamp":1616770636260,"user_tz":180,"elapsed":157800,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"352b5814-273b-4403-e458-14711103a5bc"},"source":["%%time\n","pl.fit(X_train, y_train)\n","accuracy = pl.score(X_test, y_test)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["CPU times: user 2min 24s, sys: 1min 19s, total: 3min 44s\n","Wall time: 2min 37s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8GXqlYulC6iU","executionInfo":{"status":"ok","timestamp":1616770739233,"user_tz":180,"elapsed":496,"user":{"displayName":"Lucas Nunes","photoUrl":"","userId":"13884079641685385195"}},"outputId":"b88c1250-b584-41d3-a09b-5ca72ab731df"},"source":["accuracy"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2948717948717949"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"OwPhHY2HzVfZ"},"source":["### What tactics got the winner the best score?"]},{"cell_type":"markdown","metadata":{"id":"u7nIfm3bzYPJ"},"source":["<div class=\"\"><p>Now you've implemented the winning model from start to finish. If you want to use this model locally, <a href=\"https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">this Jupyter notebook</a> contains all the code you've worked so hard on. You can now take that code and build on it!</p>\n","<p>Let's take a moment to reflect on why this model did so well. What tactics got the winner the best score?</p></div>"]},{"cell_type":"markdown","metadata":{"id":"25hJCCTlza-j"},"source":["<pre>\n","Possible Answers\n","The winner used a 500 layer deep convolutional neural network to master the budget data.\n","The winner used an ensemble of many models for classification, taking the best results as predictions.\n","<b>The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data.</b>\n","</pre>"]}]}